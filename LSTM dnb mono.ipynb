{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM на оригинальном датасете\n",
    "\n",
    "Попытка сделать монофонический выход из сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset(height=16, limit=1000, patterns_file=\"decode_patterns/patterns.pairs.tsv\", mono=True)\n",
    "\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(A)\n",
    "    idx = np.arange(L) < p*L\n",
    "    np.random.shuffle(idx)\n",
    "    yield A[idx]\n",
    "    yield B[idx]\n",
    "    yield A[np.logical_not(idx)]\n",
    "    yield B[np.logical_not(idx)]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 8, 1, 8, 1, 1, 1, 8, 1, 8, 1, 8, 1, 1, 6], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем определить модель LSTM как конечный автомат\n",
    "class DrumNBassLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBassLSTM, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.hidden_size = 34\n",
    "        self.layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.layer_count)\n",
    "        self.embed_layer = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (64, 32, 14)\n",
    "        # то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.embed_layer(output))*37\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBassLSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 258.1539917\n",
      "[1,     5] train loss: 195.4260925\n",
      "Epoch #1\n",
      "[2,     1] train loss: 240.0964661\n",
      "[2,     5] train loss: 184.3658356\n",
      "Epoch #2\n",
      "[3,     1] train loss: 229.0117950\n",
      "[3,     5] train loss: 171.8666382\n",
      "Epoch #3\n",
      "[4,     1] train loss: 216.2558899\n",
      "[4,     5] train loss: 163.2716736\n",
      "Epoch #4\n",
      "[5,     1] train loss: 196.6236420\n",
      "[5,     5] train loss: 149.1508331\n",
      "Epoch #5\n",
      "[6,     1] train loss: 182.9964447\n",
      "[6,     5] train loss: 133.1781982\n",
      "Epoch #6\n",
      "[7,     1] train loss: 155.8202820\n",
      "[7,     5] train loss: 117.2579697\n",
      "Epoch #7\n",
      "[8,     1] train loss: 128.0091095\n",
      "[8,     5] train loss: 94.0577087\n",
      "Epoch #8\n",
      "[9,     1] train loss: 101.1207123\n",
      "[9,     5] train loss: 74.2962463\n",
      "Epoch #9\n",
      "[10,     1] train loss: 76.9555817\n",
      "[10,     5] train loss: 64.9887955\n",
      "Epoch #10\n",
      "[11,     1] train loss: 67.4443741\n",
      "[11,     5] train loss: 64.2468353\n",
      "Epoch #11\n",
      "[12,     1] train loss: 64.2493668\n",
      "[12,     5] train loss: 63.2470749\n",
      "Epoch #12\n",
      "[13,     1] train loss: 66.7421265\n",
      "[13,     5] train loss: 62.5447624\n",
      "Epoch #13\n",
      "[14,     1] train loss: 64.8756256\n",
      "[14,     5] train loss: 62.6394424\n",
      "Epoch #14\n",
      "[15,     1] train loss: 64.1648636\n",
      "[15,     5] train loss: 60.2122917\n",
      "Epoch #15\n",
      "[16,     1] train loss: 63.4166756\n",
      "[16,     5] train loss: 59.0804573\n",
      "Epoch #16\n",
      "[17,     1] train loss: 59.3569679\n",
      "[17,     5] train loss: 59.5580696\n",
      "Epoch #17\n",
      "[18,     1] train loss: 60.3728409\n",
      "[18,     5] train loss: 58.5582451\n",
      "Epoch #18\n",
      "[19,     1] train loss: 54.7499809\n",
      "[19,     5] train loss: 58.5024925\n",
      "Epoch #19\n",
      "[20,     1] train loss: 57.1923943\n",
      "[20,     5] train loss: 57.8554466\n",
      "Epoch #20\n",
      "[21,     1] train loss: 54.4529305\n",
      "[21,     5] train loss: 56.7697113\n",
      "Epoch #21\n",
      "[22,     1] train loss: 57.9323082\n",
      "[22,     5] train loss: 57.2237007\n",
      "Epoch #22\n",
      "[23,     1] train loss: 60.1871681\n",
      "[23,     5] train loss: 56.9909142\n",
      "Epoch #23\n",
      "[24,     1] train loss: 54.5950737\n",
      "[24,     5] train loss: 56.1780746\n",
      "Epoch #24\n",
      "[25,     1] train loss: 57.2602081\n",
      "[25,     5] train loss: 56.3385460\n",
      "Epoch #25\n",
      "[26,     1] train loss: 55.2680626\n",
      "[26,     5] train loss: 55.8322014\n",
      "Epoch #26\n",
      "[27,     1] train loss: 56.9982681\n",
      "[27,     5] train loss: 56.3889442\n",
      "Epoch #27\n",
      "[28,     1] train loss: 55.2693710\n",
      "[28,     5] train loss: 55.1241066\n",
      "Epoch #28\n",
      "[29,     1] train loss: 55.9031067\n",
      "[29,     5] train loss: 55.9390053\n",
      "Epoch #29\n",
      "[30,     1] train loss: 55.3288841\n",
      "[30,     5] train loss: 54.3615051\n",
      "Epoch #30\n",
      "[31,     1] train loss: 55.1243362\n",
      "[31,     5] train loss: 54.5041672\n",
      "Epoch #31\n",
      "[32,     1] train loss: 54.7864037\n",
      "[32,     5] train loss: 55.0925224\n",
      "Epoch #32\n",
      "[33,     1] train loss: 56.4331322\n",
      "[33,     5] train loss: 52.6527344\n",
      "Epoch #33\n",
      "[34,     1] train loss: 55.7545700\n",
      "[34,     5] train loss: 55.3438866\n",
      "Epoch #34\n",
      "[35,     1] train loss: 54.0703773\n",
      "[35,     5] train loss: 54.4206413\n",
      "Epoch #35\n",
      "[36,     1] train loss: 50.5204010\n",
      "[36,     5] train loss: 53.4396187\n",
      "Epoch #36\n",
      "[37,     1] train loss: 55.5204811\n",
      "[37,     5] train loss: 53.7685791\n",
      "Epoch #37\n",
      "[38,     1] train loss: 48.1456642\n",
      "[38,     5] train loss: 53.8170761\n",
      "Epoch #38\n",
      "[39,     1] train loss: 52.2744675\n",
      "[39,     5] train loss: 54.3371017\n",
      "Epoch #39\n",
      "[40,     1] train loss: 51.5978699\n",
      "[40,     5] train loss: 52.5291748\n",
      "Epoch #40\n",
      "[41,     1] train loss: 52.4019585\n",
      "[41,     5] train loss: 52.5839630\n",
      "Epoch #41\n",
      "[42,     1] train loss: 52.4045029\n",
      "[42,     5] train loss: 52.0311958\n",
      "Epoch #42\n",
      "[43,     1] train loss: 51.5662994\n",
      "[43,     5] train loss: 52.3684792\n",
      "Epoch #43\n",
      "[44,     1] train loss: 48.2491570\n",
      "[44,     5] train loss: 52.2590919\n",
      "Epoch #44\n",
      "[45,     1] train loss: 52.1607094\n",
      "[45,     5] train loss: 50.4865692\n",
      "Epoch #45\n",
      "[46,     1] train loss: 51.8808861\n",
      "[46,     5] train loss: 51.5557648\n",
      "Epoch #46\n",
      "[47,     1] train loss: 51.3425140\n",
      "[47,     5] train loss: 52.3589714\n",
      "Epoch #47\n",
      "[48,     1] train loss: 52.4800529\n",
      "[48,     5] train loss: 48.6908836\n",
      "Epoch #48\n",
      "[49,     1] train loss: 52.5210228\n",
      "[49,     5] train loss: 52.4134193\n",
      "Epoch #49\n",
      "[50,     1] train loss: 49.6438942\n",
      "[50,     5] train loss: 51.1352234\n",
      "Epoch #50\n",
      "[51,     1] train loss: 47.9141464\n",
      "[51,     5] train loss: 51.2812401\n",
      "Epoch #51\n",
      "[52,     1] train loss: 49.9351311\n",
      "[52,     5] train loss: 51.7110077\n",
      "Epoch #52\n",
      "[53,     1] train loss: 47.7238159\n",
      "[53,     5] train loss: 50.1647797\n",
      "Epoch #53\n",
      "[54,     1] train loss: 49.2954788\n",
      "[54,     5] train loss: 50.2890251\n",
      "Epoch #54\n",
      "[55,     1] train loss: 47.0387268\n",
      "[55,     5] train loss: 49.9840508\n",
      "Epoch #55\n",
      "[56,     1] train loss: 50.3588181\n",
      "[56,     5] train loss: 49.6640053\n",
      "Epoch #56\n",
      "[57,     1] train loss: 47.2488823\n",
      "[57,     5] train loss: 49.7932480\n",
      "Epoch #57\n",
      "[58,     1] train loss: 50.3908081\n",
      "[58,     5] train loss: 49.8918762\n",
      "Epoch #58\n",
      "[59,     1] train loss: 47.7801971\n",
      "[59,     5] train loss: 49.7682732\n",
      "Epoch #59\n",
      "[60,     1] train loss: 48.5104485\n",
      "[60,     5] train loss: 49.7558151\n",
      "Epoch #60\n",
      "[61,     1] train loss: 50.8995209\n",
      "[61,     5] train loss: 50.2110733\n",
      "Epoch #61\n",
      "[62,     1] train loss: 49.0310364\n",
      "[62,     5] train loss: 49.4470818\n",
      "Epoch #62\n",
      "[63,     1] train loss: 49.3804741\n",
      "[63,     5] train loss: 48.9954773\n",
      "Epoch #63\n",
      "[64,     1] train loss: 44.2438545\n",
      "[64,     5] train loss: 48.6338585\n",
      "Epoch #64\n",
      "[65,     1] train loss: 41.9048233\n",
      "[65,     5] train loss: 49.0126556\n",
      "Epoch #65\n",
      "[66,     1] train loss: 47.7494049\n",
      "[66,     5] train loss: 47.9272102\n",
      "Epoch #66\n",
      "[67,     1] train loss: 46.6473541\n",
      "[67,     5] train loss: 49.8223976\n",
      "Epoch #67\n",
      "[68,     1] train loss: 48.3574448\n",
      "[68,     5] train loss: 49.0578720\n",
      "Epoch #68\n",
      "[69,     1] train loss: 45.0281715\n",
      "[69,     5] train loss: 50.4041779\n",
      "Epoch #69\n",
      "[70,     1] train loss: 42.4958267\n",
      "[70,     5] train loss: 49.2094528\n",
      "Epoch #70\n",
      "[71,     1] train loss: 42.3744240\n",
      "[71,     5] train loss: 50.1314667\n",
      "Epoch #71\n",
      "[72,     1] train loss: 45.8691673\n",
      "[72,     5] train loss: 48.5072311\n",
      "Epoch #72\n",
      "[73,     1] train loss: 44.5855980\n",
      "[73,     5] train loss: 48.3807678\n",
      "Epoch #73\n",
      "[74,     1] train loss: 42.6284409\n",
      "[74,     5] train loss: 48.9730721\n",
      "Epoch #74\n",
      "[75,     1] train loss: 46.7228813\n",
      "[75,     5] train loss: 47.3477585\n",
      "Epoch #75\n",
      "[76,     1] train loss: 44.5480804\n",
      "[76,     5] train loss: 47.3456070\n",
      "Epoch #76\n",
      "[77,     1] train loss: 42.2015457\n",
      "[77,     5] train loss: 48.9279625\n",
      "Epoch #77\n",
      "[78,     1] train loss: 46.3141403\n",
      "[78,     5] train loss: 47.6631310\n",
      "Epoch #78\n",
      "[79,     1] train loss: 46.5623474\n",
      "[79,     5] train loss: 47.1050171\n",
      "Epoch #79\n",
      "[80,     1] train loss: 43.0175438\n",
      "[80,     5] train loss: 48.2525925\n",
      "Epoch #80\n",
      "[81,     1] train loss: 43.3049393\n",
      "[81,     5] train loss: 46.8587715\n",
      "Epoch #81\n",
      "[82,     1] train loss: 46.6773567\n",
      "[82,     5] train loss: 46.0332512\n",
      "Epoch #82\n",
      "[83,     1] train loss: 46.3749733\n",
      "[83,     5] train loss: 47.8798103\n",
      "Epoch #83\n",
      "[84,     1] train loss: 43.6277771\n",
      "[84,     5] train loss: 48.7914108\n",
      "Epoch #84\n",
      "[85,     1] train loss: 44.7787056\n",
      "[85,     5] train loss: 48.3785332\n",
      "Epoch #85\n",
      "[86,     1] train loss: 43.6040878\n",
      "[86,     5] train loss: 46.5603416\n",
      "Epoch #86\n",
      "[87,     1] train loss: 45.4282951\n",
      "[87,     5] train loss: 44.5633018\n",
      "Epoch #87\n",
      "[88,     1] train loss: 45.3745308\n",
      "[88,     5] train loss: 47.1897446\n",
      "Epoch #88\n",
      "[89,     1] train loss: 44.6492805\n",
      "[89,     5] train loss: 45.8920387\n",
      "Epoch #89\n",
      "[90,     1] train loss: 44.7723236\n",
      "[90,     5] train loss: 48.0949829\n",
      "Epoch #90\n",
      "[91,     1] train loss: 43.4989319\n",
      "[91,     5] train loss: 46.6933762\n",
      "Epoch #91\n",
      "[92,     1] train loss: 46.3357925\n",
      "[92,     5] train loss: 45.5876411\n",
      "Epoch #92\n",
      "[93,     1] train loss: 39.1222115\n",
      "[93,     5] train loss: 47.1857025\n",
      "Epoch #93\n",
      "[94,     1] train loss: 46.1068687\n",
      "[94,     5] train loss: 45.8357658\n",
      "Epoch #94\n",
      "[95,     1] train loss: 44.9286804\n",
      "[95,     5] train loss: 45.4048447\n",
      "Epoch #95\n",
      "[96,     1] train loss: 43.0145454\n",
      "[96,     5] train loss: 44.5083710\n",
      "Epoch #96\n",
      "[97,     1] train loss: 44.8181229\n",
      "[97,     5] train loss: 45.0806229\n",
      "Epoch #97\n",
      "[98,     1] train loss: 45.6467972\n",
      "[98,     5] train loss: 46.6653595\n",
      "Epoch #98\n",
      "[99,     1] train loss: 43.2472992\n",
      "[99,     5] train loss: 45.6008095\n",
      "Epoch #99\n",
      "[100,     1] train loss: 39.1147232\n",
      "[100,     5] train loss: 46.8476898\n",
      "Epoch #100\n",
      "[101,     1] train loss: 42.8758392\n",
      "[101,     5] train loss: 44.7375313\n",
      "Epoch #101\n",
      "[102,     1] train loss: 40.5793152\n",
      "[102,     5] train loss: 44.6632454\n",
      "Epoch #102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,     1] train loss: 42.3271980\n",
      "[103,     5] train loss: 47.2631187\n",
      "Epoch #103\n",
      "[104,     1] train loss: 42.3905907\n",
      "[104,     5] train loss: 45.6935204\n",
      "Epoch #104\n",
      "[105,     1] train loss: 42.5125046\n",
      "[105,     5] train loss: 47.1971970\n",
      "Epoch #105\n",
      "[106,     1] train loss: 41.4987221\n",
      "[106,     5] train loss: 46.5401855\n",
      "Epoch #106\n",
      "[107,     1] train loss: 44.0937004\n",
      "[107,     5] train loss: 46.0301506\n",
      "Epoch #107\n",
      "[108,     1] train loss: 40.8332481\n",
      "[108,     5] train loss: 46.7332108\n",
      "Epoch #108\n",
      "[109,     1] train loss: 43.5257530\n",
      "[109,     5] train loss: 45.4018097\n",
      "Epoch #109\n",
      "[110,     1] train loss: 43.6779366\n",
      "[110,     5] train loss: 44.5048866\n",
      "Epoch #110\n",
      "[111,     1] train loss: 39.7247810\n",
      "[111,     5] train loss: 45.0266876\n",
      "Epoch #111\n",
      "[112,     1] train loss: 43.8909874\n",
      "[112,     5] train loss: 44.9701759\n",
      "Epoch #112\n",
      "[113,     1] train loss: 39.6761894\n",
      "[113,     5] train loss: 44.6905357\n",
      "Epoch #113\n",
      "[114,     1] train loss: 42.0943832\n",
      "[114,     5] train loss: 44.4607529\n",
      "Epoch #114\n",
      "[115,     1] train loss: 42.5108604\n",
      "[115,     5] train loss: 46.4396736\n",
      "Epoch #115\n",
      "[116,     1] train loss: 40.8233147\n",
      "[116,     5] train loss: 45.1494560\n",
      "Epoch #116\n",
      "[117,     1] train loss: 35.8654099\n",
      "[117,     5] train loss: 45.2733574\n",
      "Epoch #117\n",
      "[118,     1] train loss: 41.6548615\n",
      "[118,     5] train loss: 45.3909904\n",
      "Epoch #118\n",
      "[119,     1] train loss: 37.8362732\n",
      "[119,     5] train loss: 45.4278511\n",
      "Epoch #119\n",
      "[120,     1] train loss: 41.2870293\n",
      "[120,     5] train loss: 44.5636536\n",
      "Epoch #120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d13a0840a1ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# loss = criterion(bass_outputs, batch_bass_train.long())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 128\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "    bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "        bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "        drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        bass_test = torch.tensor(bass_test, dtype=torch.float)\n",
    "        \n",
    "    examples_count = drum_train.size()[0]\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size,].transpose(0,1)\n",
    "        # transpose нужен для обмена размерности батча и размерности шагов\n",
    "#         print(f\"batch_drum_train:{batch_drum_train.size()}, batch_bass_train:{batch_bass_train.size()}\")\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train.size()}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_drum_train = drum_train[:,:,:].transpose(0,1)\n",
    "batch_bass_train = bass_train[:,:].transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  8,  8,  ..., 10,  7,  7],\n",
       "        [ 6,  4,  4,  ...,  4,  4,  4],\n",
       "        [ 6,  5,  5,  ...,  5,  5,  5],\n",
       "        ...,\n",
       "        [ 5,  7,  7,  ...,  7,  7,  7],\n",
       "        [ 5,  7,  7,  ...,  7,  6,  6],\n",
       "        [ 5,  8,  8,  ...,  6,  7,  7]], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((16,50))\n",
    "\n",
    "batch_drum = torch.cat((drum_train, drum_test, torch.tensor(drum_validation))).transpose(0,1)\n",
    "batch_bass = torch.cat((bass_train.int(), bass_test.int(), torch.tensor(bass_validation).int())).transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(bass_outputs.size()[1]):\n",
    "        bass_seq = bass_outputs[:,i]\n",
    "#         bass_seq = batch_bass[:,i]\n",
    "#         print(f\"bass_seq:{bass_seq.size()}\")\n",
    "        bass_output = []\n",
    "        for bass_note in bass_seq:\n",
    "            bass_row = np.eye(1, 36, bass_note - 1)[0]\n",
    "            bass_output.append(bass_row)\n",
    "        bass_output = torch.tensor(bass_output).int().squeeze()\n",
    "#         print(f\"bass_output:{bass_output.size()}\")\n",
    "        \n",
    "#         print(f\"batch_drum:{batch_drum[:,i,:].size()}, bass_output:{bass_output.size()}\")\n",
    "            \n",
    "        img_dnb = torch.cat((batch_drum[:,i,:].int(),bass_output), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb), 120, 1, 1, 36)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM на оригинальном датасете\n",
    "\n",
    "Попытка сделать монофонический выход из сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset(height=16, limit=20000, patterns_file=\"decode_patterns/patterns.pairs.tsv\", mono=True)\n",
    "\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(A)\n",
    "    idx = np.arange(L) < p*L\n",
    "    np.random.shuffle(idx)\n",
    "    yield A[idx]\n",
    "    yield B[idx]\n",
    "    yield A[np.logical_not(idx)]\n",
    "    yield B[np.logical_not(idx)]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 0, 6, 0, 4, 0, 1, 0, 4, 0, 4, 0, 4, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем определить модель LSTM как конечный автомат\n",
    "class DrumNBassLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBassLSTM, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.hidden_size = 34\n",
    "        self.layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.layer_count)\n",
    "        self.embed_layer = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (64, 32, 14)\n",
    "        # то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.embed_layer(output))*37\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBassLSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 199.0520935\n",
      "[1,     6] train loss: 148.3221919\n",
      "[1,    11] train loss: 127.0628535\n",
      "[1,    16] train loss: 117.7288742\n",
      "[1,    21] train loss: 100.7913424\n",
      "[1,    26] train loss: 100.8979276\n",
      "[1,    31] train loss: 96.9147606\n",
      "[1,    36] train loss: 90.5049553\n",
      "[1,    41] train loss: 73.0213521\n",
      "[1,    46] train loss: 62.1720282\n",
      "[1,    51] train loss: 62.0881964\n",
      "[1,    56] train loss: 58.6251233\n",
      "[1,    61] train loss: 61.9828644\n",
      "[1,    66] train loss: 55.0220559\n",
      "[1,    71] train loss: 61.3406067\n",
      "[1,    76] train loss: 50.8303661\n",
      "[1,    81] train loss: 58.2211412\n",
      "[1,    86] train loss: 57.2950102\n",
      "[1,    91] train loss: 55.1494389\n",
      "[1,    96] train loss: 45.6078288\n",
      "[1,   100] train loss: 44.8960083\n",
      "Epoch #1\n",
      "[2,     1] train loss: 52.2481613\n",
      "[2,     6] train loss: 50.8732669\n",
      "[2,    11] train loss: 49.9414590\n",
      "[2,    16] train loss: 47.1087233\n",
      "[2,    21] train loss: 47.4877523\n",
      "[2,    26] train loss: 50.6399409\n",
      "[2,    31] train loss: 55.5491333\n",
      "[2,    36] train loss: 53.9843356\n",
      "[2,    41] train loss: 50.9650962\n",
      "[2,    46] train loss: 55.3773448\n",
      "[2,    51] train loss: 55.8627491\n",
      "[2,    56] train loss: 51.4007231\n",
      "[2,    61] train loss: 57.6685244\n",
      "[2,    66] train loss: 49.3014933\n",
      "[2,    71] train loss: 54.9792086\n",
      "[2,    76] train loss: 46.8493239\n",
      "[2,    81] train loss: 53.9883493\n",
      "[2,    86] train loss: 52.8234819\n",
      "[2,    91] train loss: 52.4969165\n",
      "[2,    96] train loss: 44.7555110\n",
      "[2,   100] train loss: 39.6146690\n",
      "Epoch #2\n",
      "[3,     1] train loss: 49.0654678\n",
      "[3,     6] train loss: 49.0258458\n",
      "[3,    11] train loss: 46.4985205\n",
      "[3,    16] train loss: 42.8580755\n",
      "[3,    21] train loss: 45.1836561\n",
      "[3,    26] train loss: 46.5666650\n",
      "[3,    31] train loss: 55.2394587\n",
      "[3,    36] train loss: 47.5137202\n",
      "[3,    41] train loss: 48.5128009\n",
      "[3,    46] train loss: 52.5959352\n",
      "[3,    51] train loss: 52.1957092\n",
      "[3,    56] train loss: 50.2009697\n",
      "[3,    61] train loss: 53.8129838\n",
      "[3,    66] train loss: 47.1245422\n",
      "[3,    71] train loss: 54.2135417\n",
      "[3,    76] train loss: 42.3377209\n",
      "[3,    81] train loss: 51.6407229\n",
      "[3,    86] train loss: 49.3027331\n",
      "[3,    91] train loss: 49.8620872\n",
      "[3,    96] train loss: 42.9994806\n",
      "[3,   100] train loss: 38.3175423\n",
      "Epoch #3\n",
      "[4,     1] train loss: 44.7514114\n",
      "[4,     6] train loss: 47.7055213\n",
      "[4,    11] train loss: 46.7851772\n",
      "[4,    16] train loss: 43.1197357\n",
      "[4,    21] train loss: 43.2420063\n",
      "[4,    26] train loss: 46.0377566\n",
      "[4,    31] train loss: 53.6303457\n",
      "[4,    36] train loss: 46.1991507\n",
      "[4,    41] train loss: 47.4414864\n",
      "[4,    46] train loss: 53.1108418\n",
      "[4,    51] train loss: 50.8180809\n",
      "[4,    56] train loss: 46.3195794\n",
      "[4,    61] train loss: 53.0197430\n",
      "[4,    66] train loss: 45.9179306\n",
      "[4,    71] train loss: 52.1977234\n",
      "[4,    76] train loss: 42.3510958\n",
      "[4,    81] train loss: 50.4212399\n",
      "[4,    86] train loss: 48.4743512\n",
      "[4,    91] train loss: 50.6387831\n",
      "[4,    96] train loss: 42.1878128\n",
      "[4,   100] train loss: 36.6643623\n",
      "Epoch #4\n",
      "[5,     1] train loss: 43.3973274\n",
      "[5,     6] train loss: 45.4971224\n",
      "[5,    11] train loss: 45.3132979\n",
      "[5,    16] train loss: 38.9324888\n",
      "[5,    21] train loss: 44.4573116\n",
      "[5,    26] train loss: 46.7478339\n",
      "[5,    31] train loss: 50.5233409\n",
      "[5,    36] train loss: 48.8186868\n",
      "[5,    41] train loss: 46.8268528\n",
      "[5,    46] train loss: 51.5003618\n",
      "[5,    51] train loss: 50.1823133\n",
      "[5,    56] train loss: 46.7796853\n",
      "[5,    61] train loss: 53.2255211\n",
      "[5,    66] train loss: 43.1028983\n",
      "[5,    71] train loss: 50.2415428\n",
      "[5,    76] train loss: 41.8705095\n",
      "[5,    81] train loss: 49.1558603\n",
      "[5,    86] train loss: 48.8024000\n",
      "[5,    91] train loss: 47.4445655\n",
      "[5,    96] train loss: 39.9817231\n",
      "[5,   100] train loss: 35.7443447\n",
      "Epoch #5\n",
      "[6,     1] train loss: 44.9728737\n",
      "[6,     6] train loss: 45.1124713\n",
      "[6,    11] train loss: 44.8122965\n",
      "[6,    16] train loss: 39.5698516\n",
      "[6,    21] train loss: 43.9425360\n",
      "[6,    26] train loss: 47.0798950\n",
      "[6,    31] train loss: 48.1583697\n",
      "[6,    36] train loss: 47.7265021\n",
      "[6,    41] train loss: 46.0167332\n",
      "[6,    46] train loss: 50.6733170\n",
      "[6,    51] train loss: 48.6960748\n",
      "[6,    56] train loss: 45.1363131\n",
      "[6,    61] train loss: 51.0199267\n",
      "[6,    66] train loss: 44.8524132\n",
      "[6,    71] train loss: 49.6652578\n",
      "[6,    76] train loss: 40.9462261\n",
      "[6,    81] train loss: 47.6089471\n",
      "[6,    86] train loss: 46.2150612\n",
      "[6,    91] train loss: 49.4794235\n",
      "[6,    96] train loss: 42.2998613\n",
      "[6,   100] train loss: 34.7316578\n",
      "Epoch #6\n",
      "[7,     1] train loss: 44.6635857\n",
      "[7,     6] train loss: 43.7635892\n",
      "[7,    11] train loss: 45.2056236\n",
      "[7,    16] train loss: 38.7010409\n",
      "[7,    21] train loss: 43.7600842\n",
      "[7,    26] train loss: 46.6276849\n",
      "[7,    31] train loss: 45.4394550\n",
      "[7,    36] train loss: 48.7846813\n",
      "[7,    41] train loss: 46.3793488\n",
      "[7,    46] train loss: 49.8621674\n",
      "[7,    51] train loss: 48.6577269\n",
      "[7,    56] train loss: 45.5081902\n",
      "[7,    61] train loss: 51.1906509\n",
      "[7,    66] train loss: 43.8396893\n",
      "[7,    71] train loss: 49.0864067\n",
      "[7,    76] train loss: 39.7668781\n",
      "[7,    81] train loss: 46.7595463\n",
      "[7,    86] train loss: 45.7237701\n",
      "[7,    91] train loss: 49.7390238\n",
      "[7,    96] train loss: 39.3247458\n",
      "[7,   100] train loss: 35.2910591\n",
      "Epoch #7\n",
      "[8,     1] train loss: 40.3583221\n",
      "[8,     6] train loss: 43.2486890\n",
      "[8,    11] train loss: 44.3033403\n",
      "[8,    16] train loss: 38.3544820\n",
      "[8,    21] train loss: 43.1663704\n",
      "[8,    26] train loss: 45.2556044\n",
      "[8,    31] train loss: 47.1411107\n",
      "[8,    36] train loss: 48.0916481\n",
      "[8,    41] train loss: 43.9273891\n",
      "[8,    46] train loss: 50.0770467\n",
      "[8,    51] train loss: 49.5849495\n",
      "[8,    56] train loss: 45.9151471\n",
      "[8,    61] train loss: 52.6631177\n",
      "[8,    66] train loss: 43.4015783\n",
      "[8,    71] train loss: 48.9496651\n",
      "[8,    76] train loss: 40.1252009\n",
      "[8,    81] train loss: 46.6493479\n",
      "[8,    86] train loss: 45.9327316\n",
      "[8,    91] train loss: 48.9681129\n",
      "[8,    96] train loss: 39.7566039\n",
      "[8,   100] train loss: 35.2126415\n",
      "Epoch #8\n",
      "[9,     1] train loss: 38.1094513\n",
      "[9,     6] train loss: 42.9622053\n",
      "[9,    11] train loss: 43.7093302\n",
      "[9,    16] train loss: 39.3447793\n",
      "[9,    21] train loss: 43.0458546\n",
      "[9,    26] train loss: 46.0790106\n",
      "[9,    31] train loss: 48.3185946\n",
      "[9,    36] train loss: 47.3284384\n",
      "[9,    41] train loss: 44.6174692\n",
      "[9,    46] train loss: 50.3726603\n",
      "[9,    51] train loss: 48.2432931\n",
      "[9,    56] train loss: 44.7573783\n",
      "[9,    61] train loss: 50.7458280\n",
      "[9,    66] train loss: 41.7807369\n",
      "[9,    71] train loss: 49.4771093\n",
      "[9,    76] train loss: 38.7210674\n",
      "[9,    81] train loss: 45.6676604\n",
      "[9,    86] train loss: 45.1113383\n",
      "[9,    91] train loss: 48.6097717\n",
      "[9,    96] train loss: 40.4173400\n",
      "[9,   100] train loss: 34.9964230\n",
      "Epoch #9\n",
      "[10,     1] train loss: 41.9096336\n",
      "[10,     6] train loss: 43.0184218\n",
      "[10,    11] train loss: 45.0866725\n",
      "[10,    16] train loss: 40.2500852\n",
      "[10,    21] train loss: 42.5935526\n",
      "[10,    26] train loss: 44.5754630\n",
      "[10,    31] train loss: 49.0610917\n",
      "[10,    36] train loss: 44.1330496\n",
      "[10,    41] train loss: 44.9637426\n",
      "[10,    46] train loss: 49.9301294\n",
      "[10,    51] train loss: 49.0320644\n",
      "[10,    56] train loss: 44.3990809\n",
      "[10,    61] train loss: 52.5193748\n",
      "[10,    66] train loss: 42.3975875\n",
      "[10,    71] train loss: 48.5100861\n",
      "[10,    76] train loss: 39.2648036\n",
      "[10,    81] train loss: 44.0048237\n",
      "[10,    86] train loss: 44.8651110\n",
      "[10,    91] train loss: 46.8547300\n",
      "[10,    96] train loss: 39.8051688\n",
      "[10,   100] train loss: 35.8114204\n",
      "Epoch #10\n",
      "[11,     1] train loss: 44.5297852\n",
      "[11,     6] train loss: 43.4961497\n",
      "[11,    11] train loss: 44.4047693\n",
      "[11,    16] train loss: 39.4543800\n",
      "[11,    21] train loss: 42.2457765\n",
      "[11,    26] train loss: 46.3338261\n",
      "[11,    31] train loss: 44.4731782\n",
      "[11,    36] train loss: 48.5832450\n",
      "[11,    41] train loss: 44.9340750\n",
      "[11,    46] train loss: 49.4138934\n",
      "[11,    51] train loss: 48.5179698\n",
      "[11,    56] train loss: 44.6572781\n",
      "[11,    61] train loss: 51.6458187\n",
      "[11,    66] train loss: 41.8640359\n",
      "[11,    71] train loss: 47.1287162\n",
      "[11,    76] train loss: 38.5390759\n",
      "[11,    81] train loss: 44.5128797\n",
      "[11,    86] train loss: 44.8167992\n",
      "[11,    91] train loss: 46.3693148\n",
      "[11,    96] train loss: 42.1286983\n",
      "[11,   100] train loss: 36.4526142\n",
      "Epoch #11\n",
      "[12,     1] train loss: 41.6806335\n",
      "[12,     6] train loss: 43.5193481\n",
      "[12,    11] train loss: 45.3832328\n",
      "[12,    16] train loss: 38.7035739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,    21] train loss: 42.9735724\n",
      "[12,    26] train loss: 47.9306100\n",
      "[12,    31] train loss: 47.1599579\n",
      "[12,    36] train loss: 46.5711308\n",
      "[12,    41] train loss: 43.8855839\n",
      "[12,    46] train loss: 48.6168442\n",
      "[12,    51] train loss: 49.6077271\n",
      "[12,    56] train loss: 43.6771965\n",
      "[12,    61] train loss: 49.9138203\n",
      "[12,    66] train loss: 40.8880698\n",
      "[12,    71] train loss: 47.7524821\n",
      "[12,    76] train loss: 37.6756452\n",
      "[12,    81] train loss: 45.3308595\n",
      "[12,    86] train loss: 43.7334347\n",
      "[12,    91] train loss: 48.1243776\n",
      "[12,    96] train loss: 41.2615903\n",
      "[12,   100] train loss: 34.2068943\n",
      "Epoch #12\n",
      "[13,     1] train loss: 45.6268349\n",
      "[13,     6] train loss: 42.8030408\n",
      "[13,    11] train loss: 42.1434059\n",
      "[13,    16] train loss: 39.3839378\n",
      "[13,    21] train loss: 43.3627224\n",
      "[13,    26] train loss: 44.7731514\n",
      "[13,    31] train loss: 47.9347604\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d13a0840a1ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# loss = criterion(bass_outputs, batch_bass_train.long())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 128\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "    bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "        bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "        drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        bass_test = torch.tensor(bass_test, dtype=torch.float)\n",
    "        \n",
    "    examples_count = drum_train.size()[0]\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size,].transpose(0,1)\n",
    "        # transpose нужен для обмена размерности батча и размерности шагов\n",
    "#         print(f\"batch_drum_train:{batch_drum_train.size()}, batch_bass_train:{batch_bass_train.size()}\")\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train.size()}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_drum_train = drum_train[:,:,:].transpose(0,1)\n",
    "batch_bass_train = bass_train[:,:].transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 8, 8,  ..., 8, 8, 8],\n",
       "        [5, 5, 5,  ..., 6, 6, 6],\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        ...,\n",
       "        [6, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        [7, 8, 8,  ..., 8, 8, 8]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fff71c04111a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mbass_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbass_note\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbass_seq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mbass_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m36\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbass_note\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mbass_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mbass_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((16,50))\n",
    "\n",
    "batch_drum = torch.cat((drum_train, drum_test, torch.tensor(drum_validation))).transpose(0,1)\n",
    "batch_bass = torch.cat((bass_train.int(), bass_test.int(), torch.tensor(bass_validation).int())).transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(bass_outputs.size()[1]):\n",
    "        bass_seq = bass_outputs[:,i]\n",
    "#         bass_seq = batch_bass[:,i]\n",
    "#         print(f\"bass_seq:{bass_seq.size()}\")\n",
    "        bass_output = []\n",
    "        for bass_note in bass_seq:\n",
    "            bass_row = np.eye(1, 36, bass_note - 1)[0]\n",
    "            bass_output.append(bass_row)\n",
    "        bass_output = torch.tensor(bass_output).int().squeeze()\n",
    "#         print(f\"bass_output:{bass_output.size()}\")\n",
    "        \n",
    "#         print(f\"batch_drum:{batch_drum[:,i,:].size()}, bass_output:{bass_output.size()}\")\n",
    "            \n",
    "        img_dnb = torch.cat((batch_drum[:,i,:].int(),bass_output), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb), 120, 1, 1, 36)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM на оригинальном датасете\n",
    "\n",
    "Попытка сделать монофонический выход из сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset(height=16, patterns_file=\"decode_patterns/train.tsv\", mono=True)\n",
    "\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(A)\n",
    "    idx = np.arange(L) < p*L\n",
    "    np.random.shuffle(idx)\n",
    "    yield A[idx]\n",
    "    yield B[idx]\n",
    "    yield A[np.logical_not(idx)]\n",
    "    yield B[np.logical_not(idx)]\n",
    "    \n",
    "# we can shuffle train and test set like this:\n",
    "drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "\n",
    "# selecting a validation set\n",
    "drum_validation, bass_validation = data_conversion.make_lstm_dataset(height=16\n",
    "                                                        , patterns_file=\"decode_patterns/validation.tsv\"\n",
    "                                                        , mono=True)\n",
    "drum_validation = torch.tensor(drum_validation, dtype=torch.float)\n",
    "bass_validation = torch.tensor(bass_validation, dtype=torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 16, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drum_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем определить модель LSTM как конечный автомат\n",
    "class DrumNBassLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBassLSTM, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.hidden_size = 34\n",
    "        self.layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.layer_count)\n",
    "        self.embed_layer = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (64, 32, 14)\n",
    "        # то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.embed_layer(output))*37\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBassLSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# для сравнения моделей необходим reconstruction_loss\n",
    "reconstruction_loss = criterion\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 189.2635040\n",
      "[1,     6] train loss: 172.8012441\n",
      "[1,    11] train loss: 151.4795583\n",
      "[1,    16] train loss: 143.3264440\n",
      "[1,    21] train loss: 131.4135666\n",
      "[1,    26] train loss: 116.6867142\n",
      "[1,    31] train loss: 97.0210177\n",
      "[1,    36] train loss: 82.6438230\n",
      "[1,    41] train loss: 64.6381620\n",
      "[1,    46] train loss: 61.3383236\n",
      "[1,    51] train loss: 58.1539574\n",
      "[1,    56] train loss: 59.5128174\n",
      "[1,    61] train loss: 57.8884252\n",
      "[1,    66] train loss: 58.2775796\n",
      "[1,    71] train loss: 56.7767785\n",
      "[1,    76] train loss: 60.5449511\n",
      "[1,    81] train loss: 53.7443765\n",
      "[1,    86] train loss: 58.1582381\n",
      "[1,    91] train loss: 58.6878757\n",
      "[1,    96] train loss: 54.8057639\n",
      "[1,   101] train loss: 52.2772503\n",
      "[1,   106] train loss: 59.5588964\n",
      "[1,   111] train loss: 54.9090055\n",
      "[1,   116] train loss: 53.9119129\n",
      "[1,   121] train loss: 53.6447436\n",
      "[1,   126] train loss: 51.6982117\n",
      "[1,   131] train loss: 55.0293427\n",
      "[1,   136] train loss: 58.9198023\n",
      "[1,   141] train loss: 52.5191504\n",
      "[1,   146] train loss: 54.4017601\n",
      "[1,   151] train loss: 49.3920619\n",
      "[1,   156] train loss: 53.9840825\n",
      "[1,   161] train loss: 58.7717717\n",
      "[1,   166] train loss: 56.3451710\n",
      "[1,   171] train loss: 50.7902247\n",
      "[1,   176] train loss: 53.9308039\n",
      "[1,   181] train loss: 54.8141931\n",
      "[1,   186] train loss: 52.7575792\n",
      "[1,   191] train loss: 51.2408428\n",
      "[1,   196] train loss: 48.9942455\n",
      "[1,   201] train loss: 52.0004807\n",
      "[1,   206] train loss: 56.2689648\n",
      "[1,   211] train loss: 52.4456139\n",
      "[1,   216] train loss: 48.2531153\n",
      "[1,   221] train loss: 47.7662551\n",
      "[1,   226] train loss: 50.8184083\n",
      "[1,   231] train loss: 48.5882765\n",
      "[1,   236] train loss: 51.4703833\n",
      "[1,   241] train loss: 46.6870721\n",
      "[1,   246] train loss: 46.7035205\n",
      "[1,   251] train loss: 46.7382380\n",
      "[1,   256] train loss: 48.9088554\n",
      "[1,   261] train loss: 48.4850540\n",
      "[1,   266] train loss: 43.4768162\n",
      "[1,   271] train loss: 46.7814916\n",
      "[1,   276] train loss: 50.2839120\n",
      "[1,   281] train loss: 50.7397709\n",
      "[1,   286] train loss: 53.7448247\n",
      "[1,   291] train loss: 47.8726126\n",
      "[1,   296] train loss: 50.6909275\n",
      "[1,   301] train loss: 55.8370590\n",
      "[1,   306] train loss: 42.2305902\n",
      "[1,   311] train loss: 47.9720694\n",
      "[1,   316] train loss: 57.8958505\n",
      "[1,   321] train loss: 44.2599640\n",
      "[1,   326] train loss: 50.1531410\n",
      "[1,   331] train loss: 48.3957253\n",
      "[1,   336] train loss: 51.0588385\n",
      "[1,   341] train loss: 51.5129484\n",
      "[1,   346] train loss: 50.1331323\n",
      "[1,   351] train loss: 48.2711773\n",
      "[1,   356] train loss: 48.6594187\n",
      "[1,   361] train loss: 46.7942619\n",
      "[1,   366] train loss: 52.8731143\n",
      "[1,   371] train loss: 46.3690955\n",
      "[1,   376] train loss: 44.4551105\n",
      "[1,   381] train loss: 50.4194501\n",
      "[1,   386] train loss: 42.2821229\n",
      "[1,   391] train loss: 53.5919329\n",
      "[1,   396] train loss: 55.3075447\n",
      "[1,   401] train loss: 40.7285926\n",
      "[1,   406] train loss: 49.3556213\n",
      "[1,   411] train loss: 42.1598638\n",
      "[1,   416] train loss: 40.8382111\n",
      "[1,   421] train loss: 45.5492573\n",
      "[1,   426] train loss: 51.2916075\n",
      "[1,   431] train loss: 43.5566463\n",
      "[1,   436] train loss: 52.6067874\n",
      "[1,   441] train loss: 50.1544120\n",
      "[1,   446] train loss: 47.6825339\n",
      "[1,   451] train loss: 35.3808136\n",
      "[1,   456] train loss: 50.2028618\n",
      "[1,   461] train loss: 53.6574472\n",
      "[1,   466] train loss: 55.7637685\n",
      "[1,   471] train loss: 46.2349606\n",
      "[1,   476] train loss: 47.2897695\n",
      "[1,   481] train loss: 45.3877805\n",
      "[1,   486] train loss: 47.9176941\n",
      "[1,   491] train loss: 41.3852488\n",
      "[1,   496] train loss: 44.2754148\n",
      "[1,   501] train loss: 50.7733625\n",
      "[1,   506] train loss: 43.1843618\n",
      "[1,   511] train loss: 38.0349286\n",
      "[1,   516] train loss: 41.5174777\n",
      "[1,   521] train loss: 46.4216951\n",
      "[1,   526] train loss: 42.5070623\n",
      "[1,   531] train loss: 48.7524465\n",
      "[1,   536] train loss: 45.3934739\n",
      "[1,   541] train loss: 46.3726959\n",
      "[1,   546] train loss: 50.5855675\n",
      "[1,   551] train loss: 55.4200630\n",
      "[1,   556] train loss: 46.8269253\n",
      "[1,   561] train loss: 51.5195567\n",
      "[1,   566] train loss: 45.2259916\n",
      "[1,   571] train loss: 46.3938243\n",
      "[1,   576] train loss: 48.0317802\n",
      "[1,   581] train loss: 51.1779912\n",
      "[1,   586] train loss: 44.1324126\n",
      "[1,   591] train loss: 48.3039831\n",
      "[1,   596] train loss: 46.5120748\n",
      "[1,   601] train loss: 40.1728611\n",
      "[1,   606] train loss: 43.8956534\n",
      "[1,   611] train loss: 48.2801374\n",
      "[1,   616] train loss: 47.1796341\n",
      "[1,   621] train loss: 47.9330139\n",
      "[1,   626] train loss: 45.5925503\n",
      "[1,   631] train loss: 46.8287818\n",
      "[1,   636] train loss: 43.8049024\n",
      "[1,   641] train loss: 42.3021793\n",
      "[1,   646] train loss: 45.1705850\n",
      "[1,   651] train loss: 53.9101976\n",
      "[1,   656] train loss: 41.3283577\n",
      "[1,   661] train loss: 51.0065269\n",
      "[1,   666] train loss: 50.1466370\n",
      "[1,   671] train loss: 50.1496201\n",
      "[1,   676] train loss: 50.8295180\n",
      "[1,   681] train loss: 42.4785055\n",
      "[1,   686] train loss: 48.5237605\n",
      "[1,   691] train loss: 48.0202490\n",
      "[1,   696] train loss: 42.2937743\n",
      "[1,   701] train loss: 48.8398088\n",
      "[1,   706] train loss: 43.8055782\n",
      "[1,   711] train loss: 51.7021529\n",
      "[1,   716] train loss: 45.8607171\n",
      "[1,   721] train loss: 50.4824009\n",
      "[1,   726] train loss: 42.5848687\n",
      "[1,   731] train loss: 44.6658834\n",
      "[1,   736] train loss: 47.4277871\n",
      "[1,   741] train loss: 41.0636311\n",
      "[1,   746] train loss: 50.9028594\n",
      "[1,   751] train loss: 50.0733064\n",
      "[1,   756] train loss: 42.1648242\n",
      "[1,   761] train loss: 53.3448334\n",
      "[1,   766] train loss: 48.0475744\n",
      "[1,   771] train loss: 41.6288039\n",
      "[1,   776] train loss: 48.7254136\n",
      "[1,   781] train loss: 45.2208106\n",
      "[1,   786] train loss: 47.0694167\n",
      "[1,   791] train loss: 43.5210571\n",
      "[1,   796] train loss: 50.8997167\n",
      "[1,   801] train loss: 51.5689462\n",
      "[1,   806] train loss: 50.0563246\n",
      "[1,   811] train loss: 47.7269688\n",
      "[1,   816] train loss: 48.1944796\n",
      "[1,   821] train loss: 45.8489208\n",
      "[1,   826] train loss: 43.1982009\n",
      "[1,   831] train loss: 47.2193120\n",
      "[1,   836] train loss: 41.7318656\n",
      "[1,   841] train loss: 45.0048243\n",
      "[1,   846] train loss: 40.1460978\n",
      "[1,   851] train loss: 41.9324799\n",
      "[1,   856] train loss: 36.4212545\n",
      "[1,   861] train loss: 47.3831100\n",
      "[1,   866] train loss: 53.6810404\n",
      "[1,   871] train loss: 49.0397263\n",
      "[1,   876] train loss: 45.2571214\n",
      "[1,   881] train loss: 43.8444990\n",
      "[1,   886] train loss: 46.7202454\n",
      "[1,   891] train loss: 44.5619570\n",
      "[1,   896] train loss: 41.1102587\n",
      "[1,   901] train loss: 44.5185362\n",
      "[1,   906] train loss: 45.9387919\n",
      "[1,   911] train loss: 47.0012964\n",
      "[1,   916] train loss: 37.9332994\n",
      "[1,   921] train loss: 45.5937519\n",
      "[1,   926] train loss: 39.5233262\n",
      "[1,   931] train loss: 39.2847608\n",
      "[1,   936] train loss: 43.7859993\n",
      "[1,   941] train loss: 45.8261674\n",
      "[1,   946] train loss: 42.6534259\n",
      "[1,   951] train loss: 45.7777608\n",
      "[1,   956] train loss: 48.4922225\n",
      "[1,   961] train loss: 45.8788325\n",
      "[1,   966] train loss: 43.6579043\n",
      "[1,   971] train loss: 51.7673187\n",
      "[1,   976] train loss: 47.7459215\n",
      "[1,   981] train loss: 42.4256884\n",
      "[1,   986] train loss: 57.3352019\n",
      "[1,   991] train loss: 40.7196007\n",
      "[1,   996] train loss: 44.8515898\n",
      "[1,  1001] train loss: 44.1067524\n",
      "[1,  1006] train loss: 48.4743869\n",
      "[1,  1011] train loss: 43.0032024\n",
      "[1,  1016] train loss: 43.9362520\n",
      "[1,  1021] train loss: 48.6200142\n",
      "[1,  1026] train loss: 39.9805946\n",
      "[1,  1031] train loss: 44.2921899\n",
      "[1,  1036] train loss: 42.3663247\n",
      "[1,  1041] train loss: 41.8671722\n",
      "[1,  1046] train loss: 44.1761417\n",
      "[1,  1051] train loss: 42.3354117\n",
      "[1,  1056] train loss: 47.7316017\n",
      "[1,  1061] train loss: 40.9409510\n",
      "[1,  1066] train loss: 53.9050452\n",
      "[1,  1071] train loss: 44.0724252\n",
      "[1,  1076] train loss: 45.0677891\n",
      "[1,  1081] train loss: 44.9973780\n",
      "[1,  1086] train loss: 42.8481433\n",
      "[1,  1091] train loss: 43.1695054\n",
      "[1,  1096] train loss: 39.7158896\n",
      "[1,  1101] train loss: 39.6210454\n",
      "[1,  1106] train loss: 51.1444842\n",
      "[1,  1111] train loss: 40.9115289\n",
      "[1,  1116] train loss: 53.1423613\n",
      "[1,  1121] train loss: 43.1205037\n",
      "[1,  1126] train loss: 46.3719050\n",
      "[1,  1131] train loss: 43.9019718\n",
      "[1,  1136] train loss: 41.3813839\n",
      "[1,  1141] train loss: 48.0888526\n",
      "[1,  1146] train loss: 48.4863358\n",
      "[1,  1151] train loss: 42.3867505\n",
      "[1,  1156] train loss: 41.1268126\n",
      "[1,  1161] train loss: 50.5314166\n",
      "[1,  1166] train loss: 50.3659058\n",
      "[1,  1171] train loss: 43.3161691\n",
      "[1,  1176] train loss: 43.9562060\n",
      "[1,  1181] train loss: 38.5473461\n",
      "[1,  1186] train loss: 52.9253025\n",
      "[1,  1191] train loss: 45.1412875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1196] train loss: 45.7841606\n",
      "[1,  1201] train loss: 45.0578690\n",
      "[1,  1206] train loss: 47.5325565\n",
      "[1,  1211] train loss: 43.6357231\n",
      "[1,  1216] train loss: 55.8497849\n",
      "[1,  1221] train loss: 43.5845477\n",
      "[1,  1226] train loss: 49.6684755\n",
      "[1,  1231] train loss: 39.3908653\n",
      "[1,  1236] train loss: 50.5263259\n",
      "[1,  1241] train loss: 44.8678157\n",
      "[1,  1246] train loss: 48.5691649\n",
      "[1,  1250] train loss: 45.1638329\n",
      "#1 reconstruction test loss: 54.022300720214844\n",
      "Epoch #1\n",
      "[2,     1] train loss: 61.5175056\n",
      "[2,     6] train loss: 42.3451525\n",
      "[2,    11] train loss: 46.3499959\n",
      "[2,    16] train loss: 48.5815932\n",
      "[2,    21] train loss: 47.7377809\n",
      "[2,    26] train loss: 44.2246806\n",
      "[2,    31] train loss: 43.4164486\n",
      "[2,    36] train loss: 44.0706278\n",
      "[2,    41] train loss: 43.9981117\n",
      "[2,    46] train loss: 44.6941700\n",
      "[2,    51] train loss: 40.3781090\n",
      "[2,    56] train loss: 45.0588748\n",
      "[2,    61] train loss: 47.6568139\n",
      "[2,    66] train loss: 44.3869247\n",
      "[2,    71] train loss: 42.9164124\n",
      "[2,    76] train loss: 45.8286298\n",
      "[2,    81] train loss: 43.7652238\n",
      "[2,    86] train loss: 42.3733482\n",
      "[2,    91] train loss: 45.1457005\n",
      "[2,    96] train loss: 46.1553472\n",
      "[2,   101] train loss: 41.4417788\n",
      "[2,   106] train loss: 48.5596631\n",
      "[2,   111] train loss: 44.3385881\n",
      "[2,   116] train loss: 43.2033049\n",
      "[2,   121] train loss: 38.2210395\n",
      "[2,   126] train loss: 47.1951160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-832e56053e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mbass_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnb_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_drum_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# loss = criterion(bass_outputs, batch_bass_train.long())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-cfbd66e66dc7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# пусть в input у нас приходит вектор размерности (64, 32, 14)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m37\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 526\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 72\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "    bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "        bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "        drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        bass_test = torch.tensor(bass_test, dtype=torch.float)\n",
    "        \n",
    "    examples_count = drum_train.size()[0]\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size,].transpose(0,1)\n",
    "        # transpose нужен для обмена размерности батча и размерности шагов\n",
    "#         print(f\"batch_drum_train:{batch_drum_train.size()}, batch_bass_train:{batch_bass_train.size()}\")\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_lstm(drum_test).squeeze()\n",
    "        \n",
    "        test_count = len(drum_test)\n",
    "        test_loss = 0\n",
    "        for k in range(test_count):\n",
    "            test_loss += reconstruction_loss(bass_outputs[k], bass_test[k])\n",
    "        print(f\"#{epoch + 1} reconstruction test loss: {test_loss/test_count}\")\n",
    "    \n",
    "\n",
    "#should check accuracy on validation set\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_validation).squeeze()\n",
    "\n",
    "    validation_count = len(drum_validation)\n",
    "    validation_loss = 0\n",
    "    for k in range(validation_count):\n",
    "        validation_loss += reconstruction_loss(bass_outputs[k], bass_validation[k])\n",
    "    print(f\"#{epoch + 1} reconstruction validation loss: {validation_loss/validation_count}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_drum_train = drum_train[:,:,:].transpose(0,1)\n",
    "batch_bass_train = bass_train[:,:].transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 6,  ..., 7, 7, 7],\n",
       "        [6, 7, 6,  ..., 7, 5, 7],\n",
       "        [7, 8, 6,  ..., 7, 7, 7],\n",
       "        ...,\n",
       "        [6, 7, 6,  ..., 7, 5, 8],\n",
       "        [7, 8, 6,  ..., 7, 6, 7],\n",
       "        [6, 7, 6,  ..., 8, 5, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "# вспомогательная функция для генерации midi\n",
    "# переводит посл-ть нот в \"картинку\"\n",
    "def seq_to_img(bass_seq):\n",
    "    bass_output = []\n",
    "    for bass_note in bass_seq:\n",
    "        bass_row = np.eye(1, 36, bass_note - 1)[0]\n",
    "        bass_output.append(bass_row)\n",
    "    bass_output = torch.tensor(bass_output).int().squeeze()\n",
    "    return bass_output\n",
    "\n",
    "converter = Converter((16,50))\n",
    "\n",
    "# batch_drum = torch.cat((drum_train, drum_test, torch.tensor(drum_validation))).transpose(0,1)\n",
    "# batch_bass = torch.cat((bass_train.int(), bass_test.int(), torch.tensor(bass_validation).int())).transpose(0,1)\n",
    "def output_midi(batch_drum, batch_bass, folder, output_original=False):\n",
    "    batch_drum = batch_drum.transpose(0,1)\n",
    "    batch_bass = batch_bass.int().transpose(0,1)\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_lstm(batch_drum)\n",
    "        bass_outputs = bass_outputs.squeeze().int()\n",
    "\n",
    "        for i in range(bass_outputs.size()[1]):\n",
    "            def output_seq(bass_seq, prefix=\"sample\"):\n",
    "                bass_output = seq_to_img(bass_seq)\n",
    "\n",
    "                img_dnb = torch.cat((batch_drum[:,i,:].int(),bass_output), axis=1)\n",
    "                numpy_pair = NumpyImage(np.array(img_dnb), 120, 1, 1, 36)\n",
    "                pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "                mid = build_track(pair, tempo=pair.tempo)\n",
    "                mid.save(f\"{folder}/{prefix}{i+1}.mid\")\n",
    "                \n",
    "            output_seq(bass_outputs[:,i])\n",
    "            \n",
    "            # TODO неплохо вынести генерацию оригинальной музыки в отдельный .py файл... т.е. нужно убрать этот костыль...\n",
    "            if output_original:\n",
    "                output_seq(batch_bass[:,i], \"orig/original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим обучающую и валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # если очень надо послушать тренировчную -- лучше её перезагрузить, потому что она перемешивается\n",
    "# drum, bass = data_conversion.make_lstm_dataset(height=16, patterns_file=\"decode_patterns/train.tsv\", mono=True)\n",
    "# drum = torch.tensor(drum, dtype=torch.float)\n",
    "# bass = torch.tensor(bass, dtype=torch.float)\n",
    "# output_midi(drum, bass, \"midi/lstm_mono/train\")\n",
    "output_midi(drum_validation, bass_validation, \"midi/lstm_mono/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вкусу, выводим тот же результат для кожанных мешков на ассесмент. На самом деле ничем от валидационной выборки не отличается :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hum, bass_hum = data_conversion.make_lstm_dataset(height=16, patterns_file=\"decode_patterns/human.tsv\", mono=True)\n",
    "drum_hum = torch.tensor(drum_hum, dtype=torch.float)\n",
    "bass_hum = torch.tensor(bass_hum, dtype=torch.float)\n",
    "output_midi(drum_hum, bass_hum, \"midi/lstm_mono/human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM на оригинальном датасете\n",
    "\n",
    "Попытка сделать монофонический выход из сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset(height=32, limit=1000, patterns_file=\"decode_patterns/patterns.pairs.tsv\", mono=True)\n",
    "\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(A)\n",
    "    idx = np.arange(L) < p*L\n",
    "    np.random.shuffle(idx)\n",
    "    yield A[idx]\n",
    "    yield B[idx]\n",
    "    yield A[np.logical_not(idx)]\n",
    "    yield B[np.logical_not(idx)]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  0,  0,  1,  0,  4,  0,  0,  1,  0,  6,  4,  0,  1,  0,\n",
       "        0,  1,  0,  0,  1,  0,  4,  0,  0,  1,  0,  4, 13,  0,  1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем определить модель LSTM как конечный автомат\n",
    "class DrumNBassLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBassLSTM, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.hidden_size = 34\n",
    "        self.layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.layer_count)\n",
    "        self.embed_layer = nn.Linear(self.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (64, 32, 14)\n",
    "        # то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.embed_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBassLSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 55.0815125\n",
      "[1,     5] train loss: 54.0247192\n",
      "Epoch #1\n",
      "[2,     1] train loss: 58.2123909\n",
      "[2,     5] train loss: 58.4379684\n",
      "Epoch #2\n",
      "[3,     1] train loss: 53.3883820\n",
      "[3,     5] train loss: 56.6465195\n",
      "Epoch #3\n",
      "[4,     1] train loss: 57.5456352\n",
      "[4,     5] train loss: 54.9832253\n",
      "Epoch #4\n",
      "[5,     1] train loss: 56.4686127\n",
      "[5,     5] train loss: 54.6455246\n",
      "Epoch #5\n",
      "[6,     1] train loss: 61.8999710\n",
      "[6,     5] train loss: 55.8028076\n",
      "Epoch #6\n",
      "[7,     1] train loss: 55.3449707\n",
      "[7,     5] train loss: 55.5373611\n",
      "Epoch #7\n",
      "[8,     1] train loss: 53.3062019\n",
      "[8,     5] train loss: 49.9073456\n",
      "Epoch #8\n",
      "[9,     1] train loss: 53.2670708\n",
      "[9,     5] train loss: 51.7305557\n",
      "Epoch #9\n",
      "[10,     1] train loss: 50.4468842\n",
      "[10,     5] train loss: 50.6780014\n",
      "Epoch #10\n",
      "[11,     1] train loss: 49.8167686\n",
      "[11,     5] train loss: 46.7362465\n",
      "Epoch #11\n",
      "[12,     1] train loss: 42.4862099\n",
      "[12,     5] train loss: 47.9254242\n",
      "Epoch #12\n",
      "[13,     1] train loss: 43.7688522\n",
      "[13,     5] train loss: 44.4078720\n",
      "Epoch #13\n",
      "[14,     1] train loss: 43.2510529\n",
      "[14,     5] train loss: 44.6102745\n",
      "Epoch #14\n",
      "[15,     1] train loss: 40.2559395\n",
      "[15,     5] train loss: 44.3918991\n",
      "Epoch #15\n",
      "[16,     1] train loss: 41.2371788\n",
      "[16,     5] train loss: 42.7729408\n",
      "Epoch #16\n",
      "[17,     1] train loss: 39.0725136\n",
      "[17,     5] train loss: 43.7915985\n",
      "Epoch #17\n",
      "[18,     1] train loss: 43.8863106\n",
      "[18,     5] train loss: 43.5288734\n",
      "Epoch #18\n",
      "[19,     1] train loss: 45.4213486\n",
      "[19,     5] train loss: 41.5170540\n",
      "Epoch #19\n",
      "[20,     1] train loss: 42.0856667\n",
      "[20,     5] train loss: 43.2555176\n",
      "Epoch #20\n",
      "[21,     1] train loss: 42.5988159\n",
      "[21,     5] train loss: 41.9506340\n",
      "Epoch #21\n",
      "[22,     1] train loss: 40.8662148\n",
      "[22,     5] train loss: 43.2385117\n",
      "Epoch #22\n",
      "[23,     1] train loss: 45.8304977\n",
      "[23,     5] train loss: 42.4959938\n",
      "Epoch #23\n",
      "[24,     1] train loss: 42.7924843\n",
      "[24,     5] train loss: 43.1688766\n",
      "Epoch #24\n",
      "[25,     1] train loss: 45.2735481\n",
      "[25,     5] train loss: 44.1596832\n",
      "Epoch #25\n",
      "[26,     1] train loss: 43.0719681\n",
      "[26,     5] train loss: 44.2542259\n",
      "Epoch #26\n",
      "[27,     1] train loss: 42.4616165\n",
      "[27,     5] train loss: 43.1667717\n",
      "Epoch #27\n",
      "[28,     1] train loss: 39.4669609\n",
      "[28,     5] train loss: 43.4801224\n",
      "Epoch #28\n",
      "[29,     1] train loss: 43.6318855\n",
      "[29,     5] train loss: 43.6138046\n",
      "Epoch #29\n",
      "[30,     1] train loss: 41.0204468\n",
      "[30,     5] train loss: 42.5239899\n",
      "Epoch #30\n",
      "[31,     1] train loss: 45.2571068\n",
      "[31,     5] train loss: 41.8400124\n",
      "Epoch #31\n",
      "[32,     1] train loss: 41.4676476\n",
      "[32,     5] train loss: 42.2361595\n",
      "Epoch #32\n",
      "[33,     1] train loss: 42.7446632\n",
      "[33,     5] train loss: 42.7038414\n",
      "Epoch #33\n",
      "[34,     1] train loss: 39.4066925\n",
      "[34,     5] train loss: 40.8014183\n",
      "Epoch #34\n",
      "[35,     1] train loss: 36.6027718\n",
      "[35,     5] train loss: 42.7981094\n",
      "Epoch #35\n",
      "[36,     1] train loss: 42.6210098\n",
      "[36,     5] train loss: 43.1038139\n",
      "Epoch #36\n",
      "[37,     1] train loss: 43.1704483\n",
      "[37,     5] train loss: 41.8364677\n",
      "Epoch #37\n",
      "[38,     1] train loss: 46.1528091\n",
      "[38,     5] train loss: 41.3251785\n",
      "Epoch #38\n",
      "[39,     1] train loss: 40.3942604\n",
      "[39,     5] train loss: 44.8057098\n",
      "Epoch #39\n",
      "[40,     1] train loss: 41.8252182\n",
      "[40,     5] train loss: 44.8220940\n",
      "Epoch #40\n",
      "[41,     1] train loss: 41.9430580\n",
      "[41,     5] train loss: 42.9154633\n",
      "Epoch #41\n",
      "[42,     1] train loss: 43.8114777\n",
      "[42,     5] train loss: 40.7777618\n",
      "Epoch #42\n",
      "[43,     1] train loss: 42.3486252\n",
      "[43,     5] train loss: 42.9221443\n",
      "Epoch #43\n",
      "[44,     1] train loss: 45.1600685\n",
      "[44,     5] train loss: 43.1512901\n",
      "Epoch #44\n",
      "[45,     1] train loss: 40.8686600\n",
      "[45,     5] train loss: 40.8243538\n",
      "Epoch #45\n",
      "[46,     1] train loss: 37.2045708\n",
      "[46,     5] train loss: 43.1496132\n",
      "Epoch #46\n",
      "[47,     1] train loss: 46.3886070\n",
      "[47,     5] train loss: 41.8435364\n",
      "Epoch #47\n",
      "[48,     1] train loss: 42.5394897\n",
      "[48,     5] train loss: 42.9161537\n",
      "Epoch #48\n",
      "[49,     1] train loss: 44.7910690\n",
      "[49,     5] train loss: 41.4714607\n",
      "Epoch #49\n",
      "[50,     1] train loss: 45.4804306\n",
      "[50,     5] train loss: 43.2499802\n",
      "Epoch #50\n",
      "[51,     1] train loss: 43.3880119\n",
      "[51,     5] train loss: 43.8354416\n",
      "Epoch #51\n",
      "[52,     1] train loss: 41.0459900\n",
      "[52,     5] train loss: 42.6884331\n",
      "Epoch #52\n",
      "[53,     1] train loss: 44.6966591\n",
      "[53,     5] train loss: 43.2567871\n",
      "Epoch #53\n",
      "[54,     1] train loss: 41.4954720\n",
      "[54,     5] train loss: 42.9523224\n",
      "Epoch #54\n",
      "[55,     1] train loss: 41.7682915\n",
      "[55,     5] train loss: 42.2681252\n",
      "Epoch #55\n",
      "[56,     1] train loss: 44.1120148\n",
      "[56,     5] train loss: 42.5128654\n",
      "Epoch #56\n",
      "[57,     1] train loss: 41.1571350\n",
      "[57,     5] train loss: 42.5222771\n",
      "Epoch #57\n",
      "[58,     1] train loss: 44.6306496\n",
      "[58,     5] train loss: 41.2978531\n",
      "Epoch #58\n",
      "[59,     1] train loss: 37.8381958\n",
      "[59,     5] train loss: 40.1522606\n",
      "Epoch #59\n",
      "[60,     1] train loss: 43.8794365\n",
      "[60,     5] train loss: 42.4210495\n",
      "Epoch #60\n",
      "[61,     1] train loss: 39.6554260\n",
      "[61,     5] train loss: 42.7051788\n",
      "Epoch #61\n",
      "[62,     1] train loss: 39.4763489\n",
      "[62,     5] train loss: 40.5685204\n",
      "Epoch #62\n",
      "[63,     1] train loss: 45.5572052\n",
      "[63,     5] train loss: 41.9772507\n",
      "Epoch #63\n",
      "[64,     1] train loss: 43.1360359\n",
      "[64,     5] train loss: 41.4795509\n",
      "Epoch #64\n",
      "[65,     1] train loss: 42.4336777\n",
      "[65,     5] train loss: 41.4316406\n",
      "Epoch #65\n",
      "[66,     1] train loss: 43.5212517\n",
      "[66,     5] train loss: 41.4261246\n",
      "Epoch #66\n",
      "[67,     1] train loss: 36.7924194\n",
      "[67,     5] train loss: 42.5124123\n",
      "Epoch #67\n",
      "[68,     1] train loss: 39.9920197\n",
      "[68,     5] train loss: 40.8969841\n",
      "Epoch #68\n",
      "[69,     1] train loss: 41.7930832\n",
      "[69,     5] train loss: 42.9340569\n",
      "Epoch #69\n",
      "[70,     1] train loss: 40.3092194\n",
      "[70,     5] train loss: 39.6141838\n",
      "Epoch #70\n",
      "[71,     1] train loss: 45.5753136\n",
      "[71,     5] train loss: 40.5431740\n",
      "Epoch #71\n",
      "[72,     1] train loss: 39.2716637\n",
      "[72,     5] train loss: 40.9893539\n",
      "Epoch #72\n",
      "[73,     1] train loss: 41.6344185\n",
      "[73,     5] train loss: 43.3706047\n",
      "Epoch #73\n",
      "[74,     1] train loss: 41.6858673\n",
      "[74,     5] train loss: 41.3130440\n",
      "Epoch #74\n",
      "[75,     1] train loss: 45.2617226\n",
      "[75,     5] train loss: 40.8097687\n",
      "Epoch #75\n",
      "[76,     1] train loss: 39.5288467\n",
      "[76,     5] train loss: 43.2731613\n",
      "Epoch #76\n",
      "[77,     1] train loss: 41.9818382\n",
      "[77,     5] train loss: 39.8822334\n",
      "Epoch #77\n",
      "[78,     1] train loss: 38.9693413\n",
      "[78,     5] train loss: 39.2402115\n",
      "Epoch #78\n",
      "[79,     1] train loss: 38.6253204\n",
      "[79,     5] train loss: 38.2367081\n",
      "Epoch #79\n",
      "[80,     1] train loss: 37.3133736\n",
      "[80,     5] train loss: 42.3850319\n",
      "Epoch #80\n",
      "[81,     1] train loss: 41.1951790\n",
      "[81,     5] train loss: 41.8377899\n",
      "Epoch #81\n",
      "[82,     1] train loss: 40.9601974\n",
      "[82,     5] train loss: 39.2534523\n",
      "Epoch #82\n",
      "[83,     1] train loss: 43.3842239\n",
      "[83,     5] train loss: 41.5388283\n",
      "Epoch #83\n",
      "[84,     1] train loss: 45.3206635\n",
      "[84,     5] train loss: 42.1539108\n",
      "Epoch #84\n",
      "[85,     1] train loss: 38.9847298\n",
      "[85,     5] train loss: 41.9630058\n",
      "Epoch #85\n",
      "[86,     1] train loss: 40.2358665\n",
      "[86,     5] train loss: 41.5713417\n",
      "Epoch #86\n",
      "[87,     1] train loss: 44.4443321\n",
      "[87,     5] train loss: 41.4502213\n",
      "Epoch #87\n",
      "[88,     1] train loss: 43.8039589\n",
      "[88,     5] train loss: 37.8724808\n",
      "Epoch #88\n",
      "[89,     1] train loss: 40.3932037\n",
      "[89,     5] train loss: 38.4532211\n",
      "Epoch #89\n",
      "[90,     1] train loss: 37.6708488\n",
      "[90,     5] train loss: 39.7732353\n",
      "Epoch #90\n",
      "[91,     1] train loss: 39.0152969\n",
      "[91,     5] train loss: 41.4083694\n",
      "Epoch #91\n",
      "[92,     1] train loss: 37.8278389\n",
      "[92,     5] train loss: 41.3099464\n",
      "Epoch #92\n",
      "[93,     1] train loss: 38.3892479\n",
      "[93,     5] train loss: 40.4780807\n",
      "Epoch #93\n",
      "[94,     1] train loss: 41.1958618\n",
      "[94,     5] train loss: 37.8118164\n",
      "Epoch #94\n",
      "[95,     1] train loss: 35.7535248\n",
      "[95,     5] train loss: 41.3114777\n",
      "Epoch #95\n",
      "[96,     1] train loss: 38.5852928\n",
      "[96,     5] train loss: 42.0206757\n",
      "Epoch #96\n",
      "[97,     1] train loss: 40.1299095\n",
      "[97,     5] train loss: 39.4707916\n",
      "Epoch #97\n",
      "[98,     1] train loss: 40.9636116\n",
      "[98,     5] train loss: 41.9854248\n",
      "Epoch #98\n",
      "[99,     1] train loss: 39.9299202\n",
      "[99,     5] train loss: 39.4062103\n",
      "Epoch #99\n",
      "[100,     1] train loss: 40.3121452\n",
      "[100,     5] train loss: 37.8462250\n",
      "Epoch #100\n",
      "[101,     1] train loss: 37.1286507\n",
      "[101,     5] train loss: 41.0252312\n",
      "Epoch #101\n",
      "[102,     1] train loss: 43.1704407\n",
      "[102,     5] train loss: 41.3467407\n",
      "Epoch #102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,     1] train loss: 37.3017845\n",
      "[103,     5] train loss: 40.6891533\n",
      "Epoch #103\n",
      "[104,     1] train loss: 38.1578369\n",
      "[104,     5] train loss: 41.4986488\n",
      "Epoch #104\n",
      "[105,     1] train loss: 41.4369926\n",
      "[105,     5] train loss: 40.3532227\n",
      "Epoch #105\n",
      "[106,     1] train loss: 38.2214775\n",
      "[106,     5] train loss: 41.1353477\n",
      "Epoch #106\n",
      "[107,     1] train loss: 36.0690155\n",
      "[107,     5] train loss: 41.3651764\n",
      "Epoch #107\n",
      "[108,     1] train loss: 37.2239189\n",
      "[108,     5] train loss: 39.8700073\n",
      "Epoch #108\n",
      "[109,     1] train loss: 39.8099709\n",
      "[109,     5] train loss: 41.7670700\n",
      "Epoch #109\n",
      "[110,     1] train loss: 39.7547951\n",
      "[110,     5] train loss: 40.4069702\n",
      "Epoch #110\n",
      "[111,     1] train loss: 36.1549530\n",
      "[111,     5] train loss: 37.9439552\n",
      "Epoch #111\n",
      "[112,     1] train loss: 39.8429451\n",
      "[112,     5] train loss: 39.6974319\n",
      "Epoch #112\n",
      "[113,     1] train loss: 42.8182297\n",
      "[113,     5] train loss: 38.9426468\n",
      "Epoch #113\n",
      "[114,     1] train loss: 41.3066177\n",
      "[114,     5] train loss: 39.8327782\n",
      "Epoch #114\n",
      "[115,     1] train loss: 37.1807327\n",
      "[115,     5] train loss: 40.2371986\n",
      "Epoch #115\n",
      "[116,     1] train loss: 38.7204399\n",
      "[116,     5] train loss: 39.6059387\n",
      "Epoch #116\n",
      "[117,     1] train loss: 41.4907951\n",
      "[117,     5] train loss: 38.2283401\n",
      "Epoch #117\n",
      "[118,     1] train loss: 41.7221031\n",
      "[118,     5] train loss: 37.6966232\n",
      "Epoch #118\n",
      "[119,     1] train loss: 44.4943733\n",
      "[119,     5] train loss: 38.2186226\n",
      "Epoch #119\n",
      "[120,     1] train loss: 36.7049065\n",
      "[120,     5] train loss: 40.1934471\n",
      "Epoch #120\n",
      "[121,     1] train loss: 38.7763062\n",
      "[121,     5] train loss: 38.5659912\n",
      "Epoch #121\n",
      "[122,     1] train loss: 38.9257088\n",
      "[122,     5] train loss: 39.5236641\n",
      "Epoch #122\n",
      "[123,     1] train loss: 42.3338013\n",
      "[123,     5] train loss: 41.1889198\n",
      "Epoch #123\n",
      "[124,     1] train loss: 40.3222122\n",
      "[124,     5] train loss: 37.8655830\n",
      "Epoch #124\n",
      "[125,     1] train loss: 42.5767212\n",
      "[125,     5] train loss: 39.4967102\n",
      "Epoch #125\n",
      "[126,     1] train loss: 35.7945976\n",
      "[126,     5] train loss: 38.4884888\n",
      "Epoch #126\n",
      "[127,     1] train loss: 37.4485207\n",
      "[127,     5] train loss: 40.0811066\n",
      "Epoch #127\n",
      "[128,     1] train loss: 40.1659088\n",
      "[128,     5] train loss: 38.6350922\n",
      "Epoch #128\n",
      "[129,     1] train loss: 42.6095695\n",
      "[129,     5] train loss: 38.3711281\n",
      "Epoch #129\n",
      "[130,     1] train loss: 36.7410088\n",
      "[130,     5] train loss: 39.6830383\n",
      "Epoch #130\n",
      "[131,     1] train loss: 40.1212921\n",
      "[131,     5] train loss: 38.8584023\n",
      "Epoch #131\n",
      "[132,     1] train loss: 38.2699661\n",
      "[132,     5] train loss: 40.0524872\n",
      "Epoch #132\n",
      "[133,     1] train loss: 43.3344650\n",
      "[133,     5] train loss: 38.0379440\n",
      "Epoch #133\n",
      "[134,     1] train loss: 36.0876465\n",
      "[134,     5] train loss: 39.3789978\n",
      "Epoch #134\n",
      "[135,     1] train loss: 42.9207726\n",
      "[135,     5] train loss: 41.4867973\n",
      "Epoch #135\n",
      "[136,     1] train loss: 38.3343086\n",
      "[136,     5] train loss: 39.7060806\n",
      "Epoch #136\n",
      "[137,     1] train loss: 42.3079529\n",
      "[137,     5] train loss: 37.7704765\n",
      "Epoch #137\n",
      "[138,     1] train loss: 36.2288628\n",
      "[138,     5] train loss: 40.8058350\n",
      "Epoch #138\n",
      "[139,     1] train loss: 40.4282837\n",
      "[139,     5] train loss: 40.9096977\n",
      "Epoch #139\n",
      "[140,     1] train loss: 42.9147301\n",
      "[140,     5] train loss: 39.1132568\n",
      "Epoch #140\n",
      "[141,     1] train loss: 37.5455475\n",
      "[141,     5] train loss: 40.1468010\n",
      "Epoch #141\n",
      "[142,     1] train loss: 38.2779770\n",
      "[142,     5] train loss: 41.2348778\n",
      "Epoch #142\n",
      "[143,     1] train loss: 42.9230690\n",
      "[143,     5] train loss: 39.5616257\n",
      "Epoch #143\n",
      "[144,     1] train loss: 36.1905022\n",
      "[144,     5] train loss: 38.1575958\n",
      "Epoch #144\n",
      "[145,     1] train loss: 37.7341232\n",
      "[145,     5] train loss: 40.1363129\n",
      "Epoch #145\n",
      "[146,     1] train loss: 40.5827827\n",
      "[146,     5] train loss: 38.5265106\n",
      "Epoch #146\n",
      "[147,     1] train loss: 36.3597641\n",
      "[147,     5] train loss: 39.2288101\n",
      "Epoch #147\n",
      "[148,     1] train loss: 40.0954666\n",
      "[148,     5] train loss: 38.8457031\n",
      "Epoch #148\n",
      "[149,     1] train loss: 36.2930145\n",
      "[149,     5] train loss: 40.2436096\n",
      "Epoch #149\n",
      "[150,     1] train loss: 40.9777679\n",
      "[150,     5] train loss: 37.7751328\n",
      "Epoch #150\n",
      "[151,     1] train loss: 34.6777573\n",
      "[151,     5] train loss: 38.4835518\n",
      "Epoch #151\n",
      "[152,     1] train loss: 38.1736450\n",
      "[152,     5] train loss: 38.0240623\n",
      "Epoch #152\n",
      "[153,     1] train loss: 43.2939262\n",
      "[153,     5] train loss: 41.2052719\n",
      "Epoch #153\n",
      "[154,     1] train loss: 36.1208763\n",
      "[154,     5] train loss: 40.3573135\n",
      "Epoch #154\n",
      "[155,     1] train loss: 39.2418633\n",
      "[155,     5] train loss: 39.5609306\n",
      "Epoch #155\n",
      "[156,     1] train loss: 40.2886505\n",
      "[156,     5] train loss: 40.6483047\n",
      "Epoch #156\n",
      "[157,     1] train loss: 38.5386429\n",
      "[157,     5] train loss: 38.3654160\n",
      "Epoch #157\n",
      "[158,     1] train loss: 34.2664642\n",
      "[158,     5] train loss: 39.3338882\n",
      "Epoch #158\n",
      "[159,     1] train loss: 41.1087112\n",
      "[159,     5] train loss: 38.5090660\n",
      "Epoch #159\n",
      "[160,     1] train loss: 39.0876465\n",
      "[160,     5] train loss: 39.1359512\n",
      "Epoch #160\n",
      "[161,     1] train loss: 40.5741768\n",
      "[161,     5] train loss: 36.9725914\n",
      "Epoch #161\n",
      "[162,     1] train loss: 43.1784897\n",
      "[162,     5] train loss: 39.1963287\n",
      "Epoch #162\n",
      "[163,     1] train loss: 41.7627411\n",
      "[163,     5] train loss: 39.2841896\n",
      "Epoch #163\n",
      "[164,     1] train loss: 42.1612511\n",
      "[164,     5] train loss: 38.8992828\n",
      "Epoch #164\n",
      "[165,     1] train loss: 40.0103378\n",
      "[165,     5] train loss: 39.5399284\n",
      "Epoch #165\n",
      "[166,     1] train loss: 40.8686371\n",
      "[166,     5] train loss: 38.4658836\n",
      "Epoch #166\n",
      "[167,     1] train loss: 41.1494865\n",
      "[167,     5] train loss: 39.2506119\n",
      "Epoch #167\n",
      "[168,     1] train loss: 33.2628670\n",
      "[168,     5] train loss: 39.4460808\n",
      "Epoch #168\n",
      "[169,     1] train loss: 35.4563408\n",
      "[169,     5] train loss: 38.1230598\n",
      "Epoch #169\n",
      "[170,     1] train loss: 38.1881447\n",
      "[170,     5] train loss: 37.9972580\n",
      "Epoch #170\n",
      "[171,     1] train loss: 35.6437263\n",
      "[171,     5] train loss: 38.8363518\n",
      "Epoch #171\n",
      "[172,     1] train loss: 40.4306984\n",
      "[172,     5] train loss: 40.2721535\n",
      "Epoch #172\n",
      "[173,     1] train loss: 40.9907761\n",
      "[173,     5] train loss: 38.8158363\n",
      "Epoch #173\n",
      "[174,     1] train loss: 41.4935112\n",
      "[174,     5] train loss: 39.1706268\n",
      "Epoch #174\n",
      "[175,     1] train loss: 36.0460129\n",
      "[175,     5] train loss: 36.6342705\n",
      "Epoch #175\n",
      "[176,     1] train loss: 36.5650864\n",
      "[176,     5] train loss: 37.2498894\n",
      "Epoch #176\n",
      "[177,     1] train loss: 34.5463181\n",
      "[177,     5] train loss: 39.2193817\n",
      "Epoch #177\n",
      "[178,     1] train loss: 38.6714020\n",
      "[178,     5] train loss: 40.3425117\n",
      "Epoch #178\n",
      "[179,     1] train loss: 41.2141914\n",
      "[179,     5] train loss: 38.3187431\n",
      "Epoch #179\n",
      "[180,     1] train loss: 42.1700401\n",
      "[180,     5] train loss: 36.5989616\n",
      "Epoch #180\n",
      "[181,     1] train loss: 40.1585693\n",
      "[181,     5] train loss: 36.8088486\n",
      "Epoch #181\n",
      "[182,     1] train loss: 44.0323715\n",
      "[182,     5] train loss: 37.7800148\n",
      "Epoch #182\n",
      "[183,     1] train loss: 39.3744812\n",
      "[183,     5] train loss: 37.3520630\n",
      "Epoch #183\n",
      "[184,     1] train loss: 39.6299210\n",
      "[184,     5] train loss: 38.4373466\n",
      "Epoch #184\n",
      "[185,     1] train loss: 40.6250839\n",
      "[185,     5] train loss: 39.1576965\n",
      "Epoch #185\n",
      "[186,     1] train loss: 43.8918304\n",
      "[186,     5] train loss: 37.1846077\n",
      "Epoch #186\n",
      "[187,     1] train loss: 40.1780815\n",
      "[187,     5] train loss: 38.9569176\n",
      "Epoch #187\n",
      "[188,     1] train loss: 35.4378510\n",
      "[188,     5] train loss: 36.9944832\n",
      "Epoch #188\n",
      "[189,     1] train loss: 37.2971191\n",
      "[189,     5] train loss: 37.5375877\n",
      "Epoch #189\n",
      "[190,     1] train loss: 38.5294647\n",
      "[190,     5] train loss: 40.1149025\n",
      "Epoch #190\n",
      "[191,     1] train loss: 40.3426628\n",
      "[191,     5] train loss: 38.1981567\n",
      "Epoch #191\n",
      "[192,     1] train loss: 37.8779945\n",
      "[192,     5] train loss: 37.9521355\n",
      "Epoch #192\n",
      "[193,     1] train loss: 40.2241058\n",
      "[193,     5] train loss: 35.9011368\n",
      "Epoch #193\n",
      "[194,     1] train loss: 36.8442955\n",
      "[194,     5] train loss: 37.9398109\n",
      "Epoch #194\n",
      "[195,     1] train loss: 40.8639297\n",
      "[195,     5] train loss: 39.3741867\n",
      "Epoch #195\n",
      "[196,     1] train loss: 39.6751633\n",
      "[196,     5] train loss: 36.8416077\n",
      "Epoch #196\n",
      "[197,     1] train loss: 38.8263321\n",
      "[197,     5] train loss: 36.2495193\n",
      "Epoch #197\n",
      "[198,     1] train loss: 43.5049667\n",
      "[198,     5] train loss: 37.9128418\n",
      "Epoch #198\n",
      "[199,     1] train loss: 41.0143166\n",
      "[199,     5] train loss: 35.8027443\n",
      "Epoch #199\n",
      "[200,     1] train loss: 36.8165016\n",
      "[200,     5] train loss: 37.1487274\n",
      "Epoch #200\n",
      "[201,     1] train loss: 40.3670807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201,     5] train loss: 40.0135132\n",
      "Epoch #201\n",
      "[202,     1] train loss: 40.7952499\n",
      "[202,     5] train loss: 36.3156029\n",
      "Epoch #202\n",
      "[203,     1] train loss: 38.4012680\n",
      "[203,     5] train loss: 38.4030685\n",
      "Epoch #203\n",
      "[204,     1] train loss: 39.1867142\n",
      "[204,     5] train loss: 37.7161819\n",
      "Epoch #204\n",
      "[205,     1] train loss: 35.5377617\n",
      "[205,     5] train loss: 37.7814430\n",
      "Epoch #205\n",
      "[206,     1] train loss: 41.8268089\n",
      "[206,     5] train loss: 37.9608269\n",
      "Epoch #206\n",
      "[207,     1] train loss: 37.3451729\n",
      "[207,     5] train loss: 39.3686737\n",
      "Epoch #207\n",
      "[208,     1] train loss: 37.3448830\n",
      "[208,     5] train loss: 36.7835831\n",
      "Epoch #208\n",
      "[209,     1] train loss: 41.0972366\n",
      "[209,     5] train loss: 38.6580109\n",
      "Epoch #209\n",
      "[210,     1] train loss: 37.8476562\n",
      "[210,     5] train loss: 37.2570038\n",
      "Epoch #210\n",
      "[211,     1] train loss: 36.1445122\n",
      "[211,     5] train loss: 38.1028534\n",
      "Epoch #211\n",
      "[212,     1] train loss: 37.2768288\n",
      "[212,     5] train loss: 38.9222847\n",
      "Epoch #212\n",
      "[213,     1] train loss: 43.7220345\n",
      "[213,     5] train loss: 36.7051476\n",
      "Epoch #213\n",
      "[214,     1] train loss: 39.6861343\n",
      "[214,     5] train loss: 36.8043449\n",
      "Epoch #214\n",
      "[215,     1] train loss: 38.6405945\n",
      "[215,     5] train loss: 38.9127113\n",
      "Epoch #215\n",
      "[216,     1] train loss: 36.7230606\n",
      "[216,     5] train loss: 38.1369423\n",
      "Epoch #216\n",
      "[217,     1] train loss: 40.2525520\n",
      "[217,     5] train loss: 37.5819046\n",
      "Epoch #217\n",
      "[218,     1] train loss: 37.9804230\n",
      "[218,     5] train loss: 37.7091187\n",
      "Epoch #218\n",
      "[219,     1] train loss: 34.5533829\n",
      "[219,     5] train loss: 37.7512733\n",
      "Epoch #219\n",
      "[220,     1] train loss: 38.9645004\n",
      "[220,     5] train loss: 36.7048500\n",
      "Epoch #220\n",
      "[221,     1] train loss: 38.6187935\n",
      "[221,     5] train loss: 37.3104851\n",
      "Epoch #221\n",
      "[222,     1] train loss: 35.3345871\n",
      "[222,     5] train loss: 37.6200249\n",
      "Epoch #222\n",
      "[223,     1] train loss: 33.5227165\n",
      "[223,     5] train loss: 38.9072502\n",
      "Epoch #223\n",
      "[224,     1] train loss: 40.1951675\n",
      "[224,     5] train loss: 37.2635460\n",
      "Epoch #224\n",
      "[225,     1] train loss: 36.5434799\n",
      "[225,     5] train loss: 38.2596268\n",
      "Epoch #225\n",
      "[226,     1] train loss: 35.1568069\n",
      "[226,     5] train loss: 38.1191551\n",
      "Epoch #226\n",
      "[227,     1] train loss: 40.2226105\n",
      "[227,     5] train loss: 35.2950668\n",
      "Epoch #227\n",
      "[228,     1] train loss: 40.4871979\n",
      "[228,     5] train loss: 35.4321190\n",
      "Epoch #228\n",
      "[229,     1] train loss: 34.5594673\n",
      "[229,     5] train loss: 37.1325661\n",
      "Epoch #229\n",
      "[230,     1] train loss: 38.1284370\n",
      "[230,     5] train loss: 37.5633011\n",
      "Epoch #230\n",
      "[231,     1] train loss: 38.0421371\n",
      "[231,     5] train loss: 37.0322578\n",
      "Epoch #231\n",
      "[232,     1] train loss: 37.8999672\n",
      "[232,     5] train loss: 37.8512917\n",
      "Epoch #232\n",
      "[233,     1] train loss: 42.1365700\n",
      "[233,     5] train loss: 36.7662384\n",
      "Epoch #233\n",
      "[234,     1] train loss: 38.4101791\n",
      "[234,     5] train loss: 37.5593292\n",
      "Epoch #234\n",
      "[235,     1] train loss: 39.7625618\n",
      "[235,     5] train loss: 36.5899529\n",
      "Epoch #235\n",
      "[236,     1] train loss: 36.9746437\n",
      "[236,     5] train loss: 39.3348991\n",
      "Epoch #236\n",
      "[237,     1] train loss: 40.9205894\n",
      "[237,     5] train loss: 36.8452263\n",
      "Epoch #237\n",
      "[238,     1] train loss: 37.0560913\n",
      "[238,     5] train loss: 38.5391304\n",
      "Epoch #238\n",
      "[239,     1] train loss: 41.9692802\n",
      "[239,     5] train loss: 39.1315666\n",
      "Epoch #239\n",
      "[240,     1] train loss: 40.9340630\n",
      "[240,     5] train loss: 38.3041924\n",
      "Epoch #240\n",
      "[241,     1] train loss: 40.6554451\n",
      "[241,     5] train loss: 36.2276413\n",
      "Epoch #241\n",
      "[242,     1] train loss: 36.9817619\n",
      "[242,     5] train loss: 37.0321419\n",
      "Epoch #242\n",
      "[243,     1] train loss: 41.0569649\n",
      "[243,     5] train loss: 36.5332306\n",
      "Epoch #243\n",
      "[244,     1] train loss: 38.4775047\n",
      "[244,     5] train loss: 37.9040909\n",
      "Epoch #244\n",
      "[245,     1] train loss: 38.3724785\n",
      "[245,     5] train loss: 40.3110161\n",
      "Epoch #245\n",
      "[246,     1] train loss: 42.1617241\n",
      "[246,     5] train loss: 37.8469025\n",
      "Epoch #246\n",
      "[247,     1] train loss: 38.0256004\n",
      "[247,     5] train loss: 38.7941345\n",
      "Epoch #247\n",
      "[248,     1] train loss: 38.2414932\n",
      "[248,     5] train loss: 35.7717636\n",
      "Epoch #248\n",
      "[249,     1] train loss: 39.3854370\n",
      "[249,     5] train loss: 37.4382324\n",
      "Epoch #249\n",
      "[250,     1] train loss: 38.8290863\n",
      "[250,     5] train loss: 37.2273643\n",
      "Epoch #250\n",
      "[251,     1] train loss: 42.1959953\n",
      "[251,     5] train loss: 37.0569214\n",
      "Epoch #251\n",
      "[252,     1] train loss: 37.8733368\n",
      "[252,     5] train loss: 38.6563972\n",
      "Epoch #252\n",
      "[253,     1] train loss: 38.6757622\n",
      "[253,     5] train loss: 37.2349205\n",
      "Epoch #253\n",
      "[254,     1] train loss: 35.6242638\n",
      "[254,     5] train loss: 38.1267815\n",
      "Epoch #254\n",
      "[255,     1] train loss: 38.8075905\n",
      "[255,     5] train loss: 36.0370903\n",
      "Epoch #255\n",
      "[256,     1] train loss: 42.0772247\n",
      "[256,     5] train loss: 36.7374710\n",
      "Epoch #256\n",
      "[257,     1] train loss: 37.3313484\n",
      "[257,     5] train loss: 37.0351746\n",
      "Epoch #257\n",
      "[258,     1] train loss: 41.1849480\n",
      "[258,     5] train loss: 37.2743027\n",
      "Epoch #258\n",
      "[259,     1] train loss: 40.3265915\n",
      "[259,     5] train loss: 38.4372139\n",
      "Epoch #259\n",
      "[260,     1] train loss: 38.5133781\n",
      "[260,     5] train loss: 37.5512947\n",
      "Epoch #260\n",
      "[261,     1] train loss: 41.2400246\n",
      "[261,     5] train loss: 35.0559731\n",
      "Epoch #261\n",
      "[262,     1] train loss: 42.4598427\n",
      "[262,     5] train loss: 38.7305336\n",
      "Epoch #262\n",
      "[263,     1] train loss: 39.8090248\n",
      "[263,     5] train loss: 36.3967781\n",
      "Epoch #263\n",
      "[264,     1] train loss: 37.5086060\n",
      "[264,     5] train loss: 36.4370132\n",
      "Epoch #264\n",
      "[265,     1] train loss: 39.9024353\n",
      "[265,     5] train loss: 38.1228966\n",
      "Epoch #265\n",
      "[266,     1] train loss: 36.7001572\n",
      "[266,     5] train loss: 37.5749374\n",
      "Epoch #266\n",
      "[267,     1] train loss: 40.0236244\n",
      "[267,     5] train loss: 37.3468277\n",
      "Epoch #267\n",
      "[268,     1] train loss: 37.2582054\n",
      "[268,     5] train loss: 38.1611534\n",
      "Epoch #268\n",
      "[269,     1] train loss: 39.6716766\n",
      "[269,     5] train loss: 35.5917068\n",
      "Epoch #269\n",
      "[270,     1] train loss: 38.7883072\n",
      "[270,     5] train loss: 36.9865997\n",
      "Epoch #270\n",
      "[271,     1] train loss: 41.9414940\n",
      "[271,     5] train loss: 36.3693199\n",
      "Epoch #271\n",
      "[272,     1] train loss: 40.0101852\n",
      "[272,     5] train loss: 36.7687737\n",
      "Epoch #272\n",
      "[273,     1] train loss: 35.7210083\n",
      "[273,     5] train loss: 37.3708252\n",
      "Epoch #273\n",
      "[274,     1] train loss: 40.1727066\n",
      "[274,     5] train loss: 36.0993324\n",
      "Epoch #274\n",
      "[275,     1] train loss: 38.7895775\n",
      "[275,     5] train loss: 36.8007233\n",
      "Epoch #275\n",
      "[276,     1] train loss: 33.3027992\n",
      "[276,     5] train loss: 36.6358353\n",
      "Epoch #276\n",
      "[277,     1] train loss: 42.9189148\n",
      "[277,     5] train loss: 38.1400948\n",
      "Epoch #277\n",
      "[278,     1] train loss: 42.8287773\n",
      "[278,     5] train loss: 36.8705025\n",
      "Epoch #278\n",
      "[279,     1] train loss: 38.5164413\n",
      "[279,     5] train loss: 36.2321480\n",
      "Epoch #279\n",
      "[280,     1] train loss: 41.4728851\n",
      "[280,     5] train loss: 37.8571663\n",
      "Epoch #280\n",
      "[281,     1] train loss: 43.6039772\n",
      "[281,     5] train loss: 36.1098328\n",
      "Epoch #281\n",
      "[282,     1] train loss: 40.1618462\n",
      "[282,     5] train loss: 36.0646729\n",
      "Epoch #282\n",
      "[283,     1] train loss: 38.0380516\n",
      "[283,     5] train loss: 35.4618385\n",
      "Epoch #283\n",
      "[284,     1] train loss: 37.6401329\n",
      "[284,     5] train loss: 36.1489349\n",
      "Epoch #284\n",
      "[285,     1] train loss: 39.9703407\n",
      "[285,     5] train loss: 36.5705788\n",
      "Epoch #285\n",
      "[286,     1] train loss: 38.5552025\n",
      "[286,     5] train loss: 36.5328468\n",
      "Epoch #286\n",
      "[287,     1] train loss: 34.6219101\n",
      "[287,     5] train loss: 38.7689201\n",
      "Epoch #287\n",
      "[288,     1] train loss: 39.2140617\n",
      "[288,     5] train loss: 36.9812691\n",
      "Epoch #288\n",
      "[289,     1] train loss: 39.3216591\n",
      "[289,     5] train loss: 36.7267670\n",
      "Epoch #289\n",
      "[290,     1] train loss: 35.5773087\n",
      "[290,     5] train loss: 37.0408119\n",
      "Epoch #290\n",
      "[291,     1] train loss: 41.6693192\n",
      "[291,     5] train loss: 36.8674797\n",
      "Epoch #291\n",
      "[292,     1] train loss: 36.9938393\n",
      "[292,     5] train loss: 36.6972847\n",
      "Epoch #292\n",
      "[293,     1] train loss: 37.4181595\n",
      "[293,     5] train loss: 36.6624870\n",
      "Epoch #293\n",
      "[294,     1] train loss: 36.1929283\n",
      "[294,     5] train loss: 38.0366699\n",
      "Epoch #294\n",
      "[295,     1] train loss: 35.9685478\n",
      "[295,     5] train loss: 37.4220360\n",
      "Epoch #295\n",
      "[296,     1] train loss: 40.1810074\n",
      "[296,     5] train loss: 35.4294807\n",
      "Epoch #296\n",
      "[297,     1] train loss: 39.7651100\n",
      "[297,     5] train loss: 35.5119621\n",
      "Epoch #297\n",
      "[298,     1] train loss: 40.5707283\n",
      "[298,     5] train loss: 37.6306000\n",
      "Epoch #298\n",
      "[299,     1] train loss: 39.9573250\n",
      "[299,     5] train loss: 36.8831635\n",
      "Epoch #299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300,     1] train loss: 39.3946075\n",
      "[300,     5] train loss: 36.2664185\n",
      "Epoch #300\n",
      "[301,     1] train loss: 36.3524704\n",
      "[301,     5] train loss: 36.0679825\n",
      "Epoch #301\n",
      "[302,     1] train loss: 41.3061256\n",
      "[302,     5] train loss: 37.0436882\n",
      "Epoch #302\n",
      "[303,     1] train loss: 38.0026741\n",
      "[303,     5] train loss: 36.6489006\n",
      "Epoch #303\n",
      "[304,     1] train loss: 40.7772713\n",
      "[304,     5] train loss: 37.2549118\n",
      "Epoch #304\n",
      "[305,     1] train loss: 39.6023483\n",
      "[305,     5] train loss: 36.8152008\n",
      "Epoch #305\n",
      "[306,     1] train loss: 36.7457123\n",
      "[306,     5] train loss: 37.3704300\n",
      "Epoch #306\n",
      "[307,     1] train loss: 37.5564232\n",
      "[307,     5] train loss: 35.2430977\n",
      "Epoch #307\n",
      "[308,     1] train loss: 37.7203484\n",
      "[308,     5] train loss: 36.2996910\n",
      "Epoch #308\n",
      "[309,     1] train loss: 37.0042496\n",
      "[309,     5] train loss: 37.5654617\n",
      "Epoch #309\n",
      "[310,     1] train loss: 36.6796265\n",
      "[310,     5] train loss: 34.8731987\n",
      "Epoch #310\n",
      "[311,     1] train loss: 38.5767937\n",
      "[311,     5] train loss: 36.0142326\n",
      "Epoch #311\n",
      "[312,     1] train loss: 40.4894409\n",
      "[312,     5] train loss: 36.5454567\n",
      "Epoch #312\n",
      "[313,     1] train loss: 39.3974228\n",
      "[313,     5] train loss: 36.4580421\n",
      "Epoch #313\n",
      "[314,     1] train loss: 36.5174980\n",
      "[314,     5] train loss: 36.6541786\n",
      "Epoch #314\n",
      "[315,     1] train loss: 38.5802879\n",
      "[315,     5] train loss: 37.2290352\n",
      "Epoch #315\n",
      "[316,     1] train loss: 40.8265800\n",
      "[316,     5] train loss: 35.8980850\n",
      "Epoch #316\n",
      "[317,     1] train loss: 37.4872437\n",
      "[317,     5] train loss: 36.2362175\n",
      "Epoch #317\n",
      "[318,     1] train loss: 39.4413834\n",
      "[318,     5] train loss: 36.0049431\n",
      "Epoch #318\n",
      "[319,     1] train loss: 41.0124779\n",
      "[319,     5] train loss: 36.8071465\n",
      "Epoch #319\n",
      "[320,     1] train loss: 39.9634781\n",
      "[320,     5] train loss: 36.3738602\n",
      "Epoch #320\n",
      "[321,     1] train loss: 37.7371559\n",
      "[321,     5] train loss: 35.9685921\n",
      "Epoch #321\n",
      "[322,     1] train loss: 30.5717316\n",
      "[322,     5] train loss: 37.3607613\n",
      "Epoch #322\n",
      "[323,     1] train loss: 35.6960869\n",
      "[323,     5] train loss: 37.3211525\n",
      "Epoch #323\n",
      "[324,     1] train loss: 40.2190552\n",
      "[324,     5] train loss: 36.9006729\n",
      "Epoch #324\n",
      "[325,     1] train loss: 33.7894897\n",
      "[325,     5] train loss: 36.3356262\n",
      "Epoch #325\n",
      "[326,     1] train loss: 38.0090179\n",
      "[326,     5] train loss: 35.9093384\n",
      "Epoch #326\n",
      "[327,     1] train loss: 39.7858925\n",
      "[327,     5] train loss: 35.9645187\n",
      "Epoch #327\n",
      "[328,     1] train loss: 37.0626984\n",
      "[328,     5] train loss: 36.6424446\n",
      "Epoch #328\n",
      "[329,     1] train loss: 38.1928825\n",
      "[329,     5] train loss: 37.8540169\n",
      "Epoch #329\n",
      "[330,     1] train loss: 39.0373230\n",
      "[330,     5] train loss: 37.1327469\n",
      "Epoch #330\n",
      "[331,     1] train loss: 41.4221573\n",
      "[331,     5] train loss: 36.2810257\n",
      "Epoch #331\n",
      "[332,     1] train loss: 37.0832024\n",
      "[332,     5] train loss: 37.1658287\n",
      "Epoch #332\n",
      "[333,     1] train loss: 36.0391197\n",
      "[333,     5] train loss: 37.2284492\n",
      "Epoch #333\n",
      "[334,     1] train loss: 35.1937332\n",
      "[334,     5] train loss: 36.9375183\n",
      "Epoch #334\n",
      "[335,     1] train loss: 39.2874718\n",
      "[335,     5] train loss: 36.3649666\n",
      "Epoch #335\n",
      "[336,     1] train loss: 37.7387047\n",
      "[336,     5] train loss: 36.8988335\n",
      "Epoch #336\n",
      "[337,     1] train loss: 35.3634720\n",
      "[337,     5] train loss: 35.3458939\n",
      "Epoch #337\n",
      "[338,     1] train loss: 37.9397469\n",
      "[338,     5] train loss: 35.4491119\n",
      "Epoch #338\n",
      "[339,     1] train loss: 37.5339355\n",
      "[339,     5] train loss: 36.6778519\n",
      "Epoch #339\n",
      "[340,     1] train loss: 40.6098213\n",
      "[340,     5] train loss: 37.2473198\n",
      "Epoch #340\n",
      "[341,     1] train loss: 36.5431976\n",
      "[341,     5] train loss: 37.7046829\n",
      "Epoch #341\n",
      "[342,     1] train loss: 40.2164154\n",
      "[342,     5] train loss: 36.5102783\n",
      "Epoch #342\n",
      "[343,     1] train loss: 41.5698891\n",
      "[343,     5] train loss: 35.7701691\n",
      "Epoch #343\n",
      "[344,     1] train loss: 38.1822472\n",
      "[344,     5] train loss: 35.5337219\n",
      "Epoch #344\n",
      "[345,     1] train loss: 37.1958885\n",
      "[345,     5] train loss: 36.5694221\n",
      "Epoch #345\n",
      "[346,     1] train loss: 34.3065643\n",
      "[346,     5] train loss: 37.9544655\n",
      "Epoch #346\n",
      "[347,     1] train loss: 42.9866753\n",
      "[347,     5] train loss: 37.4860123\n",
      "Epoch #347\n",
      "[348,     1] train loss: 36.3652153\n",
      "[348,     5] train loss: 37.3862267\n",
      "Epoch #348\n",
      "[349,     1] train loss: 40.2046585\n",
      "[349,     5] train loss: 35.6719208\n",
      "Epoch #349\n",
      "[350,     1] train loss: 37.0738945\n",
      "[350,     5] train loss: 36.0977257\n",
      "Epoch #350\n",
      "[351,     1] train loss: 34.1437263\n",
      "[351,     5] train loss: 36.6055756\n",
      "Epoch #351\n",
      "[352,     1] train loss: 39.0300522\n",
      "[352,     5] train loss: 35.8409866\n",
      "Epoch #352\n",
      "[353,     1] train loss: 40.3576279\n",
      "[353,     5] train loss: 36.2177444\n",
      "Epoch #353\n",
      "[354,     1] train loss: 41.2811089\n",
      "[354,     5] train loss: 37.3936462\n",
      "Epoch #354\n",
      "[355,     1] train loss: 41.2920189\n",
      "[355,     5] train loss: 38.1926353\n",
      "Epoch #355\n",
      "[356,     1] train loss: 36.5740662\n",
      "[356,     5] train loss: 38.1276047\n",
      "Epoch #356\n",
      "[357,     1] train loss: 37.0860176\n",
      "[357,     5] train loss: 35.4096764\n",
      "Epoch #357\n",
      "[358,     1] train loss: 40.2670288\n",
      "[358,     5] train loss: 35.4623024\n",
      "Epoch #358\n",
      "[359,     1] train loss: 40.1759186\n",
      "[359,     5] train loss: 37.9587234\n",
      "Epoch #359\n",
      "[360,     1] train loss: 39.6158485\n",
      "[360,     5] train loss: 36.4144318\n",
      "Epoch #360\n",
      "[361,     1] train loss: 37.2166023\n",
      "[361,     5] train loss: 37.0519081\n",
      "Epoch #361\n",
      "[362,     1] train loss: 36.9740181\n",
      "[362,     5] train loss: 36.5950035\n",
      "Epoch #362\n",
      "[363,     1] train loss: 40.4925461\n",
      "[363,     5] train loss: 35.2194611\n",
      "Epoch #363\n",
      "[364,     1] train loss: 40.0030708\n",
      "[364,     5] train loss: 36.5176865\n",
      "Epoch #364\n",
      "[365,     1] train loss: 38.4748001\n",
      "[365,     5] train loss: 37.9749077\n",
      "Epoch #365\n",
      "[366,     1] train loss: 37.6383629\n",
      "[366,     5] train loss: 37.3431267\n",
      "Epoch #366\n",
      "[367,     1] train loss: 41.3137589\n",
      "[367,     5] train loss: 37.2484451\n",
      "Epoch #367\n",
      "[368,     1] train loss: 41.0787125\n",
      "[368,     5] train loss: 36.0627892\n",
      "Epoch #368\n",
      "[369,     1] train loss: 38.9630699\n",
      "[369,     5] train loss: 36.0883102\n",
      "Epoch #369\n",
      "[370,     1] train loss: 39.3418350\n",
      "[370,     5] train loss: 36.2161873\n",
      "Epoch #370\n",
      "[371,     1] train loss: 37.7649574\n",
      "[371,     5] train loss: 36.1826225\n",
      "Epoch #371\n",
      "[372,     1] train loss: 41.9728508\n",
      "[372,     5] train loss: 35.8160164\n",
      "Epoch #372\n",
      "[373,     1] train loss: 34.3769760\n",
      "[373,     5] train loss: 37.0743340\n",
      "Epoch #373\n",
      "[374,     1] train loss: 38.2610207\n",
      "[374,     5] train loss: 36.4252144\n",
      "Epoch #374\n",
      "[375,     1] train loss: 36.3063393\n",
      "[375,     5] train loss: 36.2050171\n",
      "Epoch #375\n",
      "[376,     1] train loss: 36.1287460\n",
      "[376,     5] train loss: 36.9371445\n",
      "Epoch #376\n",
      "[377,     1] train loss: 37.8899345\n",
      "[377,     5] train loss: 36.0774139\n",
      "Epoch #377\n",
      "[378,     1] train loss: 39.4285622\n",
      "[378,     5] train loss: 36.4674515\n",
      "Epoch #378\n",
      "[379,     1] train loss: 34.9354095\n",
      "[379,     5] train loss: 37.5745438\n",
      "Epoch #379\n",
      "[380,     1] train loss: 37.3228111\n",
      "[380,     5] train loss: 35.9090065\n",
      "Epoch #380\n",
      "[381,     1] train loss: 41.8431015\n",
      "[381,     5] train loss: 34.8647835\n",
      "Epoch #381\n",
      "[382,     1] train loss: 39.9532661\n",
      "[382,     5] train loss: 36.1508972\n",
      "Epoch #382\n",
      "[383,     1] train loss: 38.2770538\n",
      "[383,     5] train loss: 36.4761505\n",
      "Epoch #383\n",
      "[384,     1] train loss: 36.5043716\n",
      "[384,     5] train loss: 37.0176819\n",
      "Epoch #384\n",
      "[385,     1] train loss: 37.2176743\n",
      "[385,     5] train loss: 36.5517517\n",
      "Epoch #385\n",
      "[386,     1] train loss: 36.4368935\n",
      "[386,     5] train loss: 33.9123230\n",
      "Epoch #386\n",
      "[387,     1] train loss: 38.6136513\n",
      "[387,     5] train loss: 36.6517220\n",
      "Epoch #387\n",
      "[388,     1] train loss: 37.2168922\n",
      "[388,     5] train loss: 36.8895653\n",
      "Epoch #388\n",
      "[389,     1] train loss: 35.9165535\n",
      "[389,     5] train loss: 36.0911896\n",
      "Epoch #389\n",
      "[390,     1] train loss: 41.0080338\n",
      "[390,     5] train loss: 35.3201332\n",
      "Epoch #390\n",
      "[391,     1] train loss: 34.5662575\n",
      "[391,     5] train loss: 37.9332512\n",
      "Epoch #391\n",
      "[392,     1] train loss: 39.2281227\n",
      "[392,     5] train loss: 36.4982666\n",
      "Epoch #392\n",
      "[393,     1] train loss: 37.2523155\n",
      "[393,     5] train loss: 37.5394623\n",
      "Epoch #393\n",
      "[394,     1] train loss: 34.4924355\n",
      "[394,     5] train loss: 35.9283478\n",
      "Epoch #394\n",
      "[395,     1] train loss: 38.7856865\n",
      "[395,     5] train loss: 36.5047646\n",
      "Epoch #395\n",
      "[396,     1] train loss: 40.9903450\n",
      "[396,     5] train loss: 36.0075798\n",
      "Epoch #396\n",
      "[397,     1] train loss: 39.7746315\n",
      "[397,     5] train loss: 35.8968262\n",
      "Epoch #397\n",
      "[398,     1] train loss: 39.8028603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[398,     5] train loss: 36.3243355\n",
      "Epoch #398\n",
      "[399,     1] train loss: 41.5402145\n",
      "[399,     5] train loss: 36.7010750\n",
      "Epoch #399\n",
      "[400,     1] train loss: 41.0632362\n",
      "[400,     5] train loss: 35.4529060\n",
      "Epoch #400\n",
      "[401,     1] train loss: 34.3475723\n",
      "[401,     5] train loss: 37.0097656\n",
      "Epoch #401\n",
      "[402,     1] train loss: 39.5837212\n",
      "[402,     5] train loss: 36.5975990\n",
      "Epoch #402\n",
      "[403,     1] train loss: 34.8061752\n",
      "[403,     5] train loss: 36.7039013\n",
      "Epoch #403\n",
      "[404,     1] train loss: 35.8986740\n",
      "[404,     5] train loss: 35.6921181\n",
      "Epoch #404\n",
      "[405,     1] train loss: 38.7603340\n",
      "[405,     5] train loss: 36.4510094\n",
      "Epoch #405\n",
      "[406,     1] train loss: 36.1041222\n",
      "[406,     5] train loss: 34.8751030\n",
      "Epoch #406\n",
      "[407,     1] train loss: 40.5157051\n",
      "[407,     5] train loss: 37.1914391\n",
      "Epoch #407\n",
      "[408,     1] train loss: 40.4533005\n",
      "[408,     5] train loss: 36.1681877\n",
      "Epoch #408\n",
      "[409,     1] train loss: 38.1235161\n",
      "[409,     5] train loss: 34.8661278\n",
      "Epoch #409\n",
      "[410,     1] train loss: 40.2241898\n",
      "[410,     5] train loss: 35.0076218\n",
      "Epoch #410\n",
      "[411,     1] train loss: 36.1799545\n",
      "[411,     5] train loss: 36.5648472\n",
      "Epoch #411\n",
      "[412,     1] train loss: 36.2744408\n",
      "[412,     5] train loss: 36.3474403\n",
      "Epoch #412\n",
      "[413,     1] train loss: 37.7188759\n",
      "[413,     5] train loss: 36.8191315\n",
      "Epoch #413\n",
      "[414,     1] train loss: 37.9141502\n",
      "[414,     5] train loss: 35.6744759\n",
      "Epoch #414\n",
      "[415,     1] train loss: 39.5008202\n",
      "[415,     5] train loss: 36.5644966\n",
      "Epoch #415\n",
      "[416,     1] train loss: 38.4218445\n",
      "[416,     5] train loss: 35.2591827\n",
      "Epoch #416\n",
      "[417,     1] train loss: 36.8510857\n",
      "[417,     5] train loss: 37.7746819\n",
      "Epoch #417\n",
      "[418,     1] train loss: 39.9921417\n",
      "[418,     5] train loss: 36.4939507\n",
      "Epoch #418\n",
      "[419,     1] train loss: 40.6849213\n",
      "[419,     5] train loss: 36.5280174\n",
      "Epoch #419\n",
      "[420,     1] train loss: 37.3549843\n",
      "[420,     5] train loss: 37.5213905\n",
      "Epoch #420\n",
      "[421,     1] train loss: 41.6085167\n",
      "[421,     5] train loss: 35.7024517\n",
      "Epoch #421\n",
      "[422,     1] train loss: 41.5536118\n",
      "[422,     5] train loss: 35.9903023\n",
      "Epoch #422\n",
      "[423,     1] train loss: 36.9825630\n",
      "[423,     5] train loss: 36.2245422\n",
      "Epoch #423\n",
      "[424,     1] train loss: 38.7932816\n",
      "[424,     5] train loss: 36.4715183\n",
      "Epoch #424\n",
      "[425,     1] train loss: 38.5639954\n",
      "[425,     5] train loss: 37.1783592\n",
      "Epoch #425\n",
      "[426,     1] train loss: 37.8694839\n",
      "[426,     5] train loss: 35.0162445\n",
      "Epoch #426\n",
      "[427,     1] train loss: 36.5802841\n",
      "[427,     5] train loss: 34.8091141\n",
      "Epoch #427\n",
      "[428,     1] train loss: 39.4544678\n",
      "[428,     5] train loss: 36.1986832\n",
      "Epoch #428\n",
      "[429,     1] train loss: 38.7297287\n",
      "[429,     5] train loss: 35.6477997\n",
      "Epoch #429\n",
      "[430,     1] train loss: 38.9572487\n",
      "[430,     5] train loss: 36.2817970\n",
      "Epoch #430\n",
      "[431,     1] train loss: 40.9024162\n",
      "[431,     5] train loss: 35.8346100\n",
      "Epoch #431\n",
      "[432,     1] train loss: 37.6244202\n",
      "[432,     5] train loss: 38.1622337\n",
      "Epoch #432\n",
      "[433,     1] train loss: 37.9688454\n",
      "[433,     5] train loss: 35.2798164\n",
      "Epoch #433\n",
      "[434,     1] train loss: 38.2362938\n",
      "[434,     5] train loss: 34.6073250\n",
      "Epoch #434\n",
      "[435,     1] train loss: 34.5199051\n",
      "[435,     5] train loss: 37.8994324\n",
      "Epoch #435\n",
      "[436,     1] train loss: 40.0733414\n",
      "[436,     5] train loss: 35.5149986\n",
      "Epoch #436\n",
      "[437,     1] train loss: 39.9809685\n",
      "[437,     5] train loss: 33.5346054\n",
      "Epoch #437\n",
      "[438,     1] train loss: 38.5486298\n",
      "[438,     5] train loss: 36.4715866\n",
      "Epoch #438\n",
      "[439,     1] train loss: 40.0057297\n",
      "[439,     5] train loss: 34.6496605\n",
      "Epoch #439\n",
      "[440,     1] train loss: 41.2136536\n",
      "[440,     5] train loss: 34.8656921\n",
      "Epoch #440\n",
      "[441,     1] train loss: 38.0895576\n",
      "[441,     5] train loss: 36.2249825\n",
      "Epoch #441\n",
      "[442,     1] train loss: 34.1925964\n",
      "[442,     5] train loss: 36.8952393\n",
      "Epoch #442\n",
      "[443,     1] train loss: 41.6828384\n",
      "[443,     5] train loss: 36.4402000\n",
      "Epoch #443\n",
      "[444,     1] train loss: 41.7271690\n",
      "[444,     5] train loss: 35.4775085\n",
      "Epoch #444\n",
      "[445,     1] train loss: 40.1041679\n",
      "[445,     5] train loss: 34.9111992\n",
      "Epoch #445\n",
      "[446,     1] train loss: 40.9381027\n",
      "[446,     5] train loss: 35.5274483\n",
      "Epoch #446\n",
      "[447,     1] train loss: 39.7882652\n",
      "[447,     5] train loss: 35.7756706\n",
      "Epoch #447\n",
      "[448,     1] train loss: 37.5118713\n",
      "[448,     5] train loss: 37.2620438\n",
      "Epoch #448\n",
      "[449,     1] train loss: 41.4593430\n",
      "[449,     5] train loss: 35.4058876\n",
      "Epoch #449\n",
      "[450,     1] train loss: 38.5550804\n",
      "[450,     5] train loss: 35.5323441\n",
      "Epoch #450\n",
      "[451,     1] train loss: 40.1778297\n",
      "[451,     5] train loss: 34.8645103\n",
      "Epoch #451\n",
      "[452,     1] train loss: 34.5368767\n",
      "[452,     5] train loss: 35.0324066\n",
      "Epoch #452\n",
      "[453,     1] train loss: 41.2192230\n",
      "[453,     5] train loss: 36.3935555\n",
      "Epoch #453\n",
      "[454,     1] train loss: 36.8246346\n",
      "[454,     5] train loss: 36.5937050\n",
      "Epoch #454\n",
      "[455,     1] train loss: 39.8556099\n",
      "[455,     5] train loss: 35.5083344\n",
      "Epoch #455\n",
      "[456,     1] train loss: 40.1836662\n",
      "[456,     5] train loss: 37.1281311\n",
      "Epoch #456\n",
      "[457,     1] train loss: 39.7104149\n",
      "[457,     5] train loss: 35.5224739\n",
      "Epoch #457\n",
      "[458,     1] train loss: 39.1441002\n",
      "[458,     5] train loss: 35.6237793\n",
      "Epoch #458\n",
      "[459,     1] train loss: 34.0507355\n",
      "[459,     5] train loss: 37.4732521\n",
      "Epoch #459\n",
      "[460,     1] train loss: 35.7560387\n",
      "[460,     5] train loss: 35.5655731\n",
      "Epoch #460\n",
      "[461,     1] train loss: 38.2598457\n",
      "[461,     5] train loss: 34.4568810\n",
      "Epoch #461\n",
      "[462,     1] train loss: 39.1977425\n",
      "[462,     5] train loss: 35.8175247\n",
      "Epoch #462\n",
      "[463,     1] train loss: 34.5352592\n",
      "[463,     5] train loss: 35.3819565\n",
      "Epoch #463\n",
      "[464,     1] train loss: 37.6145782\n",
      "[464,     5] train loss: 35.7745308\n",
      "Epoch #464\n",
      "[465,     1] train loss: 34.8057785\n",
      "[465,     5] train loss: 36.2113220\n",
      "Epoch #465\n",
      "[466,     1] train loss: 40.4934273\n",
      "[466,     5] train loss: 35.2643539\n",
      "Epoch #466\n",
      "[467,     1] train loss: 38.8763161\n",
      "[467,     5] train loss: 34.6258934\n",
      "Epoch #467\n",
      "[468,     1] train loss: 37.3257561\n",
      "[468,     5] train loss: 36.5589378\n",
      "Epoch #468\n",
      "[469,     1] train loss: 40.0506554\n",
      "[469,     5] train loss: 34.8453018\n",
      "Epoch #469\n",
      "[470,     1] train loss: 37.9658394\n",
      "[470,     5] train loss: 35.5756004\n",
      "Epoch #470\n",
      "[471,     1] train loss: 39.4440079\n",
      "[471,     5] train loss: 36.3571976\n",
      "Epoch #471\n",
      "[472,     1] train loss: 38.8280830\n",
      "[472,     5] train loss: 35.7981903\n",
      "Epoch #472\n",
      "[473,     1] train loss: 36.8787155\n",
      "[473,     5] train loss: 35.3617325\n",
      "Epoch #473\n",
      "[474,     1] train loss: 38.9587402\n",
      "[474,     5] train loss: 35.5043240\n",
      "Epoch #474\n",
      "[475,     1] train loss: 37.5075645\n",
      "[475,     5] train loss: 35.5080162\n",
      "Epoch #475\n",
      "[476,     1] train loss: 34.3767204\n",
      "[476,     5] train loss: 35.8684380\n",
      "Epoch #476\n",
      "[477,     1] train loss: 37.9174080\n",
      "[477,     5] train loss: 35.2194672\n",
      "Epoch #477\n",
      "[478,     1] train loss: 39.6582947\n",
      "[478,     5] train loss: 36.6720566\n",
      "Epoch #478\n",
      "[479,     1] train loss: 40.3113441\n",
      "[479,     5] train loss: 35.9171883\n",
      "Epoch #479\n",
      "[480,     1] train loss: 41.3887596\n",
      "[480,     5] train loss: 36.3807419\n",
      "Epoch #480\n",
      "[481,     1] train loss: 41.3786392\n",
      "[481,     5] train loss: 36.0677734\n",
      "Epoch #481\n",
      "[482,     1] train loss: 34.1009102\n",
      "[482,     5] train loss: 35.7623260\n",
      "Epoch #482\n",
      "[483,     1] train loss: 35.6108284\n",
      "[483,     5] train loss: 34.9882378\n",
      "Epoch #483\n",
      "[484,     1] train loss: 37.1086731\n",
      "[484,     5] train loss: 36.4278130\n",
      "Epoch #484\n",
      "[485,     1] train loss: 40.1905022\n",
      "[485,     5] train loss: 35.6264320\n",
      "Epoch #485\n",
      "[486,     1] train loss: 41.4918747\n",
      "[486,     5] train loss: 35.6014427\n",
      "Epoch #486\n",
      "[487,     1] train loss: 38.6331024\n",
      "[487,     5] train loss: 35.3304611\n",
      "Epoch #487\n",
      "[488,     1] train loss: 38.6656761\n",
      "[488,     5] train loss: 35.9403419\n",
      "Epoch #488\n",
      "[489,     1] train loss: 35.4224701\n",
      "[489,     5] train loss: 36.4586205\n",
      "Epoch #489\n",
      "[490,     1] train loss: 41.8201942\n",
      "[490,     5] train loss: 37.1387108\n",
      "Epoch #490\n",
      "[491,     1] train loss: 34.1718788\n",
      "[491,     5] train loss: 35.9313259\n",
      "Epoch #491\n",
      "[492,     1] train loss: 37.8358955\n",
      "[492,     5] train loss: 36.2365143\n",
      "Epoch #492\n",
      "[493,     1] train loss: 39.7016640\n",
      "[493,     5] train loss: 35.2987244\n",
      "Epoch #493\n",
      "[494,     1] train loss: 40.1106339\n",
      "[494,     5] train loss: 37.4665955\n",
      "Epoch #494\n",
      "[495,     1] train loss: 40.5051613\n",
      "[495,     5] train loss: 36.0895081\n",
      "Epoch #495\n",
      "[496,     1] train loss: 36.1335106\n",
      "[496,     5] train loss: 34.2967545\n",
      "Epoch #496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[497,     1] train loss: 36.8729668\n",
      "[497,     5] train loss: 37.8872742\n",
      "Epoch #497\n",
      "[498,     1] train loss: 35.5148964\n",
      "[498,     5] train loss: 36.8395004\n",
      "Epoch #498\n",
      "[499,     1] train loss: 38.3252754\n",
      "[499,     5] train loss: 36.9019302\n",
      "Epoch #499\n",
      "[500,     1] train loss: 37.0729294\n",
      "[500,     5] train loss: 35.7297462\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 128\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "    bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "        bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "        drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        bass_test = torch.tensor(bass_test, dtype=torch.float)\n",
    "        \n",
    "    examples_count = drum_train.size()[0]\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size,].transpose(0,1)\n",
    "        # transpose нужен для обмена размерности батча и размерности шагов\n",
    "#         print(f\"batch_drum_train:{batch_drum_train.size()}, batch_bass_train:{batch_bass_train.size()}\")\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train.size()}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_drum_train = drum_train[:,:,:].transpose(0,1)\n",
    "batch_bass_train = bass_train[:,:].transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 1, 1,  ..., 3, 3, 5],\n",
       "        [3, 6, 6,  ..., 5, 5, 2],\n",
       "        [5, 1, 1,  ..., 5, 5, 3],\n",
       "        ...,\n",
       "        [2, 5, 5,  ..., 4, 4, 0],\n",
       "        [5, 0, 0,  ..., 3, 3, 2],\n",
       "        [2, 7, 7,  ..., 6, 6, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_outputs.squeeze().int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, Converter\n",
    "\n",
    "\n",
    "converter = Converter((32,50))\n",
    "\n",
    "batch_drum = torch.cat((drum_train, drum_test, torch.tensor(drum_validation))).transpose(0,1)\n",
    "batch_bass = torch.cat((bass_train.int(), bass_test.int(), torch.tensor(bass_validation).int())).transpose(0,1)\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(bass_outputs.size()[1]):\n",
    "        bass_seq = bass_outputs[:,i]\n",
    "#         bass_seq = batch_bass[:,i]\n",
    "#         print(f\"bass_seq:{bass_seq.size()}\")\n",
    "        bass_output = []\n",
    "        for bass_note in bass_seq:\n",
    "            bass_row = np.eye(1, 36, bass_note - 1)[0]\n",
    "            bass_output.append(bass_row)\n",
    "        bass_output = torch.tensor(bass_output).int().squeeze()\n",
    "#         print(f\"bass_output:{bass_output.size()}\")\n",
    "        \n",
    "#         print(f\"batch_drum:{batch_drum[:,i,:].size()}, bass_output:{bass_output.size()}\")\n",
    "            \n",
    "        img_dnb = torch.cat((batch_drum[:,i,:].int(),bass_output), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        pair = converter.convert_numpy_image_to_pair(np.array(img_dnb))\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом\n",
    "\n",
    "В этой версии используется VAE. Энкодер -- двухслойная полносвязная нейронная сеть, декодер -- зеркальная двухслойная полносвязная нейронная сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "patterns_file = \"decode_patterns/patterns.pairs.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            limit=1000,\n",
    "                                                            patterns_file=patterns_file,\n",
    "                                                            mono=False)\n",
    "# print(drum[0])\n",
    "# drum, bass = np.array(drum), np.array(bass)\n",
    "# print(drum[0])\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=384, instrument=56, denominator=1, min_note=43)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_FFNN_to_FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_height, drum_width, melody_width):\n",
    "        super(DrumNBass_FFNN_to_FFNN, self).__init__()\n",
    "        \n",
    "        self.data_height = data_height\n",
    "        self.drum_width = drum_width\n",
    "        self.melody_width = melody_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        \n",
    "        input_dim = data_height*drum_width\n",
    "        latent_dim = 4\n",
    "        hidden_dim = (input_dim + latent_dim) // 2\n",
    "        output_dim = data_height*melody_width\n",
    "        \n",
    "        # Linear function 1: 128 * 14 = 1792 --> 2048\n",
    "        # веса накидываются тут\n",
    "        self.fc1 = nn.Linear(input_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc1.weight, mean=1.5, std=1.0)\n",
    "        # решение по весам\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc2.weight, mean=1.5, std=1.0)\n",
    "        self.relu2 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Linear function 31: 2048 --> 4\n",
    "        # для средних значений\n",
    "        self.fc31 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 31\n",
    "        self.relu31 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        # Linear function 22: 2048 --> 4\n",
    "        # для стандартных отклонений\n",
    "        self.fc32 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 22\n",
    "        self.relu32 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 4: 4 --> 2048\n",
    "        self.fc4 = nn.Linear(latent_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc4.weight, mean=1.5, std=1.0)\n",
    "        # Non-linearity 4\n",
    "        self.relu4 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc5.weight, mean=1.5, std=1.0)\n",
    "        self.relu5 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 6 (readout): 2048 --> 128 * 36 = 4608\n",
    "        self.fc6 = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 1\n",
    "        out = self.fc1(out)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 31\n",
    "        mu = self.fc31(out)\n",
    "        # Non-linearity 31\n",
    "        mu = self.relu31(mu)\n",
    "        \n",
    "        \n",
    "        # Linear function 22\n",
    "        logvar = self.fc32(out)\n",
    "        # Non-linearity 22\n",
    "        logvar = self.relu32(logvar)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 4\n",
    "        out = self.fc4(out)\n",
    "        # Non-linearity 4\n",
    "        out = self.relu4(out)\n",
    "        \n",
    "        # Linear function 5\n",
    "        out = self.fc5(out)\n",
    "        # Non-linearity 5\n",
    "        out = self.relu5(out)\n",
    "\n",
    "        # Linear function 6 (readout)\n",
    "        out = self.fc6(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image.flatten(), input)), dtype=torch.float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_conditionings(input):\n",
    "        return torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # добавляем conditioning\n",
    "        conditionings = self.get_conditionings(x)\n",
    "        images = self.get_images(x)\n",
    "        mean, logvar = self.encoder(images, conditionings)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.data_height, self.melody_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_ffnn = DrumNBass_FFNN_to_FFNN(data_height, drum_width, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_ffnn.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_ffnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_ffnn.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 51980.1250000\n",
      "[1,     6] train loss: 25536.3955078\n",
      "[1,    11] train loss: 7227.9569499\n",
      "[1,    16] train loss: 3392.6305339\n",
      "[1,    20] train loss: 3084.5725586\n",
      "Epoch #1\n",
      "[2,     1] train loss: 3950.9733887\n",
      "[2,     6] train loss: 3480.6938477\n",
      "[2,    11] train loss: 3096.0603841\n",
      "[2,    16] train loss: 3258.6243490\n",
      "[2,    20] train loss: 2873.6671387\n",
      "Epoch #2\n",
      "[3,     1] train loss: 3193.5993652\n",
      "[3,     6] train loss: 2885.1299642\n",
      "[3,    11] train loss: 2933.6507568\n",
      "[3,    16] train loss: 3002.1475830\n",
      "[3,    20] train loss: 3023.3045410\n",
      "Epoch #3\n",
      "[4,     1] train loss: 3146.3657227\n",
      "[4,     6] train loss: 2898.6712646\n",
      "[4,    11] train loss: 2844.8400065\n",
      "[4,    16] train loss: 2958.4781494\n",
      "[4,    20] train loss: 2789.6374023\n",
      "Epoch #4\n",
      "[5,     1] train loss: 3213.4360352\n",
      "[5,     6] train loss: 2907.2758789\n",
      "[5,    11] train loss: 2906.3653564\n",
      "[5,    16] train loss: 2913.3131510\n",
      "[5,    20] train loss: 2639.8975586\n",
      "Epoch #5\n",
      "[6,     1] train loss: 3192.1127930\n",
      "[6,     6] train loss: 2785.3043620\n",
      "[6,    11] train loss: 2927.1033936\n",
      "[6,    16] train loss: 2889.2034912\n",
      "[6,    20] train loss: 2875.9812500\n",
      "Epoch #6\n",
      "[7,     1] train loss: 3580.1516113\n",
      "[7,     6] train loss: 2805.0332438\n",
      "[7,    11] train loss: 2796.6379801\n",
      "[7,    16] train loss: 2793.3837077\n",
      "[7,    20] train loss: 2774.3251953\n",
      "Epoch #7\n",
      "[8,     1] train loss: 3296.9208984\n",
      "[8,     6] train loss: 2855.8288167\n",
      "[8,    11] train loss: 2834.2867025\n",
      "[8,    16] train loss: 2847.7596436\n",
      "[8,    20] train loss: 2672.6818359\n",
      "Epoch #8\n",
      "[9,     1] train loss: 3310.6213379\n",
      "[9,     6] train loss: 2863.6730957\n",
      "[9,    11] train loss: 2928.3357747\n",
      "[9,    16] train loss: 2858.9660645\n",
      "[9,    20] train loss: 2831.3917480\n",
      "Epoch #9\n",
      "[10,     1] train loss: 3345.2221680\n",
      "[10,     6] train loss: 2911.5901286\n",
      "[10,    11] train loss: 2771.4193929\n",
      "[10,    16] train loss: 2897.4307861\n",
      "[10,    20] train loss: 2855.3546387\n",
      "Epoch #10\n",
      "[11,     1] train loss: 2995.9296875\n",
      "[11,     6] train loss: 2946.2851969\n",
      "[11,    11] train loss: 2950.8857829\n",
      "[11,    16] train loss: 2811.7564697\n",
      "[11,    20] train loss: 2713.1363281\n",
      "Epoch #11\n",
      "[12,     1] train loss: 3354.0085449\n",
      "[12,     6] train loss: 2879.7957764\n",
      "[12,    11] train loss: 2933.1019287\n",
      "[12,    16] train loss: 2803.1576335\n",
      "[12,    20] train loss: 2874.6824219\n",
      "Epoch #12\n",
      "[13,     1] train loss: 3580.0429688\n",
      "[13,     6] train loss: 3012.7765299\n",
      "[13,    11] train loss: 2838.2967122\n",
      "[13,    16] train loss: 2912.9632975\n",
      "[13,    20] train loss: 2677.7212402\n",
      "Epoch #13\n",
      "[14,     1] train loss: 3433.9790039\n",
      "[14,     6] train loss: 2923.3915202\n",
      "[14,    11] train loss: 2826.4875895\n",
      "[14,    16] train loss: 2860.8663737\n",
      "[14,    20] train loss: 2833.0119629\n",
      "Epoch #14\n",
      "[15,     1] train loss: 3139.0991211\n",
      "[15,     6] train loss: 2894.1899414\n",
      "[15,    11] train loss: 2895.3575033\n",
      "[15,    16] train loss: 2934.9682210\n",
      "[15,    20] train loss: 2596.8798340\n",
      "Epoch #15\n",
      "[16,     1] train loss: 3440.8969727\n",
      "[16,     6] train loss: 2923.2276611\n",
      "[16,    11] train loss: 2885.0757243\n",
      "[16,    16] train loss: 2818.0231527\n",
      "[16,    20] train loss: 2773.4021973\n",
      "Epoch #16\n",
      "[17,     1] train loss: 3359.0102539\n",
      "[17,     6] train loss: 2939.8733724\n",
      "[17,    11] train loss: 3012.7377930\n",
      "[17,    16] train loss: 2838.0354411\n",
      "[17,    20] train loss: 2651.7054687\n",
      "Epoch #17\n",
      "[18,     1] train loss: 3347.0637207\n",
      "[18,     6] train loss: 2833.2815348\n",
      "[18,    11] train loss: 2760.1050212\n",
      "[18,    16] train loss: 2965.2025553\n",
      "[18,    20] train loss: 2816.2476563\n",
      "Epoch #18\n",
      "[19,     1] train loss: 3594.8476562\n",
      "[19,     6] train loss: 2790.8526204\n",
      "[19,    11] train loss: 2916.1131999\n",
      "[19,    16] train loss: 2752.0852458\n",
      "[19,    20] train loss: 2796.2231445\n",
      "Epoch #19\n",
      "[20,     1] train loss: 3250.0185547\n",
      "[20,     6] train loss: 2752.0139567\n",
      "[20,    11] train loss: 2791.7985840\n",
      "[20,    16] train loss: 2849.4768066\n",
      "[20,    20] train loss: 2800.1651367\n",
      "Epoch #20\n",
      "[21,     1] train loss: 3396.3129883\n",
      "[21,     6] train loss: 2989.8041585\n",
      "[21,    11] train loss: 2832.2835286\n",
      "[21,    16] train loss: 2868.4101156\n",
      "[21,    20] train loss: 2675.1880371\n",
      "Epoch #21\n",
      "[22,     1] train loss: 3558.8173828\n",
      "[22,     6] train loss: 2868.8651123\n",
      "[22,    11] train loss: 2818.0220540\n",
      "[22,    16] train loss: 2800.3701172\n",
      "[22,    20] train loss: 2642.4019043\n",
      "Epoch #22\n",
      "[23,     1] train loss: 3497.1630859\n",
      "[23,     6] train loss: 2832.3694661\n",
      "[23,    11] train loss: 2684.6507975\n",
      "[23,    16] train loss: 2825.3247070\n",
      "[23,    20] train loss: 2734.0472656\n",
      "Epoch #23\n",
      "[24,     1] train loss: 3332.7976074\n",
      "[24,     6] train loss: 2873.7814941\n",
      "[24,    11] train loss: 2693.0208333\n",
      "[24,    16] train loss: 2764.3649902\n",
      "[24,    20] train loss: 2714.8218750\n",
      "Epoch #24\n",
      "[25,     1] train loss: 3579.0434570\n",
      "[25,     6] train loss: 2796.2807210\n",
      "[25,    11] train loss: 2704.9201253\n",
      "[25,    16] train loss: 2857.9792887\n",
      "[25,    20] train loss: 2772.4163574\n",
      "Epoch #25\n",
      "[26,     1] train loss: 3222.9199219\n",
      "[26,     6] train loss: 2750.7650960\n",
      "[26,    11] train loss: 2811.5941162\n",
      "[26,    16] train loss: 2867.9685872\n",
      "[26,    20] train loss: 2644.6172363\n",
      "Epoch #26\n",
      "[27,     1] train loss: 3312.8662109\n",
      "[27,     6] train loss: 2767.4031169\n",
      "[27,    11] train loss: 2718.9026693\n",
      "[27,    16] train loss: 2931.6329753\n",
      "[27,    20] train loss: 2712.8487793\n",
      "Epoch #27\n",
      "[28,     1] train loss: 3043.9184570\n",
      "[28,     6] train loss: 2771.9017741\n",
      "[28,    11] train loss: 2783.7013346\n",
      "[28,    16] train loss: 2804.0168457\n",
      "[28,    20] train loss: 2812.5858887\n",
      "Epoch #28\n",
      "[29,     1] train loss: 3452.5898438\n",
      "[29,     6] train loss: 2772.2369385\n",
      "[29,    11] train loss: 2828.8671875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-5e300cc59a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# zero the parameter gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_ffnn(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_ffnn(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images(drum_train), dnb_ffnn.get_conditionings(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "mu\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]*100\n",
    "y = mu[:,1]*100\n",
    "z = dev*1\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*20, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "batch_drum = drum_train + drum_test + drum_validation\n",
    "batch_drum = drum_validation\n",
    "batch_bass = bass_train + bass_test + bass_validation\n",
    "batch_bass = drum_validation\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_ffnn(batch_drum)[0]\n",
    "    bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "    \n",
    "    for i in range(len(batch_drum)):\n",
    "            \n",
    "        img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , batch_drum[i].tempo\n",
    "                                , batch_drum[i].instrument\n",
    "                                , 1\n",
    "                                , batch_drum[i].min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encoder() missing 1 required positional argument: 'cond'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-d8de0f1a8779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# вычисляем два вектора в латентном пространстве\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mlatent_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnb_ffnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdnb_ffnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatent_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: encoder() missing 1 required positional argument: 'cond'"
     ]
    }
   ],
   "source": [
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 3\n",
    "sample2_id = 49\n",
    "sample1 = drum_validation[sample1_id]\n",
    "sample2 = drum_validation[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images([sample1, sample2]))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_ffnn.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , instrument\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

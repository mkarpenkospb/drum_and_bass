{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом\n",
    "\n",
    "В этой версии используется VAE. Энкодер -- двухслойная полносвязная нейронная сеть, декодер -- зеркальная двухслойная полносвязная нейронная сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "train_file = \"decode_patterns/train.tsv\" # обучающая выборка\n",
    "validation_file = \"decode_patterns/validation.tsv\" # валидационная выборка\n",
    "human_file = \"decode_patterns/human.tsv\" # как валидационная, только для ассесмента людей (read as \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            patterns_file=train_file,\n",
    "                                                            mono=False)\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "\n",
    "# selecting a validation set\n",
    "drum_validation, bass_validation = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                                                  patterns_file=validation_file,\n",
    "                                                                                  mono=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=96, instrument=27, denominator=4, min_note=45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_FFNN_to_FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_height, drum_width, melody_width):\n",
    "        super(DrumNBass_FFNN_to_FFNN, self).__init__()\n",
    "        \n",
    "        self.data_height = data_height\n",
    "        self.drum_width = drum_width\n",
    "        self.melody_width = melody_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        \n",
    "        input_dim = data_height*drum_width\n",
    "        latent_dim = 4\n",
    "        hidden_dim = (input_dim + latent_dim) // 2\n",
    "        output_dim = data_height*melody_width\n",
    "        \n",
    "        # Linear function 1: 128 * 14 = 1792 --> 2048\n",
    "        # веса накидываются тут\n",
    "        self.fc1 = nn.Linear(input_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc1.weight, mean=1.5, std=1.0)\n",
    "        # решение по весам\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc2.weight, mean=1.5, std=1.0)\n",
    "        self.relu2 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Linear function 31: 2048 --> 4\n",
    "        # для средних значений\n",
    "        self.fc31 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 31\n",
    "        self.relu31 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        # Linear function 22: 2048 --> 4\n",
    "        # для стандартных отклонений\n",
    "        self.fc32 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 22\n",
    "        self.relu32 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 4: 4 --> 2048\n",
    "        self.fc4 = nn.Linear(latent_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc4.weight, mean=1.5, std=1.0)\n",
    "        # Non-linearity 4\n",
    "        self.relu4 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc5.weight, mean=1.5, std=1.0)\n",
    "        self.relu5 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 6 (readout): 2048 --> 128 * 36 = 4608\n",
    "        self.fc6 = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 1\n",
    "        out = self.fc1(out)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 31\n",
    "        mu = self.fc31(out)\n",
    "        # Non-linearity 31\n",
    "        mu = self.relu31(mu)\n",
    "        \n",
    "        \n",
    "        # Linear function 22\n",
    "        logvar = self.fc32(out)\n",
    "        # Non-linearity 22\n",
    "        logvar = self.relu32(logvar)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 4\n",
    "        out = self.fc4(out)\n",
    "        # Non-linearity 4\n",
    "        out = self.relu4(out)\n",
    "        \n",
    "        # Linear function 5\n",
    "        out = self.fc5(out)\n",
    "        # Non-linearity 5\n",
    "        out = self.relu5(out)\n",
    "\n",
    "        # Linear function 6 (readout)\n",
    "        out = self.fc6(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image.flatten(), input)), dtype=torch.float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_conditionings(input):\n",
    "        return torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # добавляем conditioning\n",
    "        conditionings = self.get_conditionings(x)\n",
    "        images = self.get_images(x)\n",
    "        mean, logvar = self.encoder(images, conditionings)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.data_height, self.melody_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_ffnn = DrumNBass_FFNN_to_FFNN(data_height, drum_width, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_loss(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_ffnn.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_ffnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_ffnn.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 6928.3706055\n",
      "[1,     6] train loss: 5921.8031413\n",
      "[1,    11] train loss: 5963.7903646\n",
      "[1,    16] train loss: 5805.0757650\n",
      "[1,    21] train loss: 5834.2417806\n",
      "[1,    26] train loss: 5983.8310547\n",
      "[1,    31] train loss: 5811.6764323\n",
      "[1,    36] train loss: 5945.5245768\n",
      "[1,    41] train loss: 5650.2495117\n",
      "[1,    46] train loss: 5897.0062663\n",
      "[1,    51] train loss: 6009.6580404\n",
      "[1,    56] train loss: 5743.2788900\n",
      "[1,    61] train loss: 6011.6407878\n",
      "[1,    66] train loss: 5861.7403158\n",
      "[1,    71] train loss: 5861.8391113\n",
      "[1,    76] train loss: 5811.7207031\n",
      "[1,    81] train loss: 5638.9197591\n",
      "[1,    86] train loss: 6023.7460938\n",
      "[1,    91] train loss: 5726.4121094\n",
      "[1,    96] train loss: 6017.1227214\n",
      "[1,   101] train loss: 5900.8854167\n",
      "[1,   106] train loss: 5842.2268880\n",
      "[1,   111] train loss: 5946.9465332\n",
      "[1,   116] train loss: 6219.3322754\n",
      "[1,   121] train loss: 6061.3289388\n",
      "[1,   126] train loss: 5870.4851074\n",
      "[1,   131] train loss: 5741.6367188\n",
      "[1,   136] train loss: 5930.3420410\n",
      "[1,   141] train loss: 6362.7888997\n",
      "[1,   146] train loss: 5836.2211914\n",
      "[1,   151] train loss: 6055.7978516\n",
      "[1,   156] train loss: 5980.0974935\n",
      "[1,   161] train loss: 5744.8085938\n",
      "[1,   166] train loss: 5922.7553711\n",
      "[1,   171] train loss: 6031.6994629\n",
      "[1,   176] train loss: 5781.9536947\n",
      "[1,   181] train loss: 6025.2702637\n",
      "[1,   186] train loss: 5775.6399740\n",
      "[1,   191] train loss: 5781.3628743\n",
      "[1,   196] train loss: 5788.3424479\n",
      "[1,   201] train loss: 5703.9637044\n",
      "[1,   206] train loss: 5947.1285807\n",
      "[1,   211] train loss: 5750.4734701\n",
      "[1,   216] train loss: 5970.0198568\n",
      "[1,   221] train loss: 5699.4540202\n",
      "[1,   226] train loss: 6046.7810059\n",
      "[1,   231] train loss: 6067.3780924\n",
      "[1,   236] train loss: 5862.1118164\n",
      "[1,   241] train loss: 5597.3865560\n",
      "[1,   246] train loss: 5724.2453613\n",
      "[1,   251] train loss: 5660.5787760\n",
      "[1,   256] train loss: 5719.9165853\n",
      "[1,   261] train loss: 6082.2720540\n",
      "[1,   266] train loss: 5856.5947266\n",
      "[1,   271] train loss: 5923.9896647\n",
      "[1,   276] train loss: 5861.7736003\n",
      "[1,   281] train loss: 5888.2530924\n",
      "[1,   286] train loss: 5799.3484701\n",
      "[1,   291] train loss: 6185.9355469\n",
      "[1,   296] train loss: 5793.8854980\n",
      "[1,   301] train loss: 5892.7020671\n",
      "[1,   306] train loss: 5704.7226562\n",
      "[1,   311] train loss: 5786.4855957\n",
      "[1,   316] train loss: 5879.7626953\n",
      "[1,   321] train loss: 6009.3299967\n",
      "[1,   326] train loss: 5909.8497721\n",
      "[1,   331] train loss: 5856.8927409\n",
      "[1,   336] train loss: 6019.5303548\n",
      "[1,   341] train loss: 5915.2853190\n",
      "[1,   346] train loss: 5741.7824707\n",
      "[1,   351] train loss: 5958.5717773\n",
      "[1,   356] train loss: 6220.3802083\n",
      "[1,   361] train loss: 5759.5483398\n",
      "[1,   366] train loss: 5909.5336914\n",
      "[1,   371] train loss: 5615.9794922\n",
      "[1,   376] train loss: 6025.1958008\n",
      "[1,   381] train loss: 6134.8705241\n",
      "[1,   386] train loss: 6237.0895182\n",
      "[1,   391] train loss: 5675.4914551\n",
      "[1,   396] train loss: 6105.1149902\n",
      "[1,   401] train loss: 5834.4741211\n",
      "[1,   406] train loss: 5730.4287109\n",
      "[1,   411] train loss: 5932.6842448\n",
      "[1,   416] train loss: 5827.1272786\n",
      "[1,   421] train loss: 6004.4377441\n",
      "[1,   426] train loss: 5840.0965983\n",
      "[1,   431] train loss: 5749.3251953\n",
      "[1,   436] train loss: 5921.5806478\n",
      "[1,   441] train loss: 5681.2038574\n",
      "[1,   446] train loss: 5949.6907552\n",
      "[1,   451] train loss: 5778.4070638\n",
      "[1,   456] train loss: 5753.8437500\n",
      "[1,   461] train loss: 5839.2229004\n",
      "[1,   466] train loss: 6052.7363281\n",
      "[1,   471] train loss: 5756.9530436\n",
      "[1,   476] train loss: 5676.6124674\n",
      "[1,   481] train loss: 6287.9417318\n",
      "[1,   486] train loss: 5707.4839681\n",
      "[1,   491] train loss: 5825.6399740\n",
      "[1,   496] train loss: 5802.8233236\n",
      "[1,   501] train loss: 5933.8159993\n",
      "[1,   506] train loss: 6187.2704264\n",
      "[1,   511] train loss: 5445.4464518\n",
      "[1,   516] train loss: 5870.1293945\n",
      "[1,   521] train loss: 5807.3244629\n",
      "[1,   526] train loss: 5593.4825033\n",
      "[1,   531] train loss: 5760.1776530\n",
      "[1,   536] train loss: 5802.4502767\n",
      "[1,   541] train loss: 6095.5089518\n",
      "[1,   546] train loss: 5983.5997721\n",
      "[1,   551] train loss: 5951.7274577\n",
      "[1,   556] train loss: 5611.4349772\n",
      "[1,   561] train loss: 5940.8246257\n",
      "[1,   566] train loss: 5544.4090983\n",
      "[1,   571] train loss: 5954.2085775\n",
      "[1,   576] train loss: 5901.2543945\n",
      "[1,   581] train loss: 5813.7974447\n",
      "[1,   586] train loss: 6028.1930339\n",
      "[1,   591] train loss: 5916.7027995\n",
      "[1,   596] train loss: 5932.7703451\n",
      "[1,   601] train loss: 5717.2740072\n",
      "[1,   606] train loss: 5817.5839844\n",
      "[1,   611] train loss: 5859.2235514\n",
      "[1,   616] train loss: 5899.4845378\n",
      "[1,   621] train loss: 6102.5755208\n",
      "[1,   625] train loss: 5769.9468750\n",
      "#1 reconstruction test loss: 110.05210876464844\n",
      "#1 reconstruction validation loss: 110.56532287597656\n",
      "Epoch #1\n",
      "[2,     1] train loss: 7076.1943359\n",
      "[2,     6] train loss: 5795.5365397\n",
      "[2,    11] train loss: 5789.0349935\n",
      "[2,    16] train loss: 6040.1904297\n",
      "[2,    21] train loss: 5879.2665202\n",
      "[2,    26] train loss: 5886.7888184\n",
      "[2,    31] train loss: 5854.5397949\n",
      "[2,    36] train loss: 5798.6128743\n",
      "[2,    41] train loss: 5850.4002279\n",
      "[2,    46] train loss: 5747.1561686\n",
      "[2,    51] train loss: 5652.3569336\n",
      "[2,    56] train loss: 5796.0474447\n",
      "[2,    61] train loss: 5976.3319499\n",
      "[2,    66] train loss: 5971.6313477\n",
      "[2,    71] train loss: 5856.8654785\n",
      "[2,    76] train loss: 5681.1594238\n",
      "[2,    81] train loss: 5581.2788086\n",
      "[2,    86] train loss: 5912.6794434\n",
      "[2,    91] train loss: 5895.7598470\n",
      "[2,    96] train loss: 6182.8282064\n",
      "[2,   101] train loss: 5860.6265462\n",
      "[2,   106] train loss: 5998.1429036\n",
      "[2,   111] train loss: 5671.9476725\n",
      "[2,   116] train loss: 6061.0263672\n",
      "[2,   121] train loss: 5652.0232747\n",
      "[2,   126] train loss: 5887.4010417\n",
      "[2,   131] train loss: 5599.6188965\n",
      "[2,   136] train loss: 5792.4987793\n",
      "[2,   141] train loss: 5764.0043132\n",
      "[2,   146] train loss: 5558.9167480\n",
      "[2,   151] train loss: 5939.5030111\n",
      "[2,   156] train loss: 6049.6458333\n",
      "[2,   161] train loss: 5977.3357747\n",
      "[2,   166] train loss: 5647.7324219\n",
      "[2,   171] train loss: 5585.5184733\n",
      "[2,   176] train loss: 5683.9065755\n",
      "[2,   181] train loss: 5968.2060547\n",
      "[2,   186] train loss: 5783.0946452\n",
      "[2,   191] train loss: 5805.7774251\n",
      "[2,   196] train loss: 5891.3299154\n",
      "[2,   201] train loss: 5822.2255859\n",
      "[2,   206] train loss: 5833.2336426\n",
      "[2,   211] train loss: 5695.0298665\n",
      "[2,   216] train loss: 5833.9162598\n",
      "[2,   221] train loss: 6240.4257812\n",
      "[2,   226] train loss: 5963.6597493\n",
      "[2,   231] train loss: 5891.4184570\n",
      "[2,   236] train loss: 5835.8561198\n",
      "[2,   241] train loss: 5686.8103027\n",
      "[2,   246] train loss: 5916.5427246\n",
      "[2,   251] train loss: 5763.8901367\n",
      "[2,   256] train loss: 5969.4724121\n",
      "[2,   261] train loss: 5776.2830404\n",
      "[2,   266] train loss: 5829.1013997\n",
      "[2,   271] train loss: 5824.6780599\n",
      "[2,   276] train loss: 5646.9084473\n",
      "[2,   281] train loss: 5969.8385417\n",
      "[2,   286] train loss: 5905.0486654\n",
      "[2,   291] train loss: 5938.5817057\n",
      "[2,   296] train loss: 5798.8662109\n",
      "[2,   301] train loss: 5929.1300456\n",
      "[2,   306] train loss: 6004.5467936\n",
      "[2,   311] train loss: 5982.2763672\n",
      "[2,   316] train loss: 5901.1306152\n",
      "[2,   321] train loss: 5951.8321126\n",
      "[2,   326] train loss: 6118.3545736\n",
      "[2,   331] train loss: 5582.9489746\n",
      "[2,   336] train loss: 5903.1248372\n",
      "[2,   341] train loss: 5833.5397135\n",
      "[2,   346] train loss: 5885.4044596\n",
      "[2,   351] train loss: 6024.4554036\n",
      "[2,   356] train loss: 6155.6084798\n",
      "[2,   361] train loss: 5778.6746419\n",
      "[2,   366] train loss: 5890.9855957\n",
      "[2,   371] train loss: 5703.3113607\n",
      "[2,   376] train loss: 6009.4442546\n",
      "[2,   381] train loss: 5761.9526367\n",
      "[2,   386] train loss: 5942.1673991\n",
      "[2,   391] train loss: 6024.9445801\n",
      "[2,   396] train loss: 5828.6977539\n",
      "[2,   401] train loss: 5938.6840007\n",
      "[2,   406] train loss: 6079.6969401\n",
      "[2,   411] train loss: 6062.7891439\n",
      "[2,   416] train loss: 5997.1086426\n",
      "[2,   421] train loss: 5824.9897461\n",
      "[2,   426] train loss: 5926.9064941\n",
      "[2,   431] train loss: 5869.7540690\n",
      "[2,   436] train loss: 5865.7255859\n",
      "[2,   441] train loss: 5749.4201660\n",
      "[2,   446] train loss: 5890.4989421\n",
      "[2,   451] train loss: 6089.5586751\n",
      "[2,   456] train loss: 5765.3811849\n",
      "[2,   461] train loss: 5855.3920898\n",
      "[2,   466] train loss: 5784.0224609\n",
      "[2,   471] train loss: 5842.1409505\n",
      "[2,   476] train loss: 5838.4456380\n",
      "[2,   481] train loss: 5710.8705241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   486] train loss: 5611.2970378\n",
      "[2,   491] train loss: 5833.7227376\n",
      "[2,   496] train loss: 5778.2229004\n",
      "[2,   501] train loss: 5898.8133952\n",
      "[2,   506] train loss: 5929.2091471\n",
      "[2,   511] train loss: 5865.7534180\n",
      "[2,   516] train loss: 5868.3014323\n",
      "[2,   521] train loss: 5819.3842773\n",
      "[2,   526] train loss: 6298.0575358\n",
      "[2,   531] train loss: 5941.1722819\n",
      "[2,   536] train loss: 5690.3405762\n",
      "[2,   541] train loss: 5786.4567057\n",
      "[2,   546] train loss: 5951.0177409\n",
      "[2,   551] train loss: 6025.2040202\n",
      "[2,   556] train loss: 5724.3911133\n",
      "[2,   561] train loss: 5844.8251139\n",
      "[2,   566] train loss: 5656.4993490\n",
      "[2,   571] train loss: 5930.7271322\n",
      "[2,   576] train loss: 5874.1820475\n",
      "[2,   581] train loss: 5859.5931803\n",
      "[2,   586] train loss: 5815.2277018\n",
      "[2,   591] train loss: 5729.2395833\n",
      "[2,   596] train loss: 5755.6009115\n",
      "[2,   601] train loss: 5786.0463053\n",
      "[2,   606] train loss: 6254.7379557\n",
      "[2,   611] train loss: 6210.3460286\n",
      "[2,   616] train loss: 5877.6431478\n",
      "[2,   621] train loss: 5910.7590332\n",
      "[2,   625] train loss: 5729.4847656\n",
      "#2 reconstruction test loss: 109.39081573486328\n",
      "#2 reconstruction validation loss: 110.36331176757812\n",
      "Epoch #2\n",
      "[3,     1] train loss: 6971.8920898\n",
      "[3,     6] train loss: 6409.1791178\n",
      "[3,    11] train loss: 5698.4313965\n",
      "[3,    16] train loss: 5892.7169596\n",
      "[3,    21] train loss: 5848.0577799\n",
      "[3,    26] train loss: 5767.1483561\n",
      "[3,    31] train loss: 6029.0187988\n",
      "[3,    36] train loss: 5945.0279948\n",
      "[3,    41] train loss: 5729.1603190\n",
      "[3,    46] train loss: 5589.3127441\n",
      "[3,    51] train loss: 5688.1329753\n",
      "[3,    56] train loss: 6018.6290690\n",
      "[3,    61] train loss: 5980.2540690\n",
      "[3,    66] train loss: 5773.2395020\n",
      "[3,    71] train loss: 5861.2196452\n",
      "[3,    76] train loss: 5828.8356120\n",
      "[3,    81] train loss: 5975.5970866\n",
      "[3,    86] train loss: 5675.7847493\n",
      "[3,    91] train loss: 5835.5623372\n",
      "[3,    96] train loss: 6101.5847982\n",
      "[3,   101] train loss: 6007.5476888\n",
      "[3,   106] train loss: 5814.1681315\n",
      "[3,   111] train loss: 6013.1894531\n",
      "[3,   116] train loss: 5911.2468262\n",
      "[3,   121] train loss: 5731.4769694\n",
      "[3,   126] train loss: 5963.3780111\n",
      "[3,   131] train loss: 5858.9405111\n",
      "[3,   136] train loss: 6007.9495443\n",
      "[3,   141] train loss: 5851.7099609\n",
      "[3,   146] train loss: 6033.9809570\n",
      "[3,   151] train loss: 5913.4941406\n",
      "[3,   156] train loss: 6005.8330892\n",
      "[3,   161] train loss: 5877.2579753\n",
      "[3,   166] train loss: 6053.3350423\n",
      "[3,   171] train loss: 5678.6529134\n",
      "[3,   176] train loss: 6156.3224284\n",
      "[3,   181] train loss: 5686.1547852\n",
      "[3,   186] train loss: 5799.2824707\n",
      "[3,   191] train loss: 5597.8516439\n",
      "[3,   196] train loss: 5792.7596029\n",
      "[3,   201] train loss: 5919.8079427\n",
      "[3,   206] train loss: 5799.0826823\n",
      "[3,   211] train loss: 5753.0541178\n",
      "[3,   216] train loss: 6038.9925944\n",
      "[3,   221] train loss: 5989.9455566\n",
      "[3,   226] train loss: 5987.5159505\n",
      "[3,   231] train loss: 6166.5029297\n",
      "[3,   236] train loss: 5593.4115397\n",
      "[3,   241] train loss: 5877.6463216\n",
      "[3,   246] train loss: 5867.5613607\n",
      "[3,   251] train loss: 6132.5360514\n",
      "[3,   256] train loss: 5734.3137207\n",
      "[3,   261] train loss: 5937.6570638\n",
      "[3,   266] train loss: 6067.8517253\n",
      "[3,   271] train loss: 5881.3039551\n",
      "[3,   276] train loss: 5938.2809245\n",
      "[3,   281] train loss: 5823.2779948\n",
      "[3,   286] train loss: 5744.3195801\n",
      "[3,   291] train loss: 5872.8921712\n",
      "[3,   296] train loss: 5710.4838867\n",
      "[3,   301] train loss: 5948.4600423\n",
      "[3,   306] train loss: 5798.3484701\n",
      "[3,   311] train loss: 5864.4269206\n",
      "[3,   316] train loss: 5677.0880534\n",
      "[3,   321] train loss: 6072.2403158\n",
      "[3,   326] train loss: 5819.5673014\n",
      "[3,   331] train loss: 5633.6010742\n",
      "[3,   336] train loss: 6017.9976400\n",
      "[3,   341] train loss: 5699.1253255\n",
      "[3,   346] train loss: 5576.6040039\n",
      "[3,   351] train loss: 5715.1107585\n",
      "[3,   356] train loss: 5643.9230957\n",
      "[3,   361] train loss: 5901.3557129\n",
      "[3,   366] train loss: 5751.7898763\n",
      "[3,   371] train loss: 5926.5308431\n",
      "[3,   376] train loss: 5915.9785156\n",
      "[3,   381] train loss: 5603.2044271\n",
      "[3,   386] train loss: 5857.1197917\n",
      "[3,   391] train loss: 5824.6777344\n",
      "[3,   396] train loss: 5975.1919759\n",
      "[3,   401] train loss: 5900.0789388\n",
      "[3,   406] train loss: 5722.0942383\n",
      "[3,   411] train loss: 5997.1811523\n",
      "[3,   416] train loss: 6115.5018717\n",
      "[3,   421] train loss: 5981.2513021\n",
      "[3,   426] train loss: 5865.6844889\n",
      "[3,   431] train loss: 5817.0280762\n",
      "[3,   436] train loss: 6030.4207357\n",
      "[3,   441] train loss: 5996.8727214\n",
      "[3,   446] train loss: 5731.2707520\n",
      "[3,   451] train loss: 5961.2693685\n",
      "[3,   456] train loss: 5781.8830566\n",
      "[3,   461] train loss: 5777.5222168\n",
      "[3,   466] train loss: 6079.8951009\n",
      "[3,   471] train loss: 5765.3955892\n",
      "[3,   476] train loss: 5852.4050293\n",
      "[3,   481] train loss: 5707.5715332\n",
      "[3,   486] train loss: 5626.2130534\n",
      "[3,   491] train loss: 5820.1353353\n",
      "[3,   496] train loss: 5826.2486979\n",
      "[3,   501] train loss: 6159.0263672\n",
      "[3,   506] train loss: 5763.0239258\n",
      "[3,   511] train loss: 5587.6332194\n",
      "[3,   516] train loss: 5715.1897786\n",
      "[3,   521] train loss: 6110.5782878\n",
      "[3,   526] train loss: 5877.9766439\n",
      "[3,   531] train loss: 5756.5253092\n",
      "[3,   536] train loss: 5711.4625651\n",
      "[3,   541] train loss: 5798.8281250\n",
      "[3,   546] train loss: 5734.9272461\n",
      "[3,   551] train loss: 5753.7759603\n",
      "[3,   556] train loss: 5668.5133464\n",
      "[3,   561] train loss: 5632.7172038\n",
      "[3,   566] train loss: 5854.6975911\n",
      "[3,   571] train loss: 5802.2483724\n",
      "[3,   576] train loss: 5841.7423503\n",
      "[3,   581] train loss: 5793.8562826\n",
      "[3,   586] train loss: 5525.3172201\n",
      "[3,   591] train loss: 5816.3358561\n",
      "[3,   596] train loss: 5864.4875488\n",
      "[3,   601] train loss: 5758.0558268\n",
      "[3,   606] train loss: 5713.2332357\n",
      "[3,   611] train loss: 6004.6930339\n",
      "[3,   616] train loss: 5660.4891764\n",
      "[3,   621] train loss: 5760.8952637\n",
      "[3,   625] train loss: 5405.3047852\n",
      "#3 reconstruction test loss: 109.38371276855469\n",
      "#3 reconstruction validation loss: 110.08312225341797\n",
      "Epoch #3\n",
      "[4,     1] train loss: 6735.0161133\n",
      "[4,     6] train loss: 5928.3942057\n",
      "[4,    11] train loss: 5860.9571126\n",
      "[4,    16] train loss: 5834.4378255\n",
      "[4,    21] train loss: 5723.9069824\n",
      "[4,    26] train loss: 5760.0869141\n",
      "[4,    31] train loss: 5845.5120443\n",
      "[4,    36] train loss: 5702.5164388\n",
      "[4,    41] train loss: 5867.7697754\n",
      "[4,    46] train loss: 5684.2648112\n",
      "[4,    51] train loss: 5940.4902344\n",
      "[4,    56] train loss: 5988.1714681\n",
      "[4,    61] train loss: 5784.0194499\n",
      "[4,    66] train loss: 5646.2523600\n",
      "[4,    71] train loss: 5759.8133952\n",
      "[4,    76] train loss: 5910.1096191\n",
      "[4,    81] train loss: 5826.1178385\n",
      "[4,    86] train loss: 5833.5993652\n",
      "[4,    91] train loss: 5836.4830729\n",
      "[4,    96] train loss: 5857.1258138\n",
      "[4,   101] train loss: 5738.0119629\n",
      "[4,   106] train loss: 5833.5328776\n",
      "[4,   111] train loss: 5857.8509928\n",
      "[4,   116] train loss: 5805.5599772\n",
      "[4,   121] train loss: 5740.4357096\n",
      "[4,   126] train loss: 5595.7327474\n",
      "[4,   131] train loss: 6034.4078776\n",
      "[4,   136] train loss: 5736.6798503\n",
      "[4,   141] train loss: 6017.5926921\n",
      "[4,   146] train loss: 5966.3676758\n",
      "[4,   151] train loss: 6021.5033366\n",
      "[4,   156] train loss: 5873.9646810\n",
      "[4,   161] train loss: 5717.3151042\n",
      "[4,   166] train loss: 5926.3312174\n",
      "[4,   171] train loss: 5819.7076009\n",
      "[4,   176] train loss: 5854.3243001\n",
      "[4,   181] train loss: 5765.5280762\n",
      "[4,   186] train loss: 5940.4615885\n",
      "[4,   191] train loss: 5718.5124512\n",
      "[4,   196] train loss: 5802.9698079\n",
      "[4,   201] train loss: 6235.8427734\n",
      "[4,   206] train loss: 5829.7813314\n",
      "[4,   211] train loss: 5822.8085938\n",
      "[4,   216] train loss: 5704.0819499\n",
      "[4,   221] train loss: 5523.6340332\n",
      "[4,   226] train loss: 5705.7763672\n",
      "[4,   231] train loss: 6015.7219238\n",
      "[4,   236] train loss: 5924.4113770\n",
      "[4,   241] train loss: 5817.1405436\n",
      "[4,   246] train loss: 5927.4129232\n",
      "[4,   251] train loss: 6212.2456055\n",
      "[4,   256] train loss: 5925.9911296\n",
      "[4,   261] train loss: 5707.8149414\n",
      "[4,   266] train loss: 5841.4700521\n",
      "[4,   271] train loss: 5955.4809570\n",
      "[4,   276] train loss: 5898.9131673\n",
      "[4,   281] train loss: 6008.0039062\n",
      "[4,   286] train loss: 5881.8962402\n",
      "[4,   291] train loss: 5767.1791992\n",
      "[4,   296] train loss: 5756.7289225\n",
      "[4,   301] train loss: 5920.9607747\n",
      "[4,   306] train loss: 5666.3284505\n",
      "[4,   311] train loss: 5740.2294108\n",
      "[4,   316] train loss: 5990.5928548\n",
      "[4,   321] train loss: 5910.9507650\n",
      "[4,   326] train loss: 5768.5068359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   331] train loss: 5714.4501139\n",
      "[4,   336] train loss: 5827.0078125\n",
      "[4,   341] train loss: 5874.0861816\n",
      "[4,   346] train loss: 5880.8726400\n",
      "[4,   351] train loss: 5553.3754883\n",
      "[4,   356] train loss: 5858.2074382\n",
      "[4,   361] train loss: 5895.1873372\n",
      "[4,   366] train loss: 5700.0803223\n",
      "[4,   371] train loss: 5895.6504720\n",
      "[4,   376] train loss: 6065.4798991\n",
      "[4,   381] train loss: 5875.8804525\n",
      "[4,   386] train loss: 5655.1678060\n",
      "[4,   391] train loss: 6026.6853841\n",
      "[4,   396] train loss: 5885.2688802\n",
      "[4,   401] train loss: 5873.5633952\n",
      "[4,   406] train loss: 5909.4222005\n",
      "[4,   411] train loss: 5782.1136068\n",
      "[4,   416] train loss: 5772.5078939\n",
      "[4,   421] train loss: 5649.8448893\n",
      "[4,   426] train loss: 5970.0031738\n",
      "[4,   431] train loss: 5941.5084635\n",
      "[4,   436] train loss: 5942.8809408\n",
      "[4,   441] train loss: 5888.1618652\n",
      "[4,   446] train loss: 5913.8202311\n",
      "[4,   451] train loss: 5878.1209310\n",
      "[4,   456] train loss: 5669.5401204\n",
      "[4,   461] train loss: 5733.0753581\n",
      "[4,   466] train loss: 5705.6443685\n",
      "[4,   471] train loss: 5876.5856934\n",
      "[4,   476] train loss: 6261.3987630\n",
      "[4,   481] train loss: 5886.0939941\n",
      "[4,   486] train loss: 6006.4072266\n",
      "[4,   491] train loss: 5654.3235677\n",
      "[4,   496] train loss: 5943.1192220\n",
      "[4,   501] train loss: 5892.5039062\n",
      "[4,   506] train loss: 5915.8018392\n",
      "[4,   511] train loss: 5848.2397461\n",
      "[4,   516] train loss: 5705.9031576\n",
      "[4,   521] train loss: 5848.0043945\n",
      "[4,   526] train loss: 5839.1752930\n",
      "[4,   531] train loss: 5773.9227702\n",
      "[4,   536] train loss: 6110.8422038\n",
      "[4,   541] train loss: 5327.6682129\n",
      "[4,   546] train loss: 5731.9127604\n",
      "[4,   551] train loss: 5781.7389323\n",
      "[4,   556] train loss: 5810.1840007\n",
      "[4,   561] train loss: 5749.9103190\n",
      "[4,   566] train loss: 6084.0248210\n",
      "[4,   571] train loss: 6013.5546875\n",
      "[4,   576] train loss: 5751.2929688\n",
      "[4,   581] train loss: 5586.1160482\n",
      "[4,   586] train loss: 5718.3719076\n",
      "[4,   591] train loss: 5753.6434733\n",
      "[4,   596] train loss: 5933.7552897\n",
      "[4,   601] train loss: 5724.6532389\n",
      "[4,   606] train loss: 5917.2781576\n",
      "[4,   611] train loss: 5832.2854818\n",
      "[4,   616] train loss: 5913.7151693\n",
      "[4,   621] train loss: 5727.2583008\n",
      "[4,   625] train loss: 5716.3524414\n",
      "#4 reconstruction test loss: 108.9672622680664\n",
      "#4 reconstruction validation loss: 109.82723999023438\n",
      "Epoch #4\n",
      "[5,     1] train loss: 7176.1235352\n",
      "[5,     6] train loss: 5808.5607910\n",
      "[5,    11] train loss: 5957.1151530\n",
      "[5,    16] train loss: 5757.6990560\n",
      "[5,    21] train loss: 5810.3730469\n",
      "[5,    26] train loss: 5742.9358724\n",
      "[5,    31] train loss: 5812.1422526\n",
      "[5,    36] train loss: 5745.4252930\n",
      "[5,    41] train loss: 5782.1027832\n",
      "[5,    46] train loss: 5743.1627604\n",
      "[5,    51] train loss: 5889.3757324\n",
      "[5,    56] train loss: 5703.3890788\n",
      "[5,    61] train loss: 6057.2140299\n",
      "[5,    66] train loss: 5618.6210124\n",
      "[5,    71] train loss: 5954.1692708\n",
      "[5,    76] train loss: 5970.8758952\n",
      "[5,    81] train loss: 5826.0028483\n",
      "[5,    86] train loss: 5933.0384928\n",
      "[5,    91] train loss: 6027.3268229\n",
      "[5,    96] train loss: 5888.9169922\n",
      "[5,   101] train loss: 5697.3249512\n",
      "[5,   106] train loss: 5716.9080404\n",
      "[5,   111] train loss: 5907.2340495\n",
      "[5,   116] train loss: 5755.5391439\n",
      "[5,   121] train loss: 5714.2035319\n",
      "[5,   126] train loss: 5829.3029785\n",
      "[5,   131] train loss: 5891.8532715\n",
      "[5,   136] train loss: 5962.8330892\n",
      "[5,   141] train loss: 5975.2488607\n",
      "[5,   146] train loss: 5987.1994629\n",
      "[5,   151] train loss: 5925.7213542\n",
      "[5,   156] train loss: 6003.5632324\n",
      "[5,   161] train loss: 5753.6362305\n",
      "[5,   166] train loss: 5620.3623861\n",
      "[5,   171] train loss: 5799.5176595\n",
      "[5,   176] train loss: 5575.6070964\n",
      "[5,   181] train loss: 5869.4315592\n",
      "[5,   186] train loss: 5737.4993490\n",
      "[5,   191] train loss: 6004.1888021\n",
      "[5,   196] train loss: 5938.1958008\n",
      "[5,   201] train loss: 6007.4820150\n",
      "[5,   206] train loss: 6222.0814616\n",
      "[5,   211] train loss: 5847.7741699\n",
      "[5,   216] train loss: 5912.6521810\n",
      "[5,   221] train loss: 5839.3206380\n",
      "[5,   226] train loss: 5678.4476725\n",
      "[5,   231] train loss: 5620.5559896\n",
      "[5,   236] train loss: 5467.2939453\n",
      "[5,   241] train loss: 5862.0664062\n",
      "[5,   246] train loss: 6019.9177246\n",
      "[5,   251] train loss: 6168.5383301\n",
      "[5,   256] train loss: 5888.7313639\n",
      "[5,   261] train loss: 5900.6708171\n",
      "[5,   266] train loss: 5903.2719727\n",
      "[5,   271] train loss: 5964.8055013\n",
      "[5,   276] train loss: 5875.8903809\n",
      "[5,   281] train loss: 5851.0500488\n",
      "[5,   286] train loss: 5583.8396810\n",
      "[5,   291] train loss: 5838.9966634\n",
      "[5,   296] train loss: 5947.8769531\n",
      "[5,   301] train loss: 5581.5181478\n",
      "[5,   306] train loss: 5925.4549154\n",
      "[5,   311] train loss: 5857.1285807\n",
      "[5,   316] train loss: 5543.0627441\n",
      "[5,   321] train loss: 5669.7934570\n",
      "[5,   326] train loss: 5706.5396322\n",
      "[5,   331] train loss: 5738.1618652\n",
      "[5,   336] train loss: 5919.9893392\n",
      "[5,   341] train loss: 5729.2870280\n",
      "[5,   346] train loss: 5917.1124674\n",
      "[5,   351] train loss: 5787.7479655\n",
      "[5,   356] train loss: 5736.6914062\n",
      "[5,   361] train loss: 5714.5490723\n",
      "[5,   366] train loss: 5771.8330892\n",
      "[5,   371] train loss: 5908.0375163\n",
      "[5,   376] train loss: 5801.5509440\n",
      "[5,   381] train loss: 5616.8774414\n",
      "[5,   386] train loss: 5809.7446289\n",
      "[5,   391] train loss: 5911.1096191\n",
      "[5,   396] train loss: 5782.5068359\n",
      "[5,   401] train loss: 5809.8568522\n",
      "[5,   406] train loss: 5753.2877604\n",
      "[5,   411] train loss: 5703.8538411\n",
      "[5,   416] train loss: 5683.7220052\n",
      "[5,   421] train loss: 5742.9067383\n",
      "[5,   426] train loss: 5756.6880697\n",
      "[5,   431] train loss: 5907.2097168\n",
      "[5,   436] train loss: 5709.5240072\n",
      "[5,   441] train loss: 5888.9300130\n",
      "[5,   446] train loss: 5733.6145833\n",
      "[5,   451] train loss: 5840.0827637\n",
      "[5,   456] train loss: 5860.1188965\n",
      "[5,   461] train loss: 6111.8607585\n",
      "[5,   466] train loss: 5890.5171712\n",
      "[5,   471] train loss: 5709.8847656\n",
      "[5,   476] train loss: 5799.6398112\n",
      "[5,   481] train loss: 5868.7501628\n",
      "[5,   486] train loss: 5864.3853353\n",
      "[5,   491] train loss: 5725.2794596\n",
      "[5,   496] train loss: 5818.6885579\n",
      "[5,   501] train loss: 5729.6348470\n",
      "[5,   506] train loss: 5980.6981608\n",
      "[5,   511] train loss: 5655.3378092\n",
      "[5,   516] train loss: 6061.1308594\n",
      "[5,   521] train loss: 5908.4589844\n",
      "[5,   526] train loss: 5698.3699544\n",
      "[5,   531] train loss: 5940.4939779\n",
      "[5,   536] train loss: 6004.5783691\n",
      "[5,   541] train loss: 5828.1747233\n",
      "[5,   546] train loss: 5851.6250814\n",
      "[5,   551] train loss: 5993.7959798\n",
      "[5,   556] train loss: 5739.7517090\n",
      "[5,   561] train loss: 5619.5364583\n",
      "[5,   566] train loss: 5814.3653158\n",
      "[5,   571] train loss: 5836.0292155\n",
      "[5,   576] train loss: 5772.6058757\n",
      "[5,   581] train loss: 5756.7889811\n",
      "[5,   586] train loss: 5909.4953613\n",
      "[5,   591] train loss: 5475.5043945\n",
      "[5,   596] train loss: 5809.5633138\n",
      "[5,   601] train loss: 5881.0013835\n",
      "[5,   606] train loss: 5751.0358073\n",
      "[5,   611] train loss: 5710.9892578\n",
      "[5,   616] train loss: 6080.6335449\n",
      "[5,   621] train loss: 5862.4401855\n",
      "[5,   625] train loss: 5623.4032227\n",
      "#5 reconstruction test loss: 108.85502624511719\n",
      "#5 reconstruction validation loss: 109.67237854003906\n",
      "Epoch #5\n",
      "[6,     1] train loss: 6873.9155273\n",
      "[6,     6] train loss: 5740.9652507\n",
      "[6,    11] train loss: 5612.2928874\n",
      "[6,    16] train loss: 5701.0762533\n",
      "[6,    21] train loss: 5535.6795247\n",
      "[6,    26] train loss: 5868.8676758\n",
      "[6,    31] train loss: 5923.1743164\n",
      "[6,    36] train loss: 5786.2160645\n",
      "[6,    41] train loss: 5702.6971842\n",
      "[6,    46] train loss: 5799.3016764\n",
      "[6,    51] train loss: 6105.2992350\n",
      "[6,    56] train loss: 5708.5653483\n",
      "[6,    61] train loss: 6073.3047689\n",
      "[6,    66] train loss: 5735.6729329\n",
      "[6,    71] train loss: 5765.1625977\n",
      "[6,    76] train loss: 6127.5970866\n",
      "[6,    81] train loss: 6049.6743164\n",
      "[6,    86] train loss: 5608.4446615\n",
      "[6,    91] train loss: 5800.0243327\n",
      "[6,    96] train loss: 5611.8489583\n",
      "[6,   101] train loss: 5643.5581055\n",
      "[6,   106] train loss: 5863.7460124\n",
      "[6,   111] train loss: 5834.7552897\n",
      "[6,   116] train loss: 5729.0576172\n",
      "[6,   121] train loss: 6031.7587077\n",
      "[6,   126] train loss: 5574.8867188\n",
      "[6,   131] train loss: 5871.6555176\n",
      "[6,   136] train loss: 5725.2600911\n",
      "[6,   141] train loss: 5736.7255859\n",
      "[6,   146] train loss: 5819.5281576\n",
      "[6,   151] train loss: 6011.8409831\n",
      "[6,   156] train loss: 5485.9802246\n",
      "[6,   161] train loss: 5730.9566243\n",
      "[6,   166] train loss: 5957.1639811\n",
      "[6,   171] train loss: 5774.1120605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,   176] train loss: 5920.8725586\n",
      "[6,   181] train loss: 5846.3420410\n",
      "[6,   186] train loss: 6133.7180990\n",
      "[6,   191] train loss: 5768.3254395\n",
      "[6,   196] train loss: 5687.6822103\n",
      "[6,   201] train loss: 5890.0199382\n",
      "[6,   206] train loss: 5690.6122233\n",
      "[6,   211] train loss: 6116.1923014\n",
      "[6,   216] train loss: 5582.4068197\n",
      "[6,   221] train loss: 5661.9070638\n",
      "[6,   226] train loss: 5825.2491862\n",
      "[6,   231] train loss: 5950.0638021\n",
      "[6,   236] train loss: 5938.7167969\n",
      "[6,   241] train loss: 5880.8504232\n",
      "[6,   246] train loss: 5737.8528646\n",
      "[6,   251] train loss: 5755.0056152\n",
      "[6,   256] train loss: 5643.5147298\n",
      "[6,   261] train loss: 5899.0290527\n",
      "[6,   266] train loss: 6159.5253092\n",
      "[6,   271] train loss: 5874.2879232\n",
      "[6,   276] train loss: 5800.2211100\n",
      "[6,   281] train loss: 5795.7571615\n",
      "[6,   286] train loss: 5723.3738607\n",
      "[6,   291] train loss: 5779.2109375\n",
      "[6,   296] train loss: 5635.4525553\n",
      "[6,   301] train loss: 5953.8827311\n",
      "[6,   306] train loss: 5738.0202637\n",
      "[6,   311] train loss: 6123.5063477\n",
      "[6,   316] train loss: 5945.7057292\n",
      "[6,   321] train loss: 5714.8001302\n",
      "[6,   326] train loss: 5909.1490072\n",
      "[6,   331] train loss: 5811.0930176\n",
      "[6,   336] train loss: 5717.3092448\n",
      "[6,   341] train loss: 5697.9451497\n",
      "[6,   346] train loss: 5762.8410645\n",
      "[6,   351] train loss: 5636.3190104\n",
      "[6,   356] train loss: 5809.8942057\n",
      "[6,   361] train loss: 5924.8022461\n",
      "[6,   366] train loss: 5785.7092285\n",
      "[6,   371] train loss: 6203.5016276\n",
      "[6,   376] train loss: 6016.8947754\n",
      "[6,   381] train loss: 5657.4551595\n",
      "[6,   386] train loss: 5932.8910319\n",
      "[6,   391] train loss: 5600.4243164\n",
      "[6,   396] train loss: 5842.4704590\n",
      "[6,   401] train loss: 5884.7493490\n",
      "[6,   406] train loss: 5642.8152669\n",
      "[6,   411] train loss: 5734.5071615\n",
      "[6,   416] train loss: 6012.9438477\n",
      "[6,   421] train loss: 5800.8710938\n",
      "[6,   426] train loss: 5544.3369141\n",
      "[6,   431] train loss: 5726.2906901\n",
      "[6,   436] train loss: 5419.2084961\n",
      "[6,   441] train loss: 5885.7093099\n",
      "[6,   446] train loss: 5767.3479818\n",
      "[6,   451] train loss: 5967.3872884\n",
      "[6,   456] train loss: 5886.1605632\n",
      "[6,   461] train loss: 5830.3733724\n",
      "[6,   466] train loss: 5798.3214518\n",
      "[6,   471] train loss: 5806.8232422\n",
      "[6,   476] train loss: 5878.3595378\n",
      "[6,   481] train loss: 5761.6033529\n",
      "[6,   486] train loss: 5763.7710775\n",
      "[6,   491] train loss: 5956.7390951\n",
      "[6,   496] train loss: 5888.5874023\n",
      "[6,   501] train loss: 5733.5049642\n",
      "[6,   506] train loss: 5854.0584310\n",
      "[6,   511] train loss: 5599.8714193\n",
      "[6,   516] train loss: 5656.2356771\n",
      "[6,   521] train loss: 5670.5235189\n",
      "[6,   526] train loss: 6143.0405273\n",
      "[6,   531] train loss: 5746.3452962\n",
      "[6,   536] train loss: 5831.5206706\n",
      "[6,   541] train loss: 5670.2985026\n",
      "[6,   546] train loss: 6074.9531250\n",
      "[6,   551] train loss: 5941.5546875\n",
      "[6,   556] train loss: 5914.9185384\n",
      "[6,   561] train loss: 5523.2346191\n",
      "[6,   566] train loss: 5803.1191406\n",
      "[6,   571] train loss: 5718.4462077\n",
      "[6,   576] train loss: 5737.3498535\n",
      "[6,   581] train loss: 5865.4472656\n",
      "[6,   586] train loss: 5936.3145345\n",
      "[6,   591] train loss: 6012.3784993\n",
      "[6,   596] train loss: 5913.9367676\n",
      "[6,   601] train loss: 5866.9429525\n",
      "[6,   606] train loss: 5795.3516439\n",
      "[6,   611] train loss: 5849.4576009\n",
      "[6,   616] train loss: 5770.3542480\n",
      "[6,   621] train loss: 5948.2503255\n",
      "[6,   625] train loss: 5767.9501953\n",
      "#6 reconstruction test loss: 108.68460083007812\n",
      "#6 reconstruction validation loss: 109.55741119384766\n",
      "Epoch #6\n",
      "[7,     1] train loss: 6558.8676758\n",
      "[7,     6] train loss: 6020.6640625\n",
      "[7,    11] train loss: 5484.3393555\n",
      "[7,    16] train loss: 5821.7770996\n",
      "[7,    21] train loss: 5659.7758789\n",
      "[7,    26] train loss: 5523.1503906\n",
      "[7,    31] train loss: 5755.4526367\n",
      "[7,    36] train loss: 5719.1494141\n",
      "[7,    41] train loss: 5974.7469076\n",
      "[7,    46] train loss: 5638.8006185\n",
      "[7,    51] train loss: 5741.0124512\n",
      "[7,    56] train loss: 5878.6695150\n",
      "[7,    61] train loss: 5808.7839355\n",
      "[7,    66] train loss: 5817.4723307\n",
      "[7,    71] train loss: 5752.7518717\n",
      "[7,    76] train loss: 5544.4570312\n",
      "[7,    81] train loss: 5735.7584635\n",
      "[7,    86] train loss: 5381.3205566\n",
      "[7,    91] train loss: 5771.3604329\n",
      "[7,    96] train loss: 5678.5738932\n",
      "[7,   101] train loss: 5697.6471354\n",
      "[7,   106] train loss: 5766.1774089\n",
      "[7,   111] train loss: 6025.7242025\n",
      "[7,   116] train loss: 5754.8972168\n",
      "[7,   121] train loss: 5850.9595540\n",
      "[7,   126] train loss: 5750.1387533\n",
      "[7,   131] train loss: 5611.7753906\n",
      "[7,   136] train loss: 5871.7372233\n",
      "[7,   141] train loss: 5793.5157878\n",
      "[7,   146] train loss: 5738.5686849\n",
      "[7,   151] train loss: 5780.8190104\n",
      "[7,   156] train loss: 5716.3717448\n",
      "[7,   161] train loss: 6060.6933594\n",
      "[7,   166] train loss: 5789.0751953\n",
      "[7,   171] train loss: 5707.4512533\n",
      "[7,   176] train loss: 5591.9938965\n",
      "[7,   181] train loss: 5792.8056641\n",
      "[7,   186] train loss: 6127.4478353\n",
      "[7,   191] train loss: 5837.6388346\n",
      "[7,   196] train loss: 5776.9410807\n",
      "[7,   201] train loss: 6073.3588053\n",
      "[7,   206] train loss: 5831.4643555\n",
      "[7,   211] train loss: 5772.8872884\n",
      "[7,   216] train loss: 5909.2723796\n",
      "[7,   221] train loss: 5729.6219076\n",
      "[7,   226] train loss: 6110.4884440\n",
      "[7,   231] train loss: 5961.3022461\n",
      "[7,   236] train loss: 5825.6405436\n",
      "[7,   241] train loss: 6210.8763021\n",
      "[7,   246] train loss: 5880.1685384\n",
      "[7,   251] train loss: 5470.1361491\n",
      "[7,   256] train loss: 5669.9255371\n",
      "[7,   261] train loss: 5635.7591146\n",
      "[7,   266] train loss: 5755.8106283\n",
      "[7,   271] train loss: 5789.8615723\n",
      "[7,   276] train loss: 5712.6109212\n",
      "[7,   281] train loss: 5927.4725749\n",
      "[7,   286] train loss: 6019.2757161\n",
      "[7,   291] train loss: 5697.8535156\n",
      "[7,   296] train loss: 5731.5240072\n",
      "[7,   301] train loss: 6039.4233398\n",
      "[7,   306] train loss: 6031.3815104\n",
      "[7,   311] train loss: 6005.3078613\n",
      "[7,   316] train loss: 5787.9862467\n",
      "[7,   321] train loss: 5526.2705892\n",
      "[7,   326] train loss: 5861.0568034\n",
      "[7,   331] train loss: 5971.0324707\n",
      "[7,   336] train loss: 5755.9566243\n",
      "[7,   341] train loss: 5651.4816081\n",
      "[7,   346] train loss: 5918.3677572\n",
      "[7,   351] train loss: 5454.0989583\n",
      "[7,   356] train loss: 5987.4308268\n",
      "[7,   361] train loss: 5855.1939290\n",
      "[7,   366] train loss: 5974.1663411\n",
      "[7,   371] train loss: 5632.7762044\n",
      "[7,   376] train loss: 5674.1631673\n",
      "[7,   381] train loss: 5753.0409342\n",
      "[7,   386] train loss: 5676.6344401\n",
      "[7,   391] train loss: 5611.9873861\n",
      "[7,   396] train loss: 5743.8811035\n",
      "[7,   401] train loss: 5968.1449382\n",
      "[7,   406] train loss: 5720.6877441\n",
      "[7,   411] train loss: 5808.9544271\n",
      "[7,   416] train loss: 5726.0088704\n",
      "[7,   421] train loss: 6085.7190755\n",
      "[7,   426] train loss: 6002.2238770\n",
      "[7,   431] train loss: 5757.7663574\n",
      "[7,   436] train loss: 5926.2992350\n",
      "[7,   441] train loss: 5881.2611491\n",
      "[7,   446] train loss: 5807.1318359\n",
      "[7,   451] train loss: 5711.4209798\n",
      "[7,   456] train loss: 5842.1499837\n",
      "[7,   461] train loss: 5926.0912272\n",
      "[7,   466] train loss: 5731.6185710\n",
      "[7,   471] train loss: 5658.5715332\n",
      "[7,   476] train loss: 5554.0653483\n",
      "[7,   481] train loss: 6288.8551432\n",
      "[7,   486] train loss: 5974.3785807\n",
      "[7,   491] train loss: 5766.3696289\n",
      "[7,   496] train loss: 5809.0030111\n",
      "[7,   501] train loss: 5738.0856120\n",
      "[7,   506] train loss: 5762.6423340\n",
      "[7,   511] train loss: 6058.3889974\n",
      "[7,   516] train loss: 5924.5590820\n",
      "[7,   521] train loss: 5903.3470866\n",
      "[7,   526] train loss: 5584.8249512\n",
      "[7,   531] train loss: 5710.8875326\n",
      "[7,   536] train loss: 5869.1036784\n",
      "[7,   541] train loss: 5730.5279134\n",
      "[7,   546] train loss: 6078.4339193\n",
      "[7,   551] train loss: 5747.4768880\n",
      "[7,   556] train loss: 6005.7749837\n",
      "[7,   561] train loss: 5916.7906901\n",
      "[7,   566] train loss: 5886.8923340\n",
      "[7,   571] train loss: 5839.2373047\n",
      "[7,   576] train loss: 5888.5489909\n",
      "[7,   581] train loss: 5899.6768392\n",
      "[7,   586] train loss: 5805.6749674\n",
      "[7,   591] train loss: 5708.6630046\n",
      "[7,   596] train loss: 5674.9365234\n",
      "[7,   601] train loss: 5714.9215495\n",
      "[7,   606] train loss: 5584.9130046\n",
      "[7,   611] train loss: 5855.5712077\n",
      "[7,   616] train loss: 5597.0399577\n",
      "[7,   621] train loss: 5895.8981120\n",
      "[7,   625] train loss: 5734.6131836\n",
      "#7 reconstruction test loss: 108.71685028076172\n",
      "#7 reconstruction validation loss: 109.6540756225586\n",
      "Epoch #7\n",
      "[8,     1] train loss: 6776.4926758\n",
      "[8,     6] train loss: 5740.5725911\n",
      "[8,    11] train loss: 5761.9270020\n",
      "[8,    16] train loss: 6210.9881185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,    21] train loss: 5806.1759440\n",
      "[8,    26] train loss: 5748.3616536\n",
      "[8,    31] train loss: 5804.6067708\n",
      "[8,    36] train loss: 5841.6226400\n",
      "[8,    41] train loss: 5837.9134928\n",
      "[8,    46] train loss: 5684.7011719\n",
      "[8,    51] train loss: 5605.8576660\n",
      "[8,    56] train loss: 5548.7580566\n",
      "[8,    61] train loss: 5792.0468750\n",
      "[8,    66] train loss: 5690.9199219\n",
      "[8,    71] train loss: 5624.7342122\n",
      "[8,    76] train loss: 5999.9531250\n",
      "[8,    81] train loss: 5792.4306641\n",
      "[8,    86] train loss: 6142.3107910\n",
      "[8,    91] train loss: 6081.9767253\n",
      "[8,    96] train loss: 5767.3010254\n",
      "[8,   101] train loss: 5742.3252767\n",
      "[8,   106] train loss: 5722.3289388\n",
      "[8,   111] train loss: 5644.2092285\n",
      "[8,   116] train loss: 6024.6427409\n",
      "[8,   121] train loss: 5933.1539714\n",
      "[8,   126] train loss: 5843.1276042\n",
      "[8,   131] train loss: 5628.6302083\n",
      "[8,   136] train loss: 5685.3620605\n",
      "[8,   141] train loss: 5846.7691243\n",
      "[8,   146] train loss: 5687.9183757\n",
      "[8,   151] train loss: 6137.5760905\n",
      "[8,   156] train loss: 6048.7002767\n",
      "[8,   161] train loss: 5689.1496582\n",
      "[8,   166] train loss: 5733.2039388\n",
      "[8,   171] train loss: 6038.1194661\n",
      "[8,   176] train loss: 5572.0325521\n",
      "[8,   181] train loss: 5596.2733561\n",
      "[8,   186] train loss: 6038.8816732\n",
      "[8,   191] train loss: 5867.4041341\n",
      "[8,   196] train loss: 5703.3627930\n",
      "[8,   201] train loss: 5851.9745280\n",
      "[8,   206] train loss: 5578.4141439\n",
      "[8,   211] train loss: 5586.0288900\n",
      "[8,   216] train loss: 5827.0998535\n",
      "[8,   221] train loss: 5643.6420085\n",
      "[8,   226] train loss: 5627.2985840\n",
      "[8,   231] train loss: 5655.9058431\n",
      "[8,   236] train loss: 5816.3362630\n",
      "[8,   241] train loss: 5867.3675944\n",
      "[8,   246] train loss: 6011.6845703\n",
      "[8,   251] train loss: 5730.4771322\n",
      "[8,   256] train loss: 5900.2395833\n",
      "[8,   261] train loss: 5760.4813639\n",
      "[8,   266] train loss: 5900.2646484\n",
      "[8,   271] train loss: 6025.0781250\n",
      "[8,   276] train loss: 5746.5024414\n",
      "[8,   281] train loss: 5657.2317708\n",
      "[8,   286] train loss: 5560.0837402\n",
      "[8,   291] train loss: 5877.1429850\n",
      "[8,   296] train loss: 5766.5454915\n",
      "[8,   301] train loss: 5776.7338867\n",
      "[8,   306] train loss: 6177.1035156\n",
      "[8,   311] train loss: 5762.7997233\n",
      "[8,   316] train loss: 6042.5706380\n",
      "[8,   321] train loss: 5673.4115397\n",
      "[8,   326] train loss: 5796.1826986\n",
      "[8,   331] train loss: 5419.4278158\n",
      "[8,   336] train loss: 5624.1025391\n",
      "[8,   341] train loss: 6151.3296712\n",
      "[8,   346] train loss: 5809.2430013\n",
      "[8,   351] train loss: 5844.9690755\n",
      "[8,   356] train loss: 5623.0911458\n",
      "[8,   361] train loss: 5933.2685547\n",
      "[8,   366] train loss: 5883.8124186\n",
      "[8,   371] train loss: 5622.8450521\n",
      "[8,   376] train loss: 5814.1651204\n",
      "[8,   381] train loss: 5662.0990397\n",
      "[8,   386] train loss: 5626.5919596\n",
      "[8,   391] train loss: 6086.1262207\n",
      "[8,   396] train loss: 5593.7195638\n",
      "[8,   401] train loss: 5758.6133626\n",
      "[8,   406] train loss: 5871.5477702\n",
      "[8,   411] train loss: 5748.2159831\n",
      "[8,   416] train loss: 5576.9706217\n",
      "[8,   421] train loss: 5648.3105469\n",
      "[8,   426] train loss: 5775.6120605\n",
      "[8,   431] train loss: 5587.1207682\n",
      "[8,   436] train loss: 5702.4661458\n",
      "[8,   441] train loss: 5533.7506510\n",
      "[8,   446] train loss: 5541.7988281\n",
      "[8,   451] train loss: 6105.7572428\n",
      "[8,   456] train loss: 5803.6341146\n",
      "[8,   461] train loss: 5524.2461751\n",
      "[8,   466] train loss: 5958.0363770\n",
      "[8,   471] train loss: 5717.5228678\n",
      "[8,   476] train loss: 5786.5780436\n",
      "[8,   481] train loss: 5779.8682454\n",
      "[8,   486] train loss: 5842.6289876\n",
      "[8,   491] train loss: 6065.7999674\n",
      "[8,   496] train loss: 5976.4624023\n",
      "[8,   501] train loss: 5581.4943848\n",
      "[8,   506] train loss: 5732.2259928\n",
      "[8,   511] train loss: 5791.7531738\n",
      "[8,   516] train loss: 6163.0271810\n",
      "[8,   521] train loss: 5802.1601562\n",
      "[8,   526] train loss: 5763.0822754\n",
      "[8,   531] train loss: 5772.7192383\n",
      "[8,   536] train loss: 5898.1643880\n",
      "[8,   541] train loss: 5763.4990234\n",
      "[8,   546] train loss: 5761.0882161\n",
      "[8,   551] train loss: 5865.6621094\n",
      "[8,   556] train loss: 5944.4204102\n",
      "[8,   561] train loss: 5944.6381836\n",
      "[8,   566] train loss: 6019.0784505\n",
      "[8,   571] train loss: 5917.4771322\n",
      "[8,   576] train loss: 5832.7779134\n",
      "[8,   581] train loss: 5874.7408040\n",
      "[8,   586] train loss: 5863.9853516\n",
      "[8,   591] train loss: 5927.5762533\n",
      "[8,   596] train loss: 5884.4165039\n",
      "[8,   601] train loss: 5757.3694661\n",
      "[8,   606] train loss: 5814.1782227\n",
      "[8,   611] train loss: 5680.9003906\n",
      "[8,   616] train loss: 5782.3902995\n",
      "[8,   621] train loss: 5946.5249023\n",
      "[8,   625] train loss: 5479.0658203\n",
      "#8 reconstruction test loss: 107.74659729003906\n",
      "#8 reconstruction validation loss: 109.15699005126953\n",
      "Epoch #8\n",
      "[9,     1] train loss: 6338.5351562\n",
      "[9,     6] train loss: 5452.5459798\n",
      "[9,    11] train loss: 5858.8391113\n",
      "[9,    16] train loss: 5868.2872721\n",
      "[9,    21] train loss: 5833.3509928\n",
      "[9,    26] train loss: 5637.7666016\n",
      "[9,    31] train loss: 6102.1774902\n",
      "[9,    36] train loss: 5687.9468587\n",
      "[9,    41] train loss: 5619.5780436\n",
      "[9,    46] train loss: 5845.1123047\n",
      "[9,    51] train loss: 5953.9225260\n",
      "[9,    56] train loss: 5953.2077637\n",
      "[9,    61] train loss: 5714.9769694\n",
      "[9,    66] train loss: 5816.8347982\n",
      "[9,    71] train loss: 5880.2604980\n",
      "[9,    76] train loss: 5581.9373372\n",
      "[9,    81] train loss: 5625.2858073\n",
      "[9,    86] train loss: 5755.8045247\n",
      "[9,    91] train loss: 5668.8917643\n",
      "[9,    96] train loss: 5615.6795247\n",
      "[9,   101] train loss: 5806.3145345\n",
      "[9,   106] train loss: 5785.9249674\n",
      "[9,   111] train loss: 5666.8213704\n",
      "[9,   116] train loss: 5740.2767741\n",
      "[9,   121] train loss: 5952.7514648\n",
      "[9,   126] train loss: 5827.0174154\n",
      "[9,   131] train loss: 5834.2740072\n",
      "[9,   136] train loss: 5552.0882161\n",
      "[9,   141] train loss: 5862.4734701\n",
      "[9,   146] train loss: 5920.5166016\n",
      "[9,   151] train loss: 5683.7465820\n",
      "[9,   156] train loss: 5799.3873698\n",
      "[9,   161] train loss: 5805.2963053\n",
      "[9,   166] train loss: 5655.8075358\n",
      "[9,   171] train loss: 5897.2994792\n",
      "[9,   176] train loss: 5660.9313151\n",
      "[9,   181] train loss: 5523.0779622\n",
      "[9,   186] train loss: 5620.2220866\n",
      "[9,   191] train loss: 5860.7530924\n",
      "[9,   196] train loss: 5954.1661784\n",
      "[9,   201] train loss: 5949.6612956\n",
      "[9,   206] train loss: 5653.3612467\n",
      "[9,   211] train loss: 5520.6024577\n",
      "[9,   216] train loss: 6025.2458496\n",
      "[9,   221] train loss: 5689.9881185\n",
      "[9,   226] train loss: 5783.1841634\n",
      "[9,   231] train loss: 5748.0413411\n",
      "[9,   236] train loss: 5739.0938314\n",
      "[9,   241] train loss: 6070.7744141\n",
      "[9,   246] train loss: 5755.0270996\n",
      "[9,   251] train loss: 5717.5967611\n",
      "[9,   256] train loss: 5813.5661621\n",
      "[9,   261] train loss: 5927.8021647\n",
      "[9,   266] train loss: 5898.8572591\n",
      "[9,   271] train loss: 5808.7788900\n",
      "[9,   276] train loss: 5673.6329753\n",
      "[9,   281] train loss: 5832.0748698\n",
      "[9,   286] train loss: 5827.3716634\n",
      "[9,   291] train loss: 5829.0463053\n",
      "[9,   296] train loss: 5997.2159831\n",
      "[9,   301] train loss: 5754.4112142\n",
      "[9,   306] train loss: 5767.2111816\n",
      "[9,   311] train loss: 5726.2718099\n",
      "[9,   316] train loss: 5598.2931315\n",
      "[9,   321] train loss: 5698.5458984\n",
      "[9,   326] train loss: 5796.4382324\n",
      "[9,   331] train loss: 5783.3839518\n",
      "[9,   336] train loss: 5963.0693359\n",
      "[9,   341] train loss: 5725.5578613\n",
      "[9,   346] train loss: 5853.3998210\n",
      "[9,   351] train loss: 5689.1748861\n",
      "[9,   356] train loss: 5643.9804688\n",
      "[9,   361] train loss: 5677.2976074\n",
      "[9,   366] train loss: 5955.9946289\n",
      "[9,   371] train loss: 5854.1898600\n",
      "[9,   376] train loss: 5560.0748698\n",
      "[9,   381] train loss: 5877.8146973\n",
      "[9,   386] train loss: 5528.5780436\n",
      "[9,   391] train loss: 5523.6481120\n",
      "[9,   396] train loss: 5715.4667155\n",
      "[9,   401] train loss: 5723.0112305\n",
      "[9,   406] train loss: 5866.9383138\n",
      "[9,   411] train loss: 6030.4707031\n",
      "[9,   416] train loss: 6001.6721191\n",
      "[9,   421] train loss: 5618.7529297\n",
      "[9,   426] train loss: 5504.7757161\n",
      "[9,   431] train loss: 5858.7110189\n",
      "[9,   436] train loss: 5774.7220052\n",
      "[9,   441] train loss: 5663.1416829\n",
      "[9,   446] train loss: 6180.2757161\n",
      "[9,   451] train loss: 5787.4565430\n",
      "[9,   456] train loss: 5805.1525879\n",
      "[9,   461] train loss: 5720.8809408\n",
      "[9,   466] train loss: 5620.7988281\n",
      "[9,   471] train loss: 5706.3127441\n",
      "[9,   476] train loss: 5851.3683268\n",
      "[9,   481] train loss: 5955.1822917\n",
      "[9,   486] train loss: 5904.5672201\n",
      "[9,   491] train loss: 5932.3457845\n",
      "[9,   496] train loss: 5788.9729818\n",
      "[9,   501] train loss: 5783.5223796\n",
      "[9,   506] train loss: 5817.8592936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,   511] train loss: 5519.4671224\n",
      "[9,   516] train loss: 5951.2121582\n",
      "[9,   521] train loss: 5520.5595703\n",
      "[9,   526] train loss: 5829.6626790\n",
      "[9,   531] train loss: 5699.6500651\n",
      "[9,   536] train loss: 5886.5827637\n",
      "[9,   541] train loss: 5763.7381185\n",
      "[9,   546] train loss: 5676.2166341\n",
      "[9,   551] train loss: 6119.4140625\n",
      "[9,   556] train loss: 5621.8595378\n",
      "[9,   561] train loss: 5864.1541341\n",
      "[9,   566] train loss: 5588.0641276\n",
      "[9,   571] train loss: 5838.4346517\n",
      "[9,   576] train loss: 5711.3669434\n",
      "[9,   581] train loss: 5814.8123372\n",
      "[9,   586] train loss: 5989.9416504\n",
      "[9,   591] train loss: 5727.0204264\n",
      "[9,   596] train loss: 6049.4667969\n",
      "[9,   601] train loss: 5866.0513509\n",
      "[9,   606] train loss: 5849.4353027\n",
      "[9,   611] train loss: 5644.0955404\n",
      "[9,   616] train loss: 5676.1210938\n",
      "[9,   621] train loss: 5667.5866699\n",
      "[9,   625] train loss: 5514.8204102\n",
      "#9 reconstruction test loss: 108.68696594238281\n",
      "#9 reconstruction validation loss: 109.05157470703125\n",
      "Epoch #9\n",
      "[10,     1] train loss: 6814.1123047\n",
      "[10,     6] train loss: 5667.7018229\n",
      "[10,    11] train loss: 5884.2695312\n",
      "[10,    16] train loss: 5881.7294922\n",
      "[10,    21] train loss: 5785.5188802\n",
      "[10,    26] train loss: 5854.8237305\n",
      "[10,    31] train loss: 5708.8745117\n",
      "[10,    36] train loss: 5882.1189779\n",
      "[10,    41] train loss: 5825.3015137\n",
      "[10,    46] train loss: 5595.2328288\n",
      "[10,    51] train loss: 5682.2574056\n",
      "[10,    56] train loss: 5742.3821615\n",
      "[10,    61] train loss: 5496.4766439\n",
      "[10,    66] train loss: 5637.3673503\n",
      "[10,    71] train loss: 5867.9635417\n",
      "[10,    76] train loss: 5857.4895833\n",
      "[10,    81] train loss: 5679.0109049\n",
      "[10,    86] train loss: 5942.9840495\n",
      "[10,    91] train loss: 5740.5703125\n",
      "[10,    96] train loss: 6004.1203613\n",
      "[10,   101] train loss: 5570.8261719\n",
      "[10,   106] train loss: 5844.5172526\n",
      "[10,   111] train loss: 5960.7099609\n",
      "[10,   116] train loss: 5829.0383301\n",
      "[10,   121] train loss: 5813.6652018\n",
      "[10,   126] train loss: 5645.5466309\n",
      "[10,   131] train loss: 5926.4310710\n",
      "[10,   136] train loss: 5818.3911947\n",
      "[10,   141] train loss: 5945.0525716\n",
      "[10,   146] train loss: 5658.1255697\n",
      "[10,   151] train loss: 5930.9681803\n",
      "[10,   156] train loss: 5681.5845540\n",
      "[10,   161] train loss: 5898.3918457\n",
      "[10,   166] train loss: 5541.2192383\n",
      "[10,   171] train loss: 6119.8179525\n",
      "[10,   176] train loss: 5783.4612630\n",
      "[10,   181] train loss: 6051.3319499\n",
      "[10,   186] train loss: 5821.6738281\n",
      "[10,   191] train loss: 5809.6823730\n",
      "[10,   196] train loss: 5589.8826497\n",
      "[10,   201] train loss: 5494.3955892\n",
      "[10,   206] train loss: 5748.3420410\n",
      "[10,   211] train loss: 5731.3376465\n",
      "[10,   216] train loss: 5787.0637207\n",
      "[10,   221] train loss: 5626.0738932\n",
      "[10,   226] train loss: 5642.8523763\n",
      "[10,   231] train loss: 5921.2755534\n",
      "[10,   236] train loss: 5847.7667643\n",
      "[10,   241] train loss: 5943.1689453\n",
      "[10,   246] train loss: 5658.7477214\n",
      "[10,   251] train loss: 5708.8054199\n",
      "[10,   256] train loss: 5804.3181966\n",
      "[10,   261] train loss: 5885.6097819\n",
      "[10,   266] train loss: 5889.3754883\n",
      "[10,   271] train loss: 5640.8690592\n",
      "[10,   276] train loss: 5880.8552246\n",
      "[10,   281] train loss: 5685.3164876\n",
      "[10,   286] train loss: 5668.5037435\n",
      "[10,   291] train loss: 5708.6625977\n",
      "[10,   296] train loss: 5705.8158366\n",
      "[10,   301] train loss: 5792.7902832\n",
      "[10,   306] train loss: 5709.1302083\n",
      "[10,   311] train loss: 5692.1930339\n",
      "[10,   316] train loss: 5892.7906901\n",
      "[10,   321] train loss: 5768.5014648\n",
      "[10,   326] train loss: 5815.8277181\n",
      "[10,   331] train loss: 6148.7577311\n",
      "[10,   336] train loss: 5867.5089518\n",
      "[10,   341] train loss: 5814.5310059\n",
      "[10,   346] train loss: 5775.9449870\n",
      "[10,   351] train loss: 5442.6738281\n",
      "[10,   356] train loss: 5851.1158040\n",
      "[10,   361] train loss: 5874.7359212\n",
      "[10,   366] train loss: 5582.0730794\n",
      "[10,   371] train loss: 5592.3156738\n",
      "[10,   376] train loss: 5509.6837565\n",
      "[10,   381] train loss: 5882.3721517\n",
      "[10,   386] train loss: 5754.9920247\n",
      "[10,   391] train loss: 5842.9430339\n",
      "[10,   396] train loss: 5897.1427409\n",
      "[10,   401] train loss: 5622.7520345\n",
      "[10,   406] train loss: 5714.4100749\n",
      "[10,   411] train loss: 5729.5371094\n",
      "[10,   416] train loss: 5802.4379069\n",
      "[10,   421] train loss: 5953.9434408\n",
      "[10,   426] train loss: 5911.4805501\n",
      "[10,   431] train loss: 5657.5015462\n",
      "[10,   436] train loss: 5803.6370443\n",
      "[10,   441] train loss: 5767.7292480\n",
      "[10,   446] train loss: 5783.5554199\n",
      "[10,   451] train loss: 5763.5110677\n",
      "[10,   456] train loss: 6022.1349284\n",
      "[10,   461] train loss: 5816.6503092\n",
      "[10,   466] train loss: 5533.4675293\n",
      "[10,   471] train loss: 5657.0502116\n",
      "[10,   476] train loss: 5771.4779460\n",
      "[10,   481] train loss: 5732.5310872\n",
      "[10,   486] train loss: 5809.7169596\n",
      "[10,   491] train loss: 5683.3198242\n",
      "[10,   496] train loss: 5733.9261068\n",
      "[10,   501] train loss: 5893.6250814\n",
      "[10,   506] train loss: 5793.0819499\n",
      "[10,   511] train loss: 6004.5137533\n",
      "[10,   516] train loss: 5962.3455404\n",
      "[10,   521] train loss: 5865.7430827\n",
      "[10,   526] train loss: 5773.4030762\n",
      "[10,   531] train loss: 5861.5827637\n",
      "[10,   536] train loss: 5810.2150879\n",
      "[10,   541] train loss: 5434.7448730\n",
      "[10,   546] train loss: 5543.3048503\n",
      "[10,   551] train loss: 5660.9790039\n",
      "[10,   556] train loss: 5895.6835124\n",
      "[10,   561] train loss: 5963.4742839\n",
      "[10,   566] train loss: 5639.8555501\n",
      "[10,   571] train loss: 5799.8870443\n",
      "[10,   576] train loss: 5632.4930827\n",
      "[10,   581] train loss: 5623.4776204\n",
      "[10,   586] train loss: 5855.7722168\n",
      "[10,   591] train loss: 5843.0175781\n",
      "[10,   596] train loss: 5499.1065267\n",
      "[10,   601] train loss: 5517.0052897\n",
      "[10,   606] train loss: 5769.8000488\n",
      "[10,   611] train loss: 6068.2304688\n",
      "[10,   616] train loss: 5742.5380859\n",
      "[10,   621] train loss: 5714.3401693\n",
      "[10,   625] train loss: 5536.6062500\n",
      "#10 reconstruction test loss: 108.21965026855469\n",
      "#10 reconstruction validation loss: 108.9228286743164\n",
      "Epoch #10\n",
      "[11,     1] train loss: 6424.3051758\n",
      "[11,     6] train loss: 5834.7316895\n",
      "[11,    11] train loss: 5786.0786947\n",
      "[11,    16] train loss: 5713.9530436\n",
      "[11,    21] train loss: 5558.7280273\n",
      "[11,    26] train loss: 5761.8195801\n",
      "[11,    31] train loss: 5747.8869629\n",
      "[11,    36] train loss: 5844.4023438\n",
      "[11,    41] train loss: 5775.1481934\n",
      "[11,    46] train loss: 5917.9958496\n",
      "[11,    51] train loss: 5687.8757324\n",
      "[11,    56] train loss: 5903.2618815\n",
      "[11,    61] train loss: 5795.9926758\n",
      "[11,    66] train loss: 5540.0843099\n",
      "[11,    71] train loss: 5766.3331706\n",
      "[11,    76] train loss: 5669.1053874\n",
      "[11,    81] train loss: 5806.6041667\n",
      "[11,    86] train loss: 5886.3558757\n",
      "[11,    91] train loss: 6021.6586914\n",
      "[11,    96] train loss: 5577.9728190\n",
      "[11,   101] train loss: 5788.5886230\n",
      "[11,   106] train loss: 5921.1128743\n",
      "[11,   111] train loss: 5748.9050293\n",
      "[11,   116] train loss: 5749.2928060\n",
      "[11,   121] train loss: 6106.3293457\n",
      "[11,   126] train loss: 5963.4283040\n",
      "[11,   131] train loss: 5696.8272298\n",
      "[11,   136] train loss: 5899.7036947\n",
      "[11,   141] train loss: 5713.3603516\n",
      "[11,   146] train loss: 5786.9597168\n",
      "[11,   151] train loss: 5518.2422689\n",
      "[11,   156] train loss: 5736.2749023\n",
      "[11,   161] train loss: 5451.9768066\n",
      "[11,   166] train loss: 5735.8678385\n",
      "[11,   171] train loss: 5898.3804525\n",
      "[11,   176] train loss: 6011.7080892\n",
      "[11,   181] train loss: 5911.2553711\n",
      "[11,   186] train loss: 5940.6316732\n",
      "[11,   191] train loss: 5761.4752604\n",
      "[11,   196] train loss: 5906.7941895\n",
      "[11,   201] train loss: 5945.3331706\n",
      "[11,   206] train loss: 5771.8541667\n",
      "[11,   211] train loss: 5808.4790039\n",
      "[11,   216] train loss: 5660.9812012\n",
      "[11,   221] train loss: 5949.0161133\n",
      "[11,   226] train loss: 5801.8675944\n",
      "[11,   231] train loss: 5846.0467122\n",
      "[11,   236] train loss: 5850.2970378\n",
      "[11,   241] train loss: 5721.4846191\n",
      "[11,   246] train loss: 5899.9060872\n",
      "[11,   251] train loss: 5691.6306966\n",
      "[11,   256] train loss: 5754.0776367\n",
      "[11,   261] train loss: 5774.7351074\n",
      "[11,   266] train loss: 5777.1952311\n",
      "[11,   271] train loss: 5678.1787109\n",
      "[11,   276] train loss: 5917.6101888\n",
      "[11,   281] train loss: 5771.7002767\n",
      "[11,   286] train loss: 5693.3902995\n",
      "[11,   291] train loss: 5924.9398600\n",
      "[11,   296] train loss: 5714.1555176\n",
      "[11,   301] train loss: 5785.7471517\n",
      "[11,   306] train loss: 5850.6342773\n",
      "[11,   311] train loss: 5700.3115234\n",
      "[11,   316] train loss: 5859.3715007\n",
      "[11,   321] train loss: 5876.2165527\n",
      "[11,   326] train loss: 5599.2444661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11,   331] train loss: 5635.5139160\n",
      "[11,   336] train loss: 5852.2898763\n",
      "[11,   341] train loss: 5532.5003255\n",
      "[11,   346] train loss: 5802.8024902\n",
      "[11,   351] train loss: 5784.9750163\n",
      "[11,   356] train loss: 5356.2211100\n",
      "[11,   361] train loss: 5599.9878743\n",
      "[11,   366] train loss: 5308.9921875\n",
      "[11,   371] train loss: 5949.2568359\n",
      "[11,   376] train loss: 5874.4941406\n",
      "[11,   381] train loss: 5669.2674154\n",
      "[11,   386] train loss: 5845.7819824\n",
      "[11,   391] train loss: 5992.5585938\n",
      "[11,   396] train loss: 5623.0834147\n",
      "[11,   401] train loss: 5856.9790853\n",
      "[11,   406] train loss: 5744.2747396\n",
      "[11,   411] train loss: 5936.0983887\n",
      "[11,   416] train loss: 5711.9720052\n",
      "[11,   421] train loss: 5688.3970540\n",
      "[11,   426] train loss: 5669.4414062\n",
      "[11,   431] train loss: 5655.6661784\n",
      "[11,   436] train loss: 5769.0878092\n",
      "[11,   441] train loss: 5812.3725586\n",
      "[11,   446] train loss: 5515.0170898\n",
      "[11,   451] train loss: 5969.7730306\n",
      "[11,   456] train loss: 5728.4331055\n",
      "[11,   461] train loss: 5640.9700521\n",
      "[11,   466] train loss: 5836.5411784\n",
      "[11,   471] train loss: 5806.7203776\n",
      "[11,   476] train loss: 5720.9393717\n",
      "[11,   481] train loss: 5548.6844076\n",
      "[11,   486] train loss: 5641.4583333\n",
      "[11,   491] train loss: 5654.2887370\n",
      "[11,   496] train loss: 5805.2006022\n",
      "[11,   501] train loss: 5832.6638997\n",
      "[11,   506] train loss: 5857.8518066\n",
      "[11,   511] train loss: 5960.5851237\n",
      "[11,   516] train loss: 5760.3237305\n",
      "[11,   521] train loss: 5686.5233561\n",
      "[11,   526] train loss: 5637.3064779\n",
      "[11,   531] train loss: 5702.4078776\n",
      "[11,   536] train loss: 5763.4108073\n",
      "[11,   541] train loss: 5872.8702799\n",
      "[11,   546] train loss: 5653.2703451\n",
      "[11,   551] train loss: 5887.0872396\n",
      "[11,   556] train loss: 5891.1691895\n",
      "[11,   561] train loss: 5588.0344238\n",
      "[11,   566] train loss: 5905.0337728\n",
      "[11,   571] train loss: 5844.1017253\n",
      "[11,   576] train loss: 6051.4144694\n",
      "[11,   581] train loss: 5529.3265788\n",
      "[11,   586] train loss: 5630.0139974\n",
      "[11,   591] train loss: 5587.0544434\n",
      "[11,   596] train loss: 6155.1853027\n",
      "[11,   601] train loss: 5810.7952474\n",
      "[11,   606] train loss: 5889.1105143\n",
      "[11,   611] train loss: 5835.5603027\n",
      "[11,   616] train loss: 5827.6622721\n",
      "[11,   621] train loss: 5585.3087565\n",
      "[11,   625] train loss: 5511.1561523\n",
      "#11 reconstruction test loss: 107.69036102294922\n",
      "#11 reconstruction validation loss: 108.72384643554688\n",
      "Epoch #11\n",
      "[12,     1] train loss: 7121.4550781\n",
      "[12,     6] train loss: 5721.9358724\n",
      "[12,    11] train loss: 5874.4649251\n",
      "[12,    16] train loss: 5868.7278646\n",
      "[12,    21] train loss: 5778.6373698\n",
      "[12,    26] train loss: 5695.6772461\n",
      "[12,    31] train loss: 5599.8033040\n",
      "[12,    36] train loss: 5804.0903320\n",
      "[12,    41] train loss: 5694.4042969\n",
      "[12,    46] train loss: 5811.6401367\n",
      "[12,    51] train loss: 5736.3037923\n",
      "[12,    56] train loss: 5830.9822591\n",
      "[12,    61] train loss: 5889.0989583\n",
      "[12,    66] train loss: 5545.6669108\n",
      "[12,    71] train loss: 5838.5655924\n",
      "[12,    76] train loss: 5596.6336263\n",
      "[12,    81] train loss: 5685.1442871\n",
      "[12,    86] train loss: 5497.8364258\n",
      "[12,    91] train loss: 5825.0183919\n",
      "[12,    96] train loss: 5893.0077311\n",
      "[12,   101] train loss: 5586.4613444\n",
      "[12,   106] train loss: 5718.5442708\n",
      "[12,   111] train loss: 5878.2098796\n",
      "[12,   116] train loss: 5723.4303385\n",
      "[12,   121] train loss: 5945.6795247\n",
      "[12,   126] train loss: 5571.8026530\n",
      "[12,   131] train loss: 5641.0533854\n",
      "[12,   136] train loss: 5632.1284993\n",
      "[12,   141] train loss: 5502.2245280\n",
      "[12,   146] train loss: 5700.4342448\n",
      "[12,   151] train loss: 5736.0377604\n",
      "[12,   156] train loss: 5583.8203125\n",
      "[12,   161] train loss: 5622.5354818\n",
      "[12,   166] train loss: 5877.3634440\n",
      "[12,   171] train loss: 5621.2474772\n",
      "[12,   176] train loss: 5704.2055664\n",
      "[12,   181] train loss: 5896.9784342\n",
      "[12,   186] train loss: 5846.5355632\n",
      "[12,   191] train loss: 5736.8223470\n",
      "[12,   196] train loss: 5600.0100098\n",
      "[12,   201] train loss: 5755.6398926\n",
      "[12,   206] train loss: 5506.0709635\n",
      "[12,   211] train loss: 5793.4895833\n",
      "[12,   216] train loss: 5885.8553874\n",
      "[12,   221] train loss: 5763.4868164\n",
      "[12,   226] train loss: 5938.1278483\n",
      "[12,   231] train loss: 5861.9488118\n",
      "[12,   236] train loss: 5744.1765951\n",
      "[12,   241] train loss: 5710.1820475\n",
      "[12,   246] train loss: 6061.8005371\n",
      "[12,   251] train loss: 6075.8933919\n",
      "[12,   256] train loss: 5698.7902832\n",
      "[12,   261] train loss: 5833.8150228\n",
      "[12,   266] train loss: 5580.7193197\n",
      "[12,   271] train loss: 5592.8852539\n",
      "[12,   276] train loss: 5825.9076335\n",
      "[12,   281] train loss: 5737.9182129\n",
      "[12,   286] train loss: 5865.4040527\n",
      "[12,   291] train loss: 5929.3168132\n",
      "[12,   296] train loss: 5604.3723145\n",
      "[12,   301] train loss: 5730.5706380\n",
      "[12,   306] train loss: 5591.6492513\n",
      "[12,   311] train loss: 5617.2386882\n",
      "[12,   316] train loss: 5789.8942057\n",
      "[12,   321] train loss: 5805.0175781\n",
      "[12,   326] train loss: 5951.0760091\n",
      "[12,   331] train loss: 5958.4456380\n",
      "[12,   336] train loss: 5783.5738118\n",
      "[12,   341] train loss: 5586.4510091\n",
      "[12,   346] train loss: 5680.4194336\n",
      "[12,   351] train loss: 5850.8372396\n",
      "[12,   356] train loss: 5878.1079915\n",
      "[12,   361] train loss: 5470.8311361\n",
      "[12,   366] train loss: 5504.9724935\n",
      "[12,   371] train loss: 5810.3083496\n",
      "[12,   376] train loss: 5700.2653809\n",
      "[12,   381] train loss: 5955.3194173\n",
      "[12,   386] train loss: 5631.6177572\n",
      "[12,   391] train loss: 5856.9820964\n",
      "[12,   396] train loss: 5515.7644043\n",
      "[12,   401] train loss: 5924.8590495\n",
      "[12,   406] train loss: 5782.9133301\n",
      "[12,   411] train loss: 5929.4671224\n",
      "[12,   416] train loss: 5914.0666504\n",
      "[12,   421] train loss: 5517.3781738\n",
      "[12,   426] train loss: 5838.7587891\n",
      "[12,   431] train loss: 5772.0721842\n",
      "[12,   436] train loss: 5937.9209798\n",
      "[12,   441] train loss: 5792.1274414\n",
      "[12,   446] train loss: 5562.0412598\n",
      "[12,   451] train loss: 6002.2652995\n",
      "[12,   456] train loss: 5825.0223796\n",
      "[12,   461] train loss: 5655.1866048\n",
      "[12,   466] train loss: 5881.9578451\n",
      "[12,   471] train loss: 5914.1503092\n",
      "[12,   476] train loss: 5757.6031901\n",
      "[12,   481] train loss: 5679.8931478\n",
      "[12,   486] train loss: 5811.6780599\n",
      "[12,   491] train loss: 5814.5008138\n",
      "[12,   496] train loss: 5620.4899902\n",
      "[12,   501] train loss: 5712.5499674\n",
      "[12,   506] train loss: 5787.8836263\n",
      "[12,   511] train loss: 5966.3470052\n",
      "[12,   516] train loss: 5778.3951009\n",
      "[12,   521] train loss: 5767.1729329\n",
      "[12,   526] train loss: 5920.8257650\n",
      "[12,   531] train loss: 5758.2208659\n",
      "[12,   536] train loss: 6070.2895508\n",
      "[12,   541] train loss: 5534.0095215\n",
      "[12,   546] train loss: 5837.1630046\n",
      "[12,   551] train loss: 5658.7838542\n",
      "[12,   556] train loss: 5982.7579753\n",
      "[12,   561] train loss: 5728.3771159\n",
      "[12,   566] train loss: 5624.5480957\n",
      "[12,   571] train loss: 5748.7983398\n",
      "[12,   576] train loss: 5840.5182292\n",
      "[12,   581] train loss: 5650.7305501\n",
      "[12,   586] train loss: 5975.0375977\n",
      "[12,   591] train loss: 5774.9990234\n",
      "[12,   596] train loss: 5668.4836426\n",
      "[12,   601] train loss: 5973.0664062\n",
      "[12,   606] train loss: 5653.9514974\n",
      "[12,   611] train loss: 5722.1323242\n",
      "[12,   616] train loss: 5792.8240560\n",
      "[12,   621] train loss: 5756.3624674\n",
      "[12,   625] train loss: 5463.0326172\n",
      "#12 reconstruction test loss: 107.81988525390625\n",
      "#12 reconstruction validation loss: 108.73826599121094\n",
      "Epoch #12\n",
      "[13,     1] train loss: 6972.0991211\n",
      "[13,     6] train loss: 5679.6463216\n",
      "[13,    11] train loss: 5730.2044271\n",
      "[13,    16] train loss: 5612.9556478\n",
      "[13,    21] train loss: 5820.2591960\n",
      "[13,    26] train loss: 6025.6230469\n",
      "[13,    31] train loss: 5658.7211914\n",
      "[13,    36] train loss: 5730.8031413\n",
      "[13,    41] train loss: 5646.0039062\n",
      "[13,    46] train loss: 5618.1699219\n",
      "[13,    51] train loss: 5669.2639160\n",
      "[13,    56] train loss: 5922.4042155\n",
      "[13,    61] train loss: 6036.4182943\n",
      "[13,    66] train loss: 5655.8779297\n",
      "[13,    71] train loss: 5714.0644531\n",
      "[13,    76] train loss: 5650.4583333\n",
      "[13,    81] train loss: 5847.7536621\n",
      "[13,    86] train loss: 5845.8222656\n",
      "[13,    91] train loss: 5552.8343099\n",
      "[13,    96] train loss: 5541.7563477\n",
      "[13,   101] train loss: 5775.4188639\n",
      "[13,   106] train loss: 5758.0945638\n",
      "[13,   111] train loss: 5983.3052572\n",
      "[13,   116] train loss: 5664.9408366\n",
      "[13,   121] train loss: 5737.4763997\n",
      "[13,   126] train loss: 5828.7319336\n",
      "[13,   131] train loss: 5667.7514648\n",
      "[13,   136] train loss: 5992.5894368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-25777a6938b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbass_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnb_ffnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_drum_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# bass_outputs = bass_outputs.squeeze()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b0f0d179b355>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# генерируем случайную точку в латентном пространстве\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconditionings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelody_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b0f0d179b355>\u001b[0m in \u001b[0;36mdecoder\u001b[1;34m(self, x, cond)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Linear function 6 (readout)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 72\n",
    "batch_size = 64\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    \n",
    "def calc_loss_on(drum, bass, description):\n",
    "    with torch.no_grad():\n",
    "        batch_bass_raw = torch.tensor(list(map(lambda p: p.image, bass)), dtype=torch.float)\n",
    "        bass_outputs, _, _ = dnb_ffnn(drum)\n",
    "\n",
    "        count = len(drum)\n",
    "        loss = 0\n",
    "        for k in range(count):\n",
    "            loss += reconstruction_loss(bass_outputs[k], batch_bass_raw[k])\n",
    "        print(f\"#{epoch + 1} {description}: {loss/count}\")\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        current_batch_size = len(batch_drum_train)\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_ffnn(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(current_batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "    calc_loss_on(drum_test, bass_test, \"reconstruction test loss\")\n",
    "    calc_loss_on(drum_validation, bass_validation, \"reconstruction validation loss\")\n",
    "    torch.save(dnb_ffnn.state_dict(), f\"models/vae_fcnn_fcnn_{epoch+1}iter\")\n",
    "    \n",
    "\n",
    "#should check accuracy on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение текущего состояния нейросети\n",
    "Сохраим веса модели во внешний файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dnb_ffnn.state_dict(), \"models/vae_fcnn_fcnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично происходит загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_ffnn.load_state_dict(torch.load(\"models/vae_fcnn_fcnn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_ffnn(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images(drum_train), dnb_ffnn.get_conditionings(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "mu\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]*100\n",
    "y = mu[:,1]*100\n",
    "z = dev\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "def output_midi(batch_drum, batch_bass, folder):\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_ffnn(batch_drum)[0]\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "\n",
    "        for i in range(len(batch_drum)):\n",
    "\n",
    "            img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "            numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                    , batch_drum[i].tempo\n",
    "                                    , batch_drum[i].instrument\n",
    "                                    , 1\n",
    "                                    , batch_drum[i].min_note)\n",
    "            pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "            mid = build_track(pair, tempo=pair.tempo)\n",
    "            mid.save(f\"{folder}/sample{i+1}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим обучающую и валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# если очень надо послушать тренировчную -- лучше её перезагрузить, потому что она перемешивается\n",
    "# drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "#                                                             patterns_file=train_file,\n",
    "#                                                             mono=False)\n",
    "# output_midi(drum, bass, \"midi/vae_fcnn_fcnn/train\")\n",
    "output_midi(drum_validation, bass_validation, \"midi/vae_fcnn_fcnn/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вкусу, выводим тот же результат для кожанных мешков на ассесмент. На самом деле ничем от валидационной выборки не отличается :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hum, bass_hum = data_conversion.make_lstm_dataset_conditioning(height=data_height, patterns_file=human_file, mono=False)\n",
    "output_midi(drum_hum, bass_hum, \"midi/vae_fcnn_fcnn/human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample1_id = 89\n",
    "# sample2_id = 49\n",
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 17\n",
    "sample2_id = 73\n",
    "sample1 = drum_hum[sample1_id]\n",
    "sample2 = drum_hum[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    sampls = [sample1, sample2]\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images(sampls), dnb_ffnn.get_conditionings(sampls))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        # instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        instrument = drum_sample.instrument\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_ffnn.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , int(instrument)\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/vae_fcnn_fcnn/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом\n",
    "\n",
    "В этой версии используется VAE. Энкодер -- двухслойная полносвязная нейронная сеть, декодер -- зеркальная двухслойная полносвязная нейронная сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "train_file = \"decode_patterns/train.tsv\" # обучающая выборка\n",
    "validation_file = \"decode_patterns/validation.tsv\" # валидационная выборка\n",
    "human_file = \"decode_patterns/human.tsv\" # как валидационная, только для ассесмента людей (read as \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            patterns_file=train_file,\n",
    "                                                            mono=False)\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can shuffle train and test set like this:\n",
    "drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "\n",
    "# selecting a validation set\n",
    "drum_validation, bass_validation = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                                                  patterns_file=validation_file,\n",
    "                                                                                  mono=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=96, instrument=27, denominator=4, min_note=45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_FFNN_to_FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_height, drum_width, melody_width):\n",
    "        super(DrumNBass_FFNN_to_FFNN, self).__init__()\n",
    "        \n",
    "        self.data_height = data_height\n",
    "        self.drum_width = drum_width\n",
    "        self.melody_width = melody_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        \n",
    "        input_dim = data_height*drum_width\n",
    "        latent_dim = 4\n",
    "        hidden_dim = (input_dim + 2*latent_dim) // 3\n",
    "        output_dim = data_height*melody_width\n",
    "        \n",
    "        # Linear function 1: 128 * 14 = 1792 --> 2048\n",
    "        # веса накидываются тут\n",
    "        self.fc1 = nn.Linear(input_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc1.weight, mean=1.5, std=1.0)\n",
    "        # решение по весам\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc2.weight, mean=1.5, std=1.0)\n",
    "        self.relu2 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Linear function 31: 2048 --> 4\n",
    "        # для средних значений\n",
    "        self.fc31 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 31\n",
    "        self.relu31 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        # Linear function 22: 2048 --> 4\n",
    "        # для стандартных отклонений\n",
    "        self.fc32 = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Non-linearity 22\n",
    "        self.relu32 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 4: 4 --> 2048\n",
    "        self.fc4 = nn.Linear(latent_dim + self.condition_size, hidden_dim)\n",
    "#         nn.init.normal_(self.fc4.weight, mean=1.5, std=1.0)\n",
    "        # Non-linearity 4\n",
    "        self.relu4 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         nn.init.normal_(self.fc5.weight, mean=1.5, std=1.0)\n",
    "        self.relu5 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function 6 (readout): 2048 --> 128 * 36 = 4608\n",
    "        self.fc6 = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 1\n",
    "        out = self.fc1(out)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 31\n",
    "        mu = self.fc31(out)\n",
    "        # Non-linearity 31\n",
    "        mu = self.relu31(mu)\n",
    "        \n",
    "        \n",
    "        # Linear function 22\n",
    "        logvar = self.fc32(out)\n",
    "        # Non-linearity 22\n",
    "        logvar = self.relu32(logvar)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, x, cond):\n",
    "        out = torch.cat((x, cond), axis=1) # добавляем conditioning\n",
    "        # Linear function 4\n",
    "        out = self.fc4(out)\n",
    "        # Non-linearity 4\n",
    "        out = self.relu4(out)\n",
    "        \n",
    "        # Linear function 5\n",
    "        out = self.fc5(out)\n",
    "        # Non-linearity 5\n",
    "        out = self.relu5(out)\n",
    "\n",
    "        # Linear function 6 (readout)\n",
    "        out = self.fc6(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image.flatten(), input)), dtype=torch.float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_conditionings(input):\n",
    "        return torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # добавляем conditioning\n",
    "        conditionings = self.get_conditionings(x)\n",
    "        images = self.get_images(x)\n",
    "        mean, logvar = self.encoder(images, conditionings)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.data_height, self.melody_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_ffnn = DrumNBass_FFNN_to_FFNN(data_height, drum_width, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_loss(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_ffnn.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_ffnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_ffnn.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 26007.4667969\n",
      "[1,     6] train loss: 15319.2210286\n",
      "[1,    11] train loss: 6760.9116211\n",
      "[1,    16] train loss: 2677.7970785\n",
      "[1,    21] train loss: 1886.4801432\n",
      "[1,    26] train loss: 1620.8393351\n",
      "[1,    31] train loss: 1517.1423136\n",
      "[1,    36] train loss: 1621.6326294\n",
      "[1,    41] train loss: 1696.4822795\n",
      "[1,    46] train loss: 1562.4137980\n",
      "[1,    51] train loss: 1462.6774292\n",
      "[1,    56] train loss: 1460.3236491\n",
      "[1,    61] train loss: 1466.0707601\n",
      "[1,    66] train loss: 1503.2029012\n",
      "[1,    71] train loss: 1560.3666789\n",
      "[1,    76] train loss: 1403.7815145\n",
      "[1,    81] train loss: 1458.6035360\n",
      "[1,    86] train loss: 1465.6116943\n",
      "[1,    91] train loss: 1357.8545736\n",
      "[1,    96] train loss: 1555.1640422\n",
      "[1,   101] train loss: 1560.8454793\n",
      "[1,   106] train loss: 1395.5764364\n",
      "[1,   111] train loss: 1432.3311361\n",
      "[1,   116] train loss: 1435.4966227\n",
      "[1,   121] train loss: 1577.8641968\n",
      "[1,   126] train loss: 1467.0599162\n",
      "[1,   131] train loss: 1433.8996785\n",
      "[1,   136] train loss: 1465.6388753\n",
      "[1,   141] train loss: 1541.7079671\n",
      "[1,   146] train loss: 1520.6807454\n",
      "[1,   151] train loss: 1482.0120850\n",
      "[1,   156] train loss: 1485.9205933\n",
      "[1,   161] train loss: 1513.0692749\n",
      "[1,   166] train loss: 1450.3816528\n",
      "[1,   171] train loss: 1377.3466390\n",
      "[1,   176] train loss: 1339.4062093\n",
      "[1,   181] train loss: 1508.0977987\n",
      "[1,   186] train loss: 1421.1392008\n",
      "[1,   191] train loss: 1538.6064657\n",
      "[1,   196] train loss: 1501.3518270\n",
      "[1,   201] train loss: 1572.8571574\n",
      "[1,   206] train loss: 1418.1914469\n",
      "[1,   211] train loss: 1463.3738607\n",
      "[1,   216] train loss: 1559.0161336\n",
      "[1,   221] train loss: 1505.9158122\n",
      "[1,   226] train loss: 1344.2008870\n",
      "[1,   231] train loss: 1343.5054118\n",
      "[1,   236] train loss: 1513.4401449\n",
      "[1,   241] train loss: 1580.9851888\n",
      "[1,   246] train loss: 1461.2390544\n",
      "[1,   251] train loss: 1480.3358358\n",
      "[1,   256] train loss: 1469.5285848\n",
      "[1,   261] train loss: 1612.6882935\n",
      "[1,   266] train loss: 1574.4476929\n",
      "[1,   271] train loss: 1530.2798665\n",
      "[1,   276] train loss: 1530.9260661\n",
      "[1,   281] train loss: 1548.7192586\n",
      "[1,   286] train loss: 1458.4073893\n",
      "[1,   291] train loss: 1424.7454020\n",
      "[1,   296] train loss: 1494.8084106\n",
      "[1,   301] train loss: 1524.8720500\n",
      "[1,   306] train loss: 1566.9934082\n",
      "[1,   311] train loss: 1452.2150065\n",
      "[1,   316] train loss: 1510.4518433\n",
      "[1,   321] train loss: 1507.0327759\n",
      "[1,   326] train loss: 1464.0602824\n",
      "[1,   331] train loss: 1614.3984375\n",
      "[1,   336] train loss: 1421.5084025\n",
      "[1,   341] train loss: 1629.4631551\n",
      "[1,   346] train loss: 1558.7541097\n",
      "[1,   351] train loss: 1376.9471232\n",
      "[1,   356] train loss: 1480.9194132\n",
      "[1,   361] train loss: 1523.9030965\n",
      "[1,   366] train loss: 1631.6731974\n",
      "[1,   371] train loss: 1427.5732015\n",
      "[1,   376] train loss: 1608.4960938\n",
      "[1,   381] train loss: 1439.0011597\n",
      "[1,   386] train loss: 1501.6375936\n",
      "[1,   391] train loss: 1428.4210002\n",
      "[1,   396] train loss: 1472.0702108\n",
      "[1,   401] train loss: 1462.0586344\n",
      "[1,   406] train loss: 1525.5209961\n",
      "[1,   411] train loss: 1559.1220093\n",
      "[1,   416] train loss: 1517.0477091\n",
      "[1,   421] train loss: 1591.1572673\n",
      "[1,   426] train loss: 1474.9763387\n",
      "[1,   431] train loss: 1451.9028524\n",
      "[1,   436] train loss: 1421.9370321\n",
      "[1,   441] train loss: 1585.2139079\n",
      "[1,   446] train loss: 1389.4761149\n",
      "[1,   451] train loss: 1442.1575317\n",
      "[1,   456] train loss: 1623.6407674\n",
      "[1,   461] train loss: 1590.3762004\n",
      "[1,   466] train loss: 1517.7755737\n",
      "[1,   471] train loss: 1561.1360677\n",
      "[1,   476] train loss: 1508.1191406\n",
      "[1,   481] train loss: 1540.0810954\n",
      "[1,   486] train loss: 1445.5653687\n",
      "[1,   491] train loss: 1475.2304281\n",
      "[1,   496] train loss: 1415.3103231\n",
      "[1,   500] train loss: 1379.1009766\n",
      "#1 reconstruction test loss: 112.27389526367188\n",
      "Epoch #1\n",
      "[2,     1] train loss: 1739.1442871\n",
      "[2,     6] train loss: 1396.8232218\n",
      "[2,    11] train loss: 1468.1342570\n",
      "[2,    16] train loss: 1420.6816203\n",
      "[2,    21] train loss: 1549.1110026\n",
      "[2,    26] train loss: 1432.3267619\n",
      "[2,    31] train loss: 1485.4995931\n",
      "[2,    36] train loss: 1510.4992065\n",
      "[2,    41] train loss: 1523.3799845\n",
      "[2,    46] train loss: 1462.3807373\n",
      "[2,    51] train loss: 1479.5745646\n",
      "[2,    56] train loss: 1495.0994873\n",
      "[2,    61] train loss: 1625.2710368\n",
      "[2,    66] train loss: 1411.3243205\n",
      "[2,    71] train loss: 1706.3742879\n",
      "[2,    76] train loss: 1402.3564657\n",
      "[2,    81] train loss: 1437.4058228\n",
      "[2,    86] train loss: 1501.0925700\n",
      "[2,    91] train loss: 1461.6872152\n",
      "[2,    96] train loss: 1576.8773600\n",
      "[2,   101] train loss: 1544.0145467\n",
      "[2,   106] train loss: 1423.1151326\n",
      "[2,   111] train loss: 1592.4431966\n",
      "[2,   116] train loss: 1437.2090658\n",
      "[2,   121] train loss: 1500.6511027\n",
      "[2,   126] train loss: 1452.3113200\n",
      "[2,   131] train loss: 1616.0991007\n",
      "[2,   136] train loss: 1476.4635824\n",
      "[2,   141] train loss: 1429.5203654\n",
      "[2,   146] train loss: 1517.2013143\n",
      "[2,   151] train loss: 1545.1221313\n",
      "[2,   156] train loss: 1646.5742188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-51757c2da78f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreconstruction_KL_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 35\n",
    "batch_size = 16\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_ffnn(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "    with torch.no_grad():\n",
    "        batch_bass_test_raw = torch.tensor(list(map(lambda p: p.image, bass_test)), dtype=torch.float)\n",
    "        bass_outputs, _, _ = dnb_ffnn(drum_test)\n",
    "        \n",
    "        test_count = len(drum_test)\n",
    "        test_loss = 0\n",
    "        for k in range(test_count):\n",
    "            test_loss += reconstruction_loss(bass_outputs[k], batch_bass_test_raw[k])\n",
    "        print(f\"#{epoch + 1} reconstruction test loss: {test_loss/test_count}\")\n",
    "    \n",
    "\n",
    "#should check accuracy on validation set\n",
    "with torch.no_grad():\n",
    "    batch_bass_validation_raw = torch.tensor(list(map(lambda p: p.image, bass_validation)), dtype=torch.float)\n",
    "    bass_outputs, _, _ = dnb_ffnn(drum_test)\n",
    "\n",
    "    validation_count = len(drum_test)\n",
    "    validation_loss = 0\n",
    "    for k in range(validation_count):\n",
    "        validation_loss += reconstruction_loss(bass_outputs[k], batch_bass_validation_raw[k])\n",
    "    print(f\"#{epoch + 1} reconstruction validation loss: {validation_loss/validation_count}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_ffnn(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images(drum_train), dnb_ffnn.get_conditionings(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "mu\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]*100\n",
    "y = mu[:,1]*100\n",
    "z = dev\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "def output_midi(batch_drum, batch_bass, folder):\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_ffnn(batch_drum)[0]\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "\n",
    "        for i in range(len(batch_drum)):\n",
    "\n",
    "            img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "            numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                    , batch_drum[i].tempo\n",
    "                                    , batch_drum[i].instrument\n",
    "                                    , 1\n",
    "                                    , batch_drum[i].min_note)\n",
    "            pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "            mid = build_track(pair, tempo=pair.tempo)\n",
    "            mid.save(f\"{folder}/sample{i+1}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим обучающую и валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # если очень надо послушать тренировчную -- лучше её перезагрузить, потому что она перемешивается\n",
    "# drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "#                                                             patterns_file=train_file,\n",
    "#                                                             mono=False)\n",
    "# output_midi(drum_train + drum_test, bass_train + bass_test, \"midi/vae_fcnn_fcnn/train\")\n",
    "output_midi(drum_validation, bass_validation, \"midi/vae_fcnn_fcnn/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вкусу, выводим тот же результат для кожанных мешков на ассесмент. На самом деле ничем от валидационной выборки не отличается :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hum, bass_hum = data_conversion.make_lstm_dataset_conditioning(height=data_height, patterns_file=human_file, mono=False)\n",
    "output_midi(drum_hum, bass_hum, \"midi/vae_fcnn_fcnn/human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample1_id = 89\n",
    "# sample2_id = 49\n",
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 55\n",
    "sample2_id = 119\n",
    "sample1 = drum_validation[sample1_id]\n",
    "sample2 = drum_validation[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    sampls = [sample1, sample2]\n",
    "    latent_train = dnb_ffnn.encoder(dnb_ffnn.get_images(sampls), dnb_ffnn.get_conditionings(sampls))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        # instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        instrument = drum_sample.instrument\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_ffnn.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , int(instrument)\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/vae_fcnn_fcnn/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

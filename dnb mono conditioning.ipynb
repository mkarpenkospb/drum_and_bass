{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация монофонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=64,\n",
    "                                                            limit=1000,\n",
    "                                                            patterns_file=\"decode_patterns/patterns.pairs.tsv\",\n",
    "                                                            mono=True)\n",
    "# print(drum[0])\n",
    "# drum, bass = np.array(drum), np.array(bass)\n",
    "# print(drum[0])\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(A)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(A)\n",
    "    random.shuffle(B)\n",
    "    yield A[:pivot]\n",
    "    yield B[:pivot]\n",
    "    yield A[pivot:]\n",
    "    yield B[pivot:]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32), tempo=192, instrument=25, denominator=1, min_note=47)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drum_validation[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 26\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_embed_layer = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_embed_layer(output))*37\n",
    "        return output\n",
    "    \n",
    "    def decoder(self, input):\n",
    "        return input\n",
    "    \n",
    "    def forward(self, input):\n",
    "        result = self.encoder(torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float))\n",
    "        # добавляем conditioning\n",
    "        # result.append([input.tempo, input.instrument])\n",
    "        result = self.decoder(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 28.7314854\n",
      "[1,     3] train loss: 20.7457123\n",
      "Epoch #1\n",
      "[2,     1] train loss: 29.0978489\n",
      "[2,     3] train loss: 20.2607212\n",
      "Epoch #2\n",
      "[3,     1] train loss: 30.8472576\n",
      "[3,     3] train loss: 19.1461252\n",
      "Epoch #3\n",
      "[4,     1] train loss: 30.3425446\n",
      "[4,     3] train loss: 20.7995338\n",
      "Epoch #4\n",
      "[5,     1] train loss: 29.9841366\n",
      "[5,     3] train loss: 19.8327236\n",
      "Epoch #5\n",
      "[6,     1] train loss: 27.6150818\n",
      "[6,     3] train loss: 19.2308470\n",
      "Epoch #6\n",
      "[7,     1] train loss: 31.6969414\n",
      "[7,     3] train loss: 19.3956477\n",
      "Epoch #7\n",
      "[8,     1] train loss: 30.5470753\n",
      "[8,     3] train loss: 19.8454577\n",
      "Epoch #8\n",
      "[9,     1] train loss: 31.8124866\n",
      "[9,     3] train loss: 18.3017495\n",
      "Epoch #9\n",
      "[10,     1] train loss: 29.9887238\n",
      "[10,     3] train loss: 19.2276859\n",
      "Epoch #10\n",
      "[11,     1] train loss: 31.9611263\n",
      "[11,     3] train loss: 20.0223440\n",
      "Epoch #11\n",
      "[12,     1] train loss: 30.6956787\n",
      "[12,     3] train loss: 19.3831418\n",
      "Epoch #12\n",
      "[13,     1] train loss: 31.4934177\n",
      "[13,     3] train loss: 20.0582829\n",
      "Epoch #13\n",
      "[14,     1] train loss: 29.3637276\n",
      "[14,     3] train loss: 18.0276909\n",
      "Epoch #14\n",
      "[15,     1] train loss: 33.0101624\n",
      "[15,     3] train loss: 19.6124535\n",
      "Epoch #15\n",
      "[16,     1] train loss: 29.3513470\n",
      "[16,     3] train loss: 19.9157098\n",
      "Epoch #16\n",
      "[17,     1] train loss: 28.7168903\n",
      "[17,     3] train loss: 18.1657874\n",
      "Epoch #17\n",
      "[18,     1] train loss: 26.2570629\n",
      "[18,     3] train loss: 19.8141047\n",
      "Epoch #18\n",
      "[19,     1] train loss: 28.0217457\n",
      "[19,     3] train loss: 20.2381992\n",
      "Epoch #19\n",
      "[20,     1] train loss: 28.0684776\n",
      "[20,     3] train loss: 20.0009365\n",
      "Epoch #20\n",
      "[21,     1] train loss: 28.3337116\n",
      "[21,     3] train loss: 20.7107881\n",
      "Epoch #21\n",
      "[22,     1] train loss: 32.3128586\n",
      "[22,     3] train loss: 18.8891163\n",
      "Epoch #22\n",
      "[23,     1] train loss: 28.8754578\n",
      "[23,     3] train loss: 19.2472483\n",
      "Epoch #23\n",
      "[24,     1] train loss: 28.5551090\n",
      "[24,     3] train loss: 18.7527409\n",
      "Epoch #24\n",
      "[25,     1] train loss: 26.2409344\n",
      "[25,     3] train loss: 21.0938009\n",
      "Epoch #25\n",
      "[26,     1] train loss: 27.1044159\n",
      "[26,     3] train loss: 19.1894188\n",
      "Epoch #26\n",
      "[27,     1] train loss: 26.2045078\n",
      "[27,     3] train loss: 20.4204439\n",
      "Epoch #27\n",
      "[28,     1] train loss: 29.1177940\n",
      "[28,     3] train loss: 19.4798056\n",
      "Epoch #28\n",
      "[29,     1] train loss: 30.7487221\n",
      "[29,     3] train loss: 19.2741528\n",
      "Epoch #29\n",
      "[30,     1] train loss: 30.3689728\n",
      "[30,     3] train loss: 19.2733720\n",
      "Epoch #30\n",
      "[31,     1] train loss: 27.9048271\n",
      "[31,     3] train loss: 20.5277004\n",
      "Epoch #31\n",
      "[32,     1] train loss: 27.5832367\n",
      "[32,     3] train loss: 20.8143787\n",
      "Epoch #32\n",
      "[33,     1] train loss: 28.8546963\n",
      "[33,     3] train loss: 19.9638201\n",
      "Epoch #33\n",
      "[34,     1] train loss: 30.2371616\n",
      "[34,     3] train loss: 19.1330407\n",
      "Epoch #34\n",
      "[35,     1] train loss: 26.5115280\n",
      "[35,     3] train loss: 19.2761866\n",
      "Epoch #35\n",
      "[36,     1] train loss: 28.0706520\n",
      "[36,     3] train loss: 19.2332637\n",
      "Epoch #36\n",
      "[37,     1] train loss: 26.3990211\n",
      "[37,     3] train loss: 19.4819285\n",
      "Epoch #37\n",
      "[38,     1] train loss: 30.0358829\n",
      "[38,     3] train loss: 19.4548912\n",
      "Epoch #38\n",
      "[39,     1] train loss: 24.0953407\n",
      "[39,     3] train loss: 20.1798903\n",
      "Epoch #39\n",
      "[40,     1] train loss: 33.0929947\n",
      "[40,     3] train loss: 17.1699588\n",
      "Epoch #40\n",
      "[41,     1] train loss: 30.2881737\n",
      "[41,     3] train loss: 19.1162516\n",
      "Epoch #41\n",
      "[42,     1] train loss: 29.7152214\n",
      "[42,     3] train loss: 17.9235007\n",
      "Epoch #42\n",
      "[43,     1] train loss: 28.5470600\n",
      "[43,     3] train loss: 19.7987270\n",
      "Epoch #43\n",
      "[44,     1] train loss: 27.6221199\n",
      "[44,     3] train loss: 19.9622637\n",
      "Epoch #44\n",
      "[45,     1] train loss: 26.6764297\n",
      "[45,     3] train loss: 20.4053694\n",
      "Epoch #45\n",
      "[46,     1] train loss: 26.2199764\n",
      "[46,     3] train loss: 19.2467893\n",
      "Epoch #46\n",
      "[47,     1] train loss: 29.9821301\n",
      "[47,     3] train loss: 18.6956647\n",
      "Epoch #47\n",
      "[48,     1] train loss: 29.0709839\n",
      "[48,     3] train loss: 19.0365677\n",
      "Epoch #48\n",
      "[49,     1] train loss: 26.8803616\n",
      "[49,     3] train loss: 18.6859417\n",
      "Epoch #49\n",
      "[50,     1] train loss: 29.6347580\n",
      "[50,     3] train loss: 19.0104771\n",
      "Epoch #50\n",
      "[51,     1] train loss: 29.2291946\n",
      "[51,     3] train loss: 19.9186910\n",
      "Epoch #51\n",
      "[52,     1] train loss: 28.3025646\n",
      "[52,     3] train loss: 19.2865410\n",
      "Epoch #52\n",
      "[53,     1] train loss: 31.8456688\n",
      "[53,     3] train loss: 17.5343221\n",
      "Epoch #53\n",
      "[54,     1] train loss: 28.4833469\n",
      "[54,     3] train loss: 18.6031551\n",
      "Epoch #54\n",
      "[55,     1] train loss: 30.1098309\n",
      "[55,     3] train loss: 18.5966835\n",
      "Epoch #55\n",
      "[56,     1] train loss: 28.4266434\n",
      "[56,     3] train loss: 20.5843709\n",
      "Epoch #56\n",
      "[57,     1] train loss: 27.2658710\n",
      "[57,     3] train loss: 20.3008410\n",
      "Epoch #57\n",
      "[58,     1] train loss: 30.4892654\n",
      "[58,     3] train loss: 19.8590393\n",
      "Epoch #58\n",
      "[59,     1] train loss: 28.2000198\n",
      "[59,     3] train loss: 19.8731372\n",
      "Epoch #59\n",
      "[60,     1] train loss: 31.5628738\n",
      "[60,     3] train loss: 18.4092840\n",
      "Epoch #60\n",
      "[61,     1] train loss: 30.8565159\n",
      "[61,     3] train loss: 18.6195196\n",
      "Epoch #61\n",
      "[62,     1] train loss: 27.7566452\n",
      "[62,     3] train loss: 20.6293278\n",
      "Epoch #62\n",
      "[63,     1] train loss: 29.0454693\n",
      "[63,     3] train loss: 19.0108007\n",
      "Epoch #63\n",
      "[64,     1] train loss: 30.9902058\n",
      "[64,     3] train loss: 19.7716344\n",
      "Epoch #64\n",
      "[65,     1] train loss: 30.2272224\n",
      "[65,     3] train loss: 19.9665502\n",
      "Epoch #65\n",
      "[66,     1] train loss: 28.2912521\n",
      "[66,     3] train loss: 19.9039675\n",
      "Epoch #66\n",
      "[67,     1] train loss: 31.2305222\n",
      "[67,     3] train loss: 19.2068253\n",
      "Epoch #67\n",
      "[68,     1] train loss: 28.7236691\n",
      "[68,     3] train loss: 19.4955381\n",
      "Epoch #68\n",
      "[69,     1] train loss: 27.5092087\n",
      "[69,     3] train loss: 19.7462680\n",
      "Epoch #69\n",
      "[70,     1] train loss: 30.8957634\n",
      "[70,     3] train loss: 18.7090295\n",
      "Epoch #70\n",
      "[71,     1] train loss: 28.4880867\n",
      "[71,     3] train loss: 19.6698647\n",
      "Epoch #71\n",
      "[72,     1] train loss: 26.3873501\n",
      "[72,     3] train loss: 18.5379791\n",
      "Epoch #72\n",
      "[73,     1] train loss: 30.0861607\n",
      "[73,     3] train loss: 18.3127823\n",
      "Epoch #73\n",
      "[74,     1] train loss: 28.1029167\n",
      "[74,     3] train loss: 19.3865585\n",
      "Epoch #74\n",
      "[75,     1] train loss: 23.9316254\n",
      "[75,     3] train loss: 21.8538933\n",
      "Epoch #75\n",
      "[76,     1] train loss: 28.3389854\n",
      "[76,     3] train loss: 18.5799503\n",
      "Epoch #76\n",
      "[77,     1] train loss: 25.7132015\n",
      "[77,     3] train loss: 20.1005942\n",
      "Epoch #77\n",
      "[78,     1] train loss: 28.7061501\n",
      "[78,     3] train loss: 19.5333004\n",
      "Epoch #78\n",
      "[79,     1] train loss: 30.7191620\n",
      "[79,     3] train loss: 17.8737691\n",
      "Epoch #79\n",
      "[80,     1] train loss: 27.8715515\n",
      "[80,     3] train loss: 19.5157445\n",
      "Epoch #80\n",
      "[81,     1] train loss: 28.7422695\n",
      "[81,     3] train loss: 19.1299890\n",
      "Epoch #81\n",
      "[82,     1] train loss: 28.0989761\n",
      "[82,     3] train loss: 19.4462357\n",
      "Epoch #82\n",
      "[83,     1] train loss: 33.0734863\n",
      "[83,     3] train loss: 18.4225775\n",
      "Epoch #83\n",
      "[84,     1] train loss: 27.2457123\n",
      "[84,     3] train loss: 20.0254542\n",
      "Epoch #84\n",
      "[85,     1] train loss: 30.5018959\n",
      "[85,     3] train loss: 18.5097396\n",
      "Epoch #85\n",
      "[86,     1] train loss: 23.9172707\n",
      "[86,     3] train loss: 20.8686377\n",
      "Epoch #86\n",
      "[87,     1] train loss: 26.6602840\n",
      "[87,     3] train loss: 20.7603423\n",
      "Epoch #87\n",
      "[88,     1] train loss: 28.9513988\n",
      "[88,     3] train loss: 19.9224250\n",
      "Epoch #88\n",
      "[89,     1] train loss: 27.8461857\n",
      "[89,     3] train loss: 21.1725229\n",
      "Epoch #89\n",
      "[90,     1] train loss: 28.1421394\n",
      "[90,     3] train loss: 17.7051970\n",
      "Epoch #90\n",
      "[91,     1] train loss: 26.5495319\n",
      "[91,     3] train loss: 18.2626654\n",
      "Epoch #91\n",
      "[92,     1] train loss: 28.2807140\n",
      "[92,     3] train loss: 19.9727809\n",
      "Epoch #92\n",
      "[93,     1] train loss: 28.2019482\n",
      "[93,     3] train loss: 17.8247681\n",
      "Epoch #93\n",
      "[94,     1] train loss: 31.1933308\n",
      "[94,     3] train loss: 19.1721776\n",
      "Epoch #94\n",
      "[95,     1] train loss: 26.8028374\n",
      "[95,     3] train loss: 18.9275551\n",
      "Epoch #95\n",
      "[96,     1] train loss: 25.9645500\n",
      "[96,     3] train loss: 18.8333556\n",
      "Epoch #96\n",
      "[97,     1] train loss: 28.4912300\n",
      "[97,     3] train loss: 19.0355396\n",
      "Epoch #97\n",
      "[98,     1] train loss: 28.9227715\n",
      "[98,     3] train loss: 19.6415825\n",
      "Epoch #98\n",
      "[99,     1] train loss: 28.0440693\n",
      "[99,     3] train loss: 20.2670555\n",
      "Epoch #99\n",
      "[100,     1] train loss: 29.8081150\n",
      "[100,     3] train loss: 18.6954460\n",
      "Epoch #100\n",
      "[101,     1] train loss: 30.6242599\n",
      "[101,     3] train loss: 18.9144154\n",
      "Epoch #101\n",
      "[102,     1] train loss: 27.9517994\n",
      "[102,     3] train loss: 18.6803023\n",
      "Epoch #102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,     1] train loss: 28.1820908\n",
      "[103,     3] train loss: 20.0367050\n",
      "Epoch #103\n",
      "[104,     1] train loss: 29.7157860\n",
      "[104,     3] train loss: 18.0000725\n",
      "Epoch #104\n",
      "[105,     1] train loss: 27.6471233\n",
      "[105,     3] train loss: 18.6831741\n",
      "Epoch #105\n",
      "[106,     1] train loss: 27.5312004\n",
      "[106,     3] train loss: 20.0077883\n",
      "Epoch #106\n",
      "[107,     1] train loss: 30.0931702\n",
      "[107,     3] train loss: 18.5425822\n",
      "Epoch #107\n",
      "[108,     1] train loss: 27.8343620\n",
      "[108,     3] train loss: 19.1943773\n",
      "Epoch #108\n",
      "[109,     1] train loss: 27.1991177\n",
      "[109,     3] train loss: 18.3654277\n",
      "Epoch #109\n",
      "[110,     1] train loss: 26.5292759\n",
      "[110,     3] train loss: 19.8883368\n",
      "Epoch #110\n",
      "[111,     1] train loss: 26.6493549\n",
      "[111,     3] train loss: 20.4923522\n",
      "Epoch #111\n",
      "[112,     1] train loss: 25.3714256\n",
      "[112,     3] train loss: 20.3374659\n",
      "Epoch #112\n",
      "[113,     1] train loss: 26.7329941\n",
      "[113,     3] train loss: 19.8648879\n",
      "Epoch #113\n",
      "[114,     1] train loss: 27.3965340\n",
      "[114,     3] train loss: 20.2863102\n",
      "Epoch #114\n",
      "[115,     1] train loss: 29.4847088\n",
      "[115,     3] train loss: 18.3064473\n",
      "Epoch #115\n",
      "[116,     1] train loss: 29.1981010\n",
      "[116,     3] train loss: 18.6156222\n",
      "Epoch #116\n",
      "[117,     1] train loss: 30.1132565\n",
      "[117,     3] train loss: 19.7620729\n",
      "Epoch #117\n",
      "[118,     1] train loss: 30.4977646\n",
      "[118,     3] train loss: 18.1620706\n",
      "Epoch #118\n",
      "[119,     1] train loss: 29.1021080\n",
      "[119,     3] train loss: 19.3841890\n",
      "Epoch #119\n",
      "[120,     1] train loss: 30.1654072\n",
      "[120,     3] train loss: 19.2142169\n",
      "Epoch #120\n",
      "[121,     1] train loss: 27.4908657\n",
      "[121,     3] train loss: 19.0141691\n",
      "Epoch #121\n",
      "[122,     1] train loss: 29.7616558\n",
      "[122,     3] train loss: 18.0069580\n",
      "Epoch #122\n",
      "[123,     1] train loss: 25.8365803\n",
      "[123,     3] train loss: 20.8947951\n",
      "Epoch #123\n",
      "[124,     1] train loss: 32.9024582\n",
      "[124,     3] train loss: 19.0066980\n",
      "Epoch #124\n",
      "[125,     1] train loss: 28.2489376\n",
      "[125,     3] train loss: 18.8608068\n",
      "Epoch #125\n",
      "[126,     1] train loss: 29.4772148\n",
      "[126,     3] train loss: 17.6399269\n",
      "Epoch #126\n",
      "[127,     1] train loss: 27.9478512\n",
      "[127,     3] train loss: 18.6712678\n",
      "Epoch #127\n",
      "[128,     1] train loss: 31.6037369\n",
      "[128,     3] train loss: 16.9207694\n",
      "Epoch #128\n",
      "[129,     1] train loss: 31.8426704\n",
      "[129,     3] train loss: 17.6318747\n",
      "Epoch #129\n",
      "[130,     1] train loss: 28.4568195\n",
      "[130,     3] train loss: 18.1134396\n",
      "Epoch #130\n",
      "[131,     1] train loss: 25.3558750\n",
      "[131,     3] train loss: 19.1648375\n",
      "Epoch #131\n",
      "[132,     1] train loss: 26.0756931\n",
      "[132,     3] train loss: 20.1733615\n",
      "Epoch #132\n",
      "[133,     1] train loss: 33.7746239\n",
      "[133,     3] train loss: 17.5962849\n",
      "Epoch #133\n",
      "[134,     1] train loss: 27.8966751\n",
      "[134,     3] train loss: 19.0854397\n",
      "Epoch #134\n",
      "[135,     1] train loss: 28.2347355\n",
      "[135,     3] train loss: 18.2602749\n",
      "Epoch #135\n",
      "[136,     1] train loss: 31.5865574\n",
      "[136,     3] train loss: 16.0809396\n",
      "Epoch #136\n",
      "[137,     1] train loss: 28.0891781\n",
      "[137,     3] train loss: 18.9280001\n",
      "Epoch #137\n",
      "[138,     1] train loss: 28.6532726\n",
      "[138,     3] train loss: 19.7117748\n",
      "Epoch #138\n",
      "[139,     1] train loss: 27.2339725\n",
      "[139,     3] train loss: 20.0866604\n",
      "Epoch #139\n",
      "[140,     1] train loss: 30.7643566\n",
      "[140,     3] train loss: 17.3323250\n",
      "Epoch #140\n",
      "[141,     1] train loss: 26.6927853\n",
      "[141,     3] train loss: 19.3107745\n",
      "Epoch #141\n",
      "[142,     1] train loss: 28.3770962\n",
      "[142,     3] train loss: 19.8929920\n",
      "Epoch #142\n",
      "[143,     1] train loss: 28.9222050\n",
      "[143,     3] train loss: 19.1684647\n",
      "Epoch #143\n",
      "[144,     1] train loss: 28.1917839\n",
      "[144,     3] train loss: 19.1197198\n",
      "Epoch #144\n",
      "[145,     1] train loss: 26.4084911\n",
      "[145,     3] train loss: 18.4501311\n",
      "Epoch #145\n",
      "[146,     1] train loss: 28.4991264\n",
      "[146,     3] train loss: 18.8974107\n",
      "Epoch #146\n",
      "[147,     1] train loss: 34.2384186\n",
      "[147,     3] train loss: 17.5748844\n",
      "Epoch #147\n",
      "[148,     1] train loss: 26.6807194\n",
      "[148,     3] train loss: 18.7971141\n",
      "Epoch #148\n",
      "[149,     1] train loss: 29.4229164\n",
      "[149,     3] train loss: 17.9394544\n",
      "Epoch #149\n",
      "[150,     1] train loss: 32.7482262\n",
      "[150,     3] train loss: 17.7515221\n",
      "Epoch #150\n",
      "[151,     1] train loss: 29.1263351\n",
      "[151,     3] train loss: 19.4090131\n",
      "Epoch #151\n",
      "[152,     1] train loss: 28.1249218\n",
      "[152,     3] train loss: 18.7639554\n",
      "Epoch #152\n",
      "[153,     1] train loss: 29.3552628\n",
      "[153,     3] train loss: 18.4152768\n",
      "Epoch #153\n",
      "[154,     1] train loss: 30.1623039\n",
      "[154,     3] train loss: 17.2950071\n",
      "Epoch #154\n",
      "[155,     1] train loss: 29.7590389\n",
      "[155,     3] train loss: 19.2222691\n",
      "Epoch #155\n",
      "[156,     1] train loss: 31.0393085\n",
      "[156,     3] train loss: 18.1985512\n",
      "Epoch #156\n",
      "[157,     1] train loss: 29.0408974\n",
      "[157,     3] train loss: 18.7548135\n",
      "Epoch #157\n",
      "[158,     1] train loss: 29.4778881\n",
      "[158,     3] train loss: 17.8379294\n",
      "Epoch #158\n",
      "[159,     1] train loss: 25.3797474\n",
      "[159,     3] train loss: 20.6451956\n",
      "Epoch #159\n",
      "[160,     1] train loss: 28.8955250\n",
      "[160,     3] train loss: 18.0039368\n",
      "Epoch #160\n",
      "[161,     1] train loss: 28.6492767\n",
      "[161,     3] train loss: 20.6015104\n",
      "Epoch #161\n",
      "[162,     1] train loss: 26.8063812\n",
      "[162,     3] train loss: 19.6159808\n",
      "Epoch #162\n",
      "[163,     1] train loss: 28.8009796\n",
      "[163,     3] train loss: 19.2694918\n",
      "Epoch #163\n",
      "[164,     1] train loss: 27.6085110\n",
      "[164,     3] train loss: 18.7219639\n",
      "Epoch #164\n",
      "[165,     1] train loss: 29.0657101\n",
      "[165,     3] train loss: 18.5193615\n",
      "Epoch #165\n",
      "[166,     1] train loss: 28.5575542\n",
      "[166,     3] train loss: 17.9007066\n",
      "Epoch #166\n",
      "[167,     1] train loss: 26.5936966\n",
      "[167,     3] train loss: 20.1076419\n",
      "Epoch #167\n",
      "[168,     1] train loss: 29.7835674\n",
      "[168,     3] train loss: 18.6134872\n",
      "Epoch #168\n",
      "[169,     1] train loss: 30.2804222\n",
      "[169,     3] train loss: 18.5107492\n",
      "Epoch #169\n",
      "[170,     1] train loss: 31.3920059\n",
      "[170,     3] train loss: 18.8878371\n",
      "Epoch #170\n",
      "[171,     1] train loss: 29.5160160\n",
      "[171,     3] train loss: 18.8094648\n",
      "Epoch #171\n",
      "[172,     1] train loss: 29.7518845\n",
      "[172,     3] train loss: 19.0364679\n",
      "Epoch #172\n",
      "[173,     1] train loss: 29.8314533\n",
      "[173,     3] train loss: 16.9196440\n",
      "Epoch #173\n",
      "[174,     1] train loss: 23.5817299\n",
      "[174,     3] train loss: 20.1567586\n",
      "Epoch #174\n",
      "[175,     1] train loss: 29.8977051\n",
      "[175,     3] train loss: 18.2741362\n",
      "Epoch #175\n",
      "[176,     1] train loss: 29.6431351\n",
      "[176,     3] train loss: 17.7852147\n",
      "Epoch #176\n",
      "[177,     1] train loss: 26.5368748\n",
      "[177,     3] train loss: 19.5451927\n",
      "Epoch #177\n",
      "[178,     1] train loss: 27.3951263\n",
      "[178,     3] train loss: 19.6457100\n",
      "Epoch #178\n",
      "[179,     1] train loss: 24.2787323\n",
      "[179,     3] train loss: 19.3626728\n",
      "Epoch #179\n",
      "[180,     1] train loss: 29.9550056\n",
      "[180,     3] train loss: 18.2628015\n",
      "Epoch #180\n",
      "[181,     1] train loss: 29.5684624\n",
      "[181,     3] train loss: 17.7719046\n",
      "Epoch #181\n",
      "[182,     1] train loss: 29.4875584\n",
      "[182,     3] train loss: 17.4478645\n",
      "Epoch #182\n",
      "[183,     1] train loss: 27.4702148\n",
      "[183,     3] train loss: 17.5635262\n",
      "Epoch #183\n",
      "[184,     1] train loss: 31.5907841\n",
      "[184,     3] train loss: 17.9034125\n",
      "Epoch #184\n",
      "[185,     1] train loss: 30.8592911\n",
      "[185,     3] train loss: 18.7996108\n",
      "Epoch #185\n",
      "[186,     1] train loss: 27.0673046\n",
      "[186,     3] train loss: 18.2917131\n",
      "Epoch #186\n",
      "[187,     1] train loss: 25.3041821\n",
      "[187,     3] train loss: 18.0022259\n",
      "Epoch #187\n",
      "[188,     1] train loss: 28.4997025\n",
      "[188,     3] train loss: 19.4242757\n",
      "Epoch #188\n",
      "[189,     1] train loss: 24.9337101\n",
      "[189,     3] train loss: 20.2303066\n",
      "Epoch #189\n",
      "[190,     1] train loss: 26.0059853\n",
      "[190,     3] train loss: 20.8081125\n",
      "Epoch #190\n",
      "[191,     1] train loss: 26.8821564\n",
      "[191,     3] train loss: 19.3119774\n",
      "Epoch #191\n",
      "[192,     1] train loss: 27.6120777\n",
      "[192,     3] train loss: 18.2123127\n",
      "Epoch #192\n",
      "[193,     1] train loss: 28.8113117\n",
      "[193,     3] train loss: 17.8283037\n",
      "Epoch #193\n",
      "[194,     1] train loss: 30.0939236\n",
      "[194,     3] train loss: 18.8907750\n",
      "Epoch #194\n",
      "[195,     1] train loss: 25.4054260\n",
      "[195,     3] train loss: 20.2031549\n",
      "Epoch #195\n",
      "[196,     1] train loss: 31.5022507\n",
      "[196,     3] train loss: 18.5518456\n",
      "Epoch #196\n",
      "[197,     1] train loss: 26.0311089\n",
      "[197,     3] train loss: 19.8356628\n",
      "Epoch #197\n",
      "[198,     1] train loss: 31.3437176\n",
      "[198,     3] train loss: 18.5055765\n",
      "Epoch #198\n",
      "[199,     1] train loss: 26.3464909\n",
      "[199,     3] train loss: 19.5143890\n",
      "Epoch #199\n",
      "[200,     1] train loss: 26.9471951\n",
      "[200,     3] train loss: 20.9454524\n",
      "Epoch #200\n",
      "[201,     1] train loss: 28.9666996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201,     3] train loss: 19.1579348\n",
      "Epoch #201\n",
      "[202,     1] train loss: 27.2719536\n",
      "[202,     3] train loss: 18.9451764\n",
      "Epoch #202\n",
      "[203,     1] train loss: 28.1660423\n",
      "[203,     3] train loss: 17.9565531\n",
      "Epoch #203\n",
      "[204,     1] train loss: 28.4628468\n",
      "[204,     3] train loss: 19.1123473\n",
      "Epoch #204\n",
      "[205,     1] train loss: 26.0691242\n",
      "[205,     3] train loss: 20.3905163\n",
      "Epoch #205\n",
      "[206,     1] train loss: 27.4014263\n",
      "[206,     3] train loss: 18.6769555\n",
      "Epoch #206\n",
      "[207,     1] train loss: 27.9602318\n",
      "[207,     3] train loss: 19.1028055\n",
      "Epoch #207\n",
      "[208,     1] train loss: 28.7978764\n",
      "[208,     3] train loss: 19.9825357\n",
      "Epoch #208\n",
      "[209,     1] train loss: 29.8750973\n",
      "[209,     3] train loss: 17.8352534\n",
      "Epoch #209\n",
      "[210,     1] train loss: 26.7148476\n",
      "[210,     3] train loss: 20.1192767\n",
      "Epoch #210\n",
      "[211,     1] train loss: 27.8816757\n",
      "[211,     3] train loss: 19.2965908\n",
      "Epoch #211\n",
      "[212,     1] train loss: 27.9196453\n",
      "[212,     3] train loss: 19.7565893\n",
      "Epoch #212\n",
      "[213,     1] train loss: 24.8081093\n",
      "[213,     3] train loss: 19.7720661\n",
      "Epoch #213\n",
      "[214,     1] train loss: 26.3977680\n",
      "[214,     3] train loss: 19.3350372\n",
      "Epoch #214\n",
      "[215,     1] train loss: 28.2395935\n",
      "[215,     3] train loss: 20.7396240\n",
      "Epoch #215\n",
      "[216,     1] train loss: 29.7770576\n",
      "[216,     3] train loss: 18.5005519\n",
      "Epoch #216\n",
      "[217,     1] train loss: 29.2745533\n",
      "[217,     3] train loss: 20.7040278\n",
      "Epoch #217\n",
      "[218,     1] train loss: 30.9561539\n",
      "[218,     3] train loss: 17.9576721\n",
      "Epoch #218\n",
      "[219,     1] train loss: 26.4113407\n",
      "[219,     3] train loss: 17.1952686\n",
      "Epoch #219\n",
      "[220,     1] train loss: 27.6994438\n",
      "[220,     3] train loss: 19.4762433\n",
      "Epoch #220\n",
      "[221,     1] train loss: 24.1115799\n",
      "[221,     3] train loss: 20.3105481\n",
      "Epoch #221\n",
      "[222,     1] train loss: 28.7274113\n",
      "[222,     3] train loss: 17.8607121\n",
      "Epoch #222\n",
      "[223,     1] train loss: 28.3311272\n",
      "[223,     3] train loss: 19.5857430\n",
      "Epoch #223\n",
      "[224,     1] train loss: 28.2853203\n",
      "[224,     3] train loss: 18.6465041\n",
      "Epoch #224\n",
      "[225,     1] train loss: 27.8605080\n",
      "[225,     3] train loss: 18.7672774\n",
      "Epoch #225\n",
      "[226,     1] train loss: 27.9193439\n",
      "[226,     3] train loss: 18.0727952\n",
      "Epoch #226\n",
      "[227,     1] train loss: 29.0385342\n",
      "[227,     3] train loss: 19.6622111\n",
      "Epoch #227\n",
      "[228,     1] train loss: 26.1777096\n",
      "[228,     3] train loss: 18.0023460\n",
      "Epoch #228\n",
      "[229,     1] train loss: 29.4114208\n",
      "[229,     3] train loss: 17.4400260\n",
      "Epoch #229\n",
      "[230,     1] train loss: 26.2097111\n",
      "[230,     3] train loss: 19.5923545\n",
      "Epoch #230\n",
      "[231,     1] train loss: 30.3972034\n",
      "[231,     3] train loss: 17.4885317\n",
      "Epoch #231\n",
      "[232,     1] train loss: 29.6439018\n",
      "[232,     3] train loss: 19.7800128\n",
      "Epoch #232\n",
      "[233,     1] train loss: 27.1017437\n",
      "[233,     3] train loss: 19.8032443\n",
      "Epoch #233\n",
      "[234,     1] train loss: 27.1291847\n",
      "[234,     3] train loss: 18.9922822\n",
      "Epoch #234\n",
      "[235,     1] train loss: 29.8855476\n",
      "[235,     3] train loss: 18.1327610\n",
      "Epoch #235\n",
      "[236,     1] train loss: 25.4969902\n",
      "[236,     3] train loss: 19.8148289\n",
      "Epoch #236\n",
      "[237,     1] train loss: 28.4689426\n",
      "[237,     3] train loss: 20.1677577\n",
      "Epoch #237\n",
      "[238,     1] train loss: 26.7120972\n",
      "[238,     3] train loss: 19.6170203\n",
      "Epoch #238\n",
      "[239,     1] train loss: 28.4214401\n",
      "[239,     3] train loss: 18.7357782\n",
      "Epoch #239\n",
      "[240,     1] train loss: 26.1399765\n",
      "[240,     3] train loss: 19.5491670\n",
      "Epoch #240\n",
      "[241,     1] train loss: 26.6022587\n",
      "[241,     3] train loss: 19.1149896\n",
      "Epoch #241\n",
      "[242,     1] train loss: 27.4700623\n",
      "[242,     3] train loss: 18.7862670\n",
      "Epoch #242\n",
      "[243,     1] train loss: 29.9739761\n",
      "[243,     3] train loss: 18.3630492\n",
      "Epoch #243\n",
      "[244,     1] train loss: 31.2061157\n",
      "[244,     3] train loss: 16.4087607\n",
      "Epoch #244\n",
      "[245,     1] train loss: 26.7739258\n",
      "[245,     3] train loss: 19.5666485\n",
      "Epoch #245\n",
      "[246,     1] train loss: 28.5859318\n",
      "[246,     3] train loss: 18.5384204\n",
      "Epoch #246\n",
      "[247,     1] train loss: 27.9087009\n",
      "[247,     3] train loss: 18.7260984\n",
      "Epoch #247\n",
      "[248,     1] train loss: 26.6786060\n",
      "[248,     3] train loss: 19.9017042\n",
      "Epoch #248\n",
      "[249,     1] train loss: 27.0518131\n",
      "[249,     3] train loss: 19.2364108\n",
      "Epoch #249\n",
      "[250,     1] train loss: 27.6401043\n",
      "[250,     3] train loss: 18.5695305\n",
      "Epoch #250\n",
      "[251,     1] train loss: 26.6194286\n",
      "[251,     3] train loss: 20.3110491\n",
      "Epoch #251\n",
      "[252,     1] train loss: 26.2933331\n",
      "[252,     3] train loss: 18.4756749\n",
      "Epoch #252\n",
      "[253,     1] train loss: 27.8705273\n",
      "[253,     3] train loss: 17.0251624\n",
      "Epoch #253\n",
      "[254,     1] train loss: 28.1401005\n",
      "[254,     3] train loss: 19.3682073\n",
      "Epoch #254\n",
      "[255,     1] train loss: 30.4713058\n",
      "[255,     3] train loss: 18.0951869\n",
      "Epoch #255\n",
      "[256,     1] train loss: 26.0592613\n",
      "[256,     3] train loss: 20.8346488\n",
      "Epoch #256\n",
      "[257,     1] train loss: 24.9670086\n",
      "[257,     3] train loss: 20.0543683\n",
      "Epoch #257\n",
      "[258,     1] train loss: 28.1388531\n",
      "[258,     3] train loss: 19.6485780\n",
      "Epoch #258\n",
      "[259,     1] train loss: 28.0394173\n",
      "[259,     3] train loss: 18.6699060\n",
      "Epoch #259\n",
      "[260,     1] train loss: 29.4599342\n",
      "[260,     3] train loss: 18.5840778\n",
      "Epoch #260\n",
      "[261,     1] train loss: 28.2061939\n",
      "[261,     3] train loss: 19.0754852\n",
      "Epoch #261\n",
      "[262,     1] train loss: 27.8952732\n",
      "[262,     3] train loss: 18.5655390\n",
      "Epoch #262\n",
      "[263,     1] train loss: 28.1898537\n",
      "[263,     3] train loss: 18.8797747\n",
      "Epoch #263\n",
      "[264,     1] train loss: 31.4993286\n",
      "[264,     3] train loss: 18.4462064\n",
      "Epoch #264\n",
      "[265,     1] train loss: 32.4326935\n",
      "[265,     3] train loss: 18.8420099\n",
      "Epoch #265\n",
      "[266,     1] train loss: 27.2102547\n",
      "[266,     3] train loss: 17.5496254\n",
      "Epoch #266\n",
      "[267,     1] train loss: 28.6741600\n",
      "[267,     3] train loss: 18.7244002\n",
      "Epoch #267\n",
      "[268,     1] train loss: 28.6438618\n",
      "[268,     3] train loss: 18.1444969\n",
      "Epoch #268\n",
      "[269,     1] train loss: 31.8729210\n",
      "[269,     3] train loss: 17.5982126\n",
      "Epoch #269\n",
      "[270,     1] train loss: 29.4086742\n",
      "[270,     3] train loss: 16.8985659\n",
      "Epoch #270\n",
      "[271,     1] train loss: 26.0603867\n",
      "[271,     3] train loss: 19.6048311\n",
      "Epoch #271\n",
      "[272,     1] train loss: 28.7842503\n",
      "[272,     3] train loss: 18.8372828\n",
      "Epoch #272\n",
      "[273,     1] train loss: 27.2619686\n",
      "[273,     3] train loss: 18.5384515\n",
      "Epoch #273\n",
      "[274,     1] train loss: 26.9477291\n",
      "[274,     3] train loss: 19.4619471\n",
      "Epoch #274\n",
      "[275,     1] train loss: 23.5582008\n",
      "[275,     3] train loss: 21.0239716\n",
      "Epoch #275\n",
      "[276,     1] train loss: 28.5268536\n",
      "[276,     3] train loss: 20.1928329\n",
      "Epoch #276\n",
      "[277,     1] train loss: 27.9674416\n",
      "[277,     3] train loss: 17.4956710\n",
      "Epoch #277\n",
      "[278,     1] train loss: 28.6722698\n",
      "[278,     3] train loss: 18.8643277\n",
      "Epoch #278\n",
      "[279,     1] train loss: 24.8741894\n",
      "[279,     3] train loss: 18.3887952\n",
      "Epoch #279\n",
      "[280,     1] train loss: 29.2709961\n",
      "[280,     3] train loss: 18.5152810\n",
      "Epoch #280\n",
      "[281,     1] train loss: 26.9101448\n",
      "[281,     3] train loss: 20.4960473\n",
      "Epoch #281\n",
      "[282,     1] train loss: 29.8599148\n",
      "[282,     3] train loss: 19.0067291\n",
      "Epoch #282\n",
      "[283,     1] train loss: 24.8897018\n",
      "[283,     3] train loss: 20.4684607\n",
      "Epoch #283\n",
      "[284,     1] train loss: 29.3209534\n",
      "[284,     3] train loss: 18.3684317\n",
      "Epoch #284\n",
      "[285,     1] train loss: 30.0417614\n",
      "[285,     3] train loss: 19.0243689\n",
      "Epoch #285\n",
      "[286,     1] train loss: 27.5569305\n",
      "[286,     3] train loss: 18.1497542\n",
      "Epoch #286\n",
      "[287,     1] train loss: 27.0098286\n",
      "[287,     3] train loss: 19.3174566\n",
      "Epoch #287\n",
      "[288,     1] train loss: 26.8374557\n",
      "[288,     3] train loss: 17.9873943\n",
      "Epoch #288\n",
      "[289,     1] train loss: 29.7747345\n",
      "[289,     3] train loss: 18.7654661\n",
      "Epoch #289\n",
      "[290,     1] train loss: 28.4484539\n",
      "[290,     3] train loss: 17.6367378\n",
      "Epoch #290\n",
      "[291,     1] train loss: 26.4045410\n",
      "[291,     3] train loss: 20.0427259\n",
      "Epoch #291\n",
      "[292,     1] train loss: 27.1399632\n",
      "[292,     3] train loss: 18.6555049\n",
      "Epoch #292\n",
      "[293,     1] train loss: 28.5849152\n",
      "[293,     3] train loss: 20.4281731\n",
      "Epoch #293\n",
      "[294,     1] train loss: 26.8726444\n",
      "[294,     3] train loss: 17.4946156\n",
      "Epoch #294\n",
      "[295,     1] train loss: 26.3910084\n",
      "[295,     3] train loss: 18.5893510\n",
      "Epoch #295\n",
      "[296,     1] train loss: 28.8648529\n",
      "[296,     3] train loss: 17.8826752\n",
      "Epoch #296\n",
      "[297,     1] train loss: 28.8277187\n",
      "[297,     3] train loss: 19.6063328\n",
      "Epoch #297\n",
      "[298,     1] train loss: 25.7154331\n",
      "[298,     3] train loss: 19.7767525\n",
      "Epoch #298\n",
      "[299,     1] train loss: 28.2485638\n",
      "[299,     3] train loss: 17.8344409\n",
      "Epoch #299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300,     1] train loss: 30.3923187\n",
      "[300,     3] train loss: 17.6899738\n",
      "Epoch #300\n",
      "[301,     1] train loss: 27.6293888\n",
      "[301,     3] train loss: 17.8493633\n",
      "Epoch #301\n",
      "[302,     1] train loss: 27.4966278\n",
      "[302,     3] train loss: 19.7745558\n",
      "Epoch #302\n",
      "[303,     1] train loss: 27.9991074\n",
      "[303,     3] train loss: 19.2103895\n",
      "Epoch #303\n",
      "[304,     1] train loss: 30.2026062\n",
      "[304,     3] train loss: 17.5811253\n",
      "Epoch #304\n",
      "[305,     1] train loss: 28.0624447\n",
      "[305,     3] train loss: 18.5894216\n",
      "Epoch #305\n",
      "[306,     1] train loss: 28.4760761\n",
      "[306,     3] train loss: 18.7672176\n",
      "Epoch #306\n",
      "[307,     1] train loss: 30.1471806\n",
      "[307,     3] train loss: 18.3585606\n",
      "Epoch #307\n",
      "[308,     1] train loss: 27.3481598\n",
      "[308,     3] train loss: 18.5899919\n",
      "Epoch #308\n",
      "[309,     1] train loss: 30.1893063\n",
      "[309,     3] train loss: 20.0835444\n",
      "Epoch #309\n",
      "[310,     1] train loss: 32.1561852\n",
      "[310,     3] train loss: 18.3760217\n",
      "Epoch #310\n",
      "[311,     1] train loss: 28.7718067\n",
      "[311,     3] train loss: 17.8580774\n",
      "Epoch #311\n",
      "[312,     1] train loss: 28.8223610\n",
      "[312,     3] train loss: 17.6096242\n",
      "Epoch #312\n",
      "[313,     1] train loss: 26.6827374\n",
      "[313,     3] train loss: 18.9236368\n",
      "Epoch #313\n",
      "[314,     1] train loss: 29.6812801\n",
      "[314,     3] train loss: 18.5453396\n",
      "Epoch #314\n",
      "[315,     1] train loss: 30.9755421\n",
      "[315,     3] train loss: 18.4337540\n",
      "Epoch #315\n",
      "[316,     1] train loss: 31.6537533\n",
      "[316,     3] train loss: 18.3150260\n",
      "Epoch #316\n",
      "[317,     1] train loss: 30.2086926\n",
      "[317,     3] train loss: 17.4888465\n",
      "Epoch #317\n",
      "[318,     1] train loss: 30.0660152\n",
      "[318,     3] train loss: 18.1389300\n",
      "Epoch #318\n",
      "[319,     1] train loss: 26.7288780\n",
      "[319,     3] train loss: 18.4064433\n",
      "Epoch #319\n",
      "[320,     1] train loss: 26.7791996\n",
      "[320,     3] train loss: 19.9296627\n",
      "Epoch #320\n",
      "[321,     1] train loss: 28.3147373\n",
      "[321,     3] train loss: 17.9405117\n",
      "Epoch #321\n",
      "[322,     1] train loss: 25.3691483\n",
      "[322,     3] train loss: 20.1556498\n",
      "Epoch #322\n",
      "[323,     1] train loss: 25.9718990\n",
      "[323,     3] train loss: 19.2167110\n",
      "Epoch #323\n",
      "[324,     1] train loss: 30.3270359\n",
      "[324,     3] train loss: 17.3434168\n",
      "Epoch #324\n",
      "[325,     1] train loss: 30.4468365\n",
      "[325,     3] train loss: 18.1992270\n",
      "Epoch #325\n",
      "[326,     1] train loss: 28.0341263\n",
      "[326,     3] train loss: 19.5594552\n",
      "Epoch #326\n",
      "[327,     1] train loss: 27.5396633\n",
      "[327,     3] train loss: 19.6848583\n",
      "Epoch #327\n",
      "[328,     1] train loss: 27.1670036\n",
      "[328,     3] train loss: 19.2896105\n",
      "Epoch #328\n",
      "[329,     1] train loss: 26.7942219\n",
      "[329,     3] train loss: 20.0402972\n",
      "Epoch #329\n",
      "[330,     1] train loss: 28.7309399\n",
      "[330,     3] train loss: 19.4880371\n",
      "Epoch #330\n",
      "[331,     1] train loss: 29.6385288\n",
      "[331,     3] train loss: 18.1284618\n",
      "Epoch #331\n",
      "[332,     1] train loss: 26.5527115\n",
      "[332,     3] train loss: 18.4693012\n",
      "Epoch #332\n",
      "[333,     1] train loss: 29.1404858\n",
      "[333,     3] train loss: 18.7132327\n",
      "Epoch #333\n",
      "[334,     1] train loss: 28.4567280\n",
      "[334,     3] train loss: 19.3200843\n",
      "Epoch #334\n",
      "[335,     1] train loss: 28.8355942\n",
      "[335,     3] train loss: 18.6700974\n",
      "Epoch #335\n",
      "[336,     1] train loss: 26.4835567\n",
      "[336,     3] train loss: 20.1339423\n",
      "Epoch #336\n",
      "[337,     1] train loss: 29.1486473\n",
      "[337,     3] train loss: 18.3532174\n",
      "Epoch #337\n",
      "[338,     1] train loss: 25.7507401\n",
      "[338,     3] train loss: 19.9620488\n",
      "Epoch #338\n",
      "[339,     1] train loss: 25.1067829\n",
      "[339,     3] train loss: 18.0910950\n",
      "Epoch #339\n",
      "[340,     1] train loss: 26.9042873\n",
      "[340,     3] train loss: 18.9574935\n",
      "Epoch #340\n",
      "[341,     1] train loss: 28.1644554\n",
      "[341,     3] train loss: 17.9284159\n",
      "Epoch #341\n",
      "[342,     1] train loss: 28.7909069\n",
      "[342,     3] train loss: 18.9188913\n",
      "Epoch #342\n",
      "[343,     1] train loss: 28.3720417\n",
      "[343,     3] train loss: 19.3489507\n",
      "Epoch #343\n",
      "[344,     1] train loss: 32.5976181\n",
      "[344,     3] train loss: 19.1953462\n",
      "Epoch #344\n",
      "[345,     1] train loss: 26.8124638\n",
      "[345,     3] train loss: 19.3573119\n",
      "Epoch #345\n",
      "[346,     1] train loss: 30.0899372\n",
      "[346,     3] train loss: 17.7724857\n",
      "Epoch #346\n",
      "[347,     1] train loss: 26.3523102\n",
      "[347,     3] train loss: 20.0816237\n",
      "Epoch #347\n",
      "[348,     1] train loss: 27.7294197\n",
      "[348,     3] train loss: 19.5792898\n",
      "Epoch #348\n",
      "[349,     1] train loss: 27.5906620\n",
      "[349,     3] train loss: 17.8198083\n",
      "Epoch #349\n",
      "[350,     1] train loss: 26.4070721\n",
      "[350,     3] train loss: 19.1031844\n",
      "Epoch #350\n",
      "[351,     1] train loss: 24.8948498\n",
      "[351,     3] train loss: 20.2322706\n",
      "Epoch #351\n",
      "[352,     1] train loss: 28.6348515\n",
      "[352,     3] train loss: 19.2234491\n",
      "Epoch #352\n",
      "[353,     1] train loss: 28.1985550\n",
      "[353,     3] train loss: 17.1023159\n",
      "Epoch #353\n",
      "[354,     1] train loss: 26.2694244\n",
      "[354,     3] train loss: 18.2139721\n",
      "Epoch #354\n",
      "[355,     1] train loss: 28.8187733\n",
      "[355,     3] train loss: 19.4628620\n",
      "Epoch #355\n",
      "[356,     1] train loss: 28.5988979\n",
      "[356,     3] train loss: 18.5607026\n",
      "Epoch #356\n",
      "[357,     1] train loss: 30.5789146\n",
      "[357,     3] train loss: 16.9241517\n",
      "Epoch #357\n",
      "[358,     1] train loss: 26.3654480\n",
      "[358,     3] train loss: 18.5576960\n",
      "Epoch #358\n",
      "[359,     1] train loss: 27.1830215\n",
      "[359,     3] train loss: 18.0993112\n",
      "Epoch #359\n",
      "[360,     1] train loss: 29.9245663\n",
      "[360,     3] train loss: 18.7785842\n",
      "Epoch #360\n",
      "[361,     1] train loss: 31.1102333\n",
      "[361,     3] train loss: 18.3035558\n",
      "Epoch #361\n",
      "[362,     1] train loss: 28.4592705\n",
      "[362,     3] train loss: 19.1509444\n",
      "Epoch #362\n",
      "[363,     1] train loss: 29.5792618\n",
      "[363,     3] train loss: 18.0175858\n",
      "Epoch #363\n",
      "[364,     1] train loss: 29.0359898\n",
      "[364,     3] train loss: 17.9801865\n",
      "Epoch #364\n",
      "[365,     1] train loss: 28.2432995\n",
      "[365,     3] train loss: 19.3354238\n",
      "Epoch #365\n",
      "[366,     1] train loss: 29.8612347\n",
      "[366,     3] train loss: 18.6786639\n",
      "Epoch #366\n",
      "[367,     1] train loss: 28.7635632\n",
      "[367,     3] train loss: 18.2046763\n",
      "Epoch #367\n",
      "[368,     1] train loss: 30.7653141\n",
      "[368,     3] train loss: 17.2448832\n",
      "Epoch #368\n",
      "[369,     1] train loss: 26.5950737\n",
      "[369,     3] train loss: 20.9164098\n",
      "Epoch #369\n",
      "[370,     1] train loss: 29.5674248\n",
      "[370,     3] train loss: 19.4698900\n",
      "Epoch #370\n",
      "[371,     1] train loss: 34.5576515\n",
      "[371,     3] train loss: 17.7889798\n",
      "Epoch #371\n",
      "[372,     1] train loss: 30.9259148\n",
      "[372,     3] train loss: 17.1381372\n",
      "Epoch #372\n",
      "[373,     1] train loss: 31.1898155\n",
      "[373,     3] train loss: 18.0221774\n",
      "Epoch #373\n",
      "[374,     1] train loss: 28.5918789\n",
      "[374,     3] train loss: 18.0244598\n",
      "Epoch #374\n",
      "[375,     1] train loss: 29.3013458\n",
      "[375,     3] train loss: 18.4262975\n",
      "Epoch #375\n",
      "[376,     1] train loss: 29.5234337\n",
      "[376,     3] train loss: 18.4457607\n",
      "Epoch #376\n",
      "[377,     1] train loss: 27.5540562\n",
      "[377,     3] train loss: 19.8799502\n",
      "Epoch #377\n",
      "[378,     1] train loss: 26.7818356\n",
      "[378,     3] train loss: 21.0745112\n",
      "Epoch #378\n",
      "[379,     1] train loss: 32.5529060\n",
      "[379,     3] train loss: 17.3333848\n",
      "Epoch #379\n",
      "[380,     1] train loss: 26.4850368\n",
      "[380,     3] train loss: 18.4030272\n",
      "Epoch #380\n",
      "[381,     1] train loss: 24.8201885\n",
      "[381,     3] train loss: 19.8769779\n",
      "Epoch #381\n",
      "[382,     1] train loss: 27.0268745\n",
      "[382,     3] train loss: 18.2947731\n",
      "Epoch #382\n",
      "[383,     1] train loss: 28.5032158\n",
      "[383,     3] train loss: 18.0614929\n",
      "Epoch #383\n",
      "[384,     1] train loss: 30.0505695\n",
      "[384,     3] train loss: 17.6321691\n",
      "Epoch #384\n",
      "[385,     1] train loss: 29.7246666\n",
      "[385,     3] train loss: 18.5215429\n",
      "Epoch #385\n",
      "[386,     1] train loss: 25.9269295\n",
      "[386,     3] train loss: 19.5866992\n",
      "Epoch #386\n",
      "[387,     1] train loss: 24.8374596\n",
      "[387,     3] train loss: 21.3904921\n",
      "Epoch #387\n",
      "[388,     1] train loss: 24.4144363\n",
      "[388,     3] train loss: 20.6048527\n",
      "Epoch #388\n",
      "[389,     1] train loss: 26.5557995\n",
      "[389,     3] train loss: 19.3532925\n",
      "Epoch #389\n",
      "[390,     1] train loss: 26.9062042\n",
      "[390,     3] train loss: 19.8866749\n",
      "Epoch #390\n",
      "[391,     1] train loss: 29.4371490\n",
      "[391,     3] train loss: 18.1996797\n",
      "Epoch #391\n",
      "[392,     1] train loss: 26.9801636\n",
      "[392,     3] train loss: 19.8250217\n",
      "Epoch #392\n",
      "[393,     1] train loss: 26.4820118\n",
      "[393,     3] train loss: 19.3619207\n",
      "Epoch #393\n",
      "[394,     1] train loss: 28.0291252\n",
      "[394,     3] train loss: 18.5044435\n",
      "Epoch #394\n",
      "[395,     1] train loss: 30.5569630\n",
      "[395,     3] train loss: 17.6423225\n",
      "Epoch #395\n",
      "[396,     1] train loss: 28.3137913\n",
      "[396,     3] train loss: 18.4402593\n",
      "Epoch #396\n",
      "[397,     1] train loss: 30.3099518\n",
      "[397,     3] train loss: 18.7162393\n",
      "Epoch #397\n",
      "[398,     1] train loss: 27.7349052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[398,     3] train loss: 19.5867825\n",
      "Epoch #398\n",
      "[399,     1] train loss: 29.6713867\n",
      "[399,     3] train loss: 18.1012936\n",
      "Epoch #399\n",
      "[400,     1] train loss: 29.7954197\n",
      "[400,     3] train loss: 17.8211708\n",
      "Epoch #400\n",
      "[401,     1] train loss: 30.1878490\n",
      "[401,     3] train loss: 17.4114666\n",
      "Epoch #401\n",
      "[402,     1] train loss: 28.0783558\n",
      "[402,     3] train loss: 18.2314981\n",
      "Epoch #402\n",
      "[403,     1] train loss: 26.1399498\n",
      "[403,     3] train loss: 16.7252471\n",
      "Epoch #403\n",
      "[404,     1] train loss: 29.1355400\n",
      "[404,     3] train loss: 17.9241695\n",
      "Epoch #404\n",
      "[405,     1] train loss: 26.9448223\n",
      "[405,     3] train loss: 20.4522317\n",
      "Epoch #405\n",
      "[406,     1] train loss: 25.8406010\n",
      "[406,     3] train loss: 19.5218582\n",
      "Epoch #406\n",
      "[407,     1] train loss: 28.5256939\n",
      "[407,     3] train loss: 19.9732679\n",
      "Epoch #407\n",
      "[408,     1] train loss: 30.1805439\n",
      "[408,     3] train loss: 17.5216904\n",
      "Epoch #408\n",
      "[409,     1] train loss: 28.6391373\n",
      "[409,     3] train loss: 19.7009761\n",
      "Epoch #409\n",
      "[410,     1] train loss: 25.5798473\n",
      "[410,     3] train loss: 20.0173505\n",
      "Epoch #410\n",
      "[411,     1] train loss: 31.2589188\n",
      "[411,     3] train loss: 17.7772566\n",
      "Epoch #411\n",
      "[412,     1] train loss: 29.5745678\n",
      "[412,     3] train loss: 18.4010086\n",
      "Epoch #412\n",
      "[413,     1] train loss: 30.6161137\n",
      "[413,     3] train loss: 17.7833926\n",
      "Epoch #413\n",
      "[414,     1] train loss: 25.2500954\n",
      "[414,     3] train loss: 19.5643800\n",
      "Epoch #414\n",
      "[415,     1] train loss: 26.9783955\n",
      "[415,     3] train loss: 18.1723105\n",
      "Epoch #415\n",
      "[416,     1] train loss: 27.8860779\n",
      "[416,     3] train loss: 19.9587173\n",
      "Epoch #416\n",
      "[417,     1] train loss: 25.1588593\n",
      "[417,     3] train loss: 20.0760180\n",
      "Epoch #417\n",
      "[418,     1] train loss: 26.0284863\n",
      "[418,     3] train loss: 20.9746024\n",
      "Epoch #418\n",
      "[419,     1] train loss: 25.6515236\n",
      "[419,     3] train loss: 19.9093520\n",
      "Epoch #419\n",
      "[420,     1] train loss: 29.3703308\n",
      "[420,     3] train loss: 17.9295343\n",
      "Epoch #420\n",
      "[421,     1] train loss: 26.5696526\n",
      "[421,     3] train loss: 20.0901699\n",
      "Epoch #421\n",
      "[422,     1] train loss: 29.2973728\n",
      "[422,     3] train loss: 18.4759261\n",
      "Epoch #422\n",
      "[423,     1] train loss: 29.5907784\n",
      "[423,     3] train loss: 19.2363040\n",
      "Epoch #423\n",
      "[424,     1] train loss: 25.4555244\n",
      "[424,     3] train loss: 19.8650856\n",
      "Epoch #424\n",
      "[425,     1] train loss: 29.7933636\n",
      "[425,     3] train loss: 19.7249184\n",
      "Epoch #425\n",
      "[426,     1] train loss: 28.6520691\n",
      "[426,     3] train loss: 18.5892404\n",
      "Epoch #426\n",
      "[427,     1] train loss: 27.8386383\n",
      "[427,     3] train loss: 19.0203915\n",
      "Epoch #427\n",
      "[428,     1] train loss: 24.8689384\n",
      "[428,     3] train loss: 19.2821153\n",
      "Epoch #428\n",
      "[429,     1] train loss: 29.1678200\n",
      "[429,     3] train loss: 17.7772878\n",
      "Epoch #429\n",
      "[430,     1] train loss: 27.5281105\n",
      "[430,     3] train loss: 19.6919257\n",
      "Epoch #430\n",
      "[431,     1] train loss: 28.6488972\n",
      "[431,     3] train loss: 18.7870108\n",
      "Epoch #431\n",
      "[432,     1] train loss: 31.9709816\n",
      "[432,     3] train loss: 19.1805223\n",
      "Epoch #432\n",
      "[433,     1] train loss: 27.1133175\n",
      "[433,     3] train loss: 18.7608223\n",
      "Epoch #433\n",
      "[434,     1] train loss: 29.1252785\n",
      "[434,     3] train loss: 18.0350418\n",
      "Epoch #434\n",
      "[435,     1] train loss: 25.3394203\n",
      "[435,     3] train loss: 19.6026624\n",
      "Epoch #435\n",
      "[436,     1] train loss: 27.8945770\n",
      "[436,     3] train loss: 17.2689177\n",
      "Epoch #436\n",
      "[437,     1] train loss: 27.7209435\n",
      "[437,     3] train loss: 18.8258279\n",
      "Epoch #437\n",
      "[438,     1] train loss: 29.3133698\n",
      "[438,     3] train loss: 16.2729130\n",
      "Epoch #438\n",
      "[439,     1] train loss: 29.8251095\n",
      "[439,     3] train loss: 19.6858953\n",
      "Epoch #439\n",
      "[440,     1] train loss: 30.9172077\n",
      "[440,     3] train loss: 17.5709089\n",
      "Epoch #440\n",
      "[441,     1] train loss: 29.2257004\n",
      "[441,     3] train loss: 18.3559831\n",
      "Epoch #441\n",
      "[442,     1] train loss: 27.7812634\n",
      "[442,     3] train loss: 17.6345577\n",
      "Epoch #442\n",
      "[443,     1] train loss: 27.4874649\n",
      "[443,     3] train loss: 18.6849340\n",
      "Epoch #443\n",
      "[444,     1] train loss: 28.4707241\n",
      "[444,     3] train loss: 18.3845692\n",
      "Epoch #444\n",
      "[445,     1] train loss: 29.5192432\n",
      "[445,     3] train loss: 18.7678528\n",
      "Epoch #445\n",
      "[446,     1] train loss: 27.3407593\n",
      "[446,     3] train loss: 17.3723551\n",
      "Epoch #446\n",
      "[447,     1] train loss: 27.9999428\n",
      "[447,     3] train loss: 19.7884769\n",
      "Epoch #447\n",
      "[448,     1] train loss: 25.3519707\n",
      "[448,     3] train loss: 20.3654639\n",
      "Epoch #448\n",
      "[449,     1] train loss: 29.7678547\n",
      "[449,     3] train loss: 18.5144533\n",
      "Epoch #449\n",
      "[450,     1] train loss: 29.5523491\n",
      "[450,     3] train loss: 18.4469471\n",
      "Epoch #450\n",
      "[451,     1] train loss: 27.0180016\n",
      "[451,     3] train loss: 18.7257233\n",
      "Epoch #451\n",
      "[452,     1] train loss: 25.9662552\n",
      "[452,     3] train loss: 19.5102221\n",
      "Epoch #452\n",
      "[453,     1] train loss: 31.3133049\n",
      "[453,     3] train loss: 17.4253190\n",
      "Epoch #453\n",
      "[454,     1] train loss: 30.5068722\n",
      "[454,     3] train loss: 19.1922716\n",
      "Epoch #454\n",
      "[455,     1] train loss: 26.0699806\n",
      "[455,     3] train loss: 18.9271793\n",
      "Epoch #455\n",
      "[456,     1] train loss: 25.4186230\n",
      "[456,     3] train loss: 20.3401896\n",
      "Epoch #456\n",
      "[457,     1] train loss: 28.0229816\n",
      "[457,     3] train loss: 19.8991553\n",
      "Epoch #457\n",
      "[458,     1] train loss: 24.3729706\n",
      "[458,     3] train loss: 19.6380984\n",
      "Epoch #458\n",
      "[459,     1] train loss: 29.6069355\n",
      "[459,     3] train loss: 18.5166651\n",
      "Epoch #459\n",
      "[460,     1] train loss: 32.0640945\n",
      "[460,     3] train loss: 17.0097720\n",
      "Epoch #460\n",
      "[461,     1] train loss: 28.7259769\n",
      "[461,     3] train loss: 19.0599016\n",
      "Epoch #461\n",
      "[462,     1] train loss: 33.9180717\n",
      "[462,     3] train loss: 16.7200839\n",
      "Epoch #462\n",
      "[463,     1] train loss: 28.1386375\n",
      "[463,     3] train loss: 18.6062190\n",
      "Epoch #463\n",
      "[464,     1] train loss: 27.2216015\n",
      "[464,     3] train loss: 18.8165766\n",
      "Epoch #464\n",
      "[465,     1] train loss: 28.9076233\n",
      "[465,     3] train loss: 18.0654049\n",
      "Epoch #465\n",
      "[466,     1] train loss: 29.1660767\n",
      "[466,     3] train loss: 18.2355162\n",
      "Epoch #466\n",
      "[467,     1] train loss: 27.7795887\n",
      "[467,     3] train loss: 19.3083534\n",
      "Epoch #467\n",
      "[468,     1] train loss: 29.3481636\n",
      "[468,     3] train loss: 19.1394844\n",
      "Epoch #468\n",
      "[469,     1] train loss: 29.2681828\n",
      "[469,     3] train loss: 17.3180415\n",
      "Epoch #469\n",
      "[470,     1] train loss: 30.0061893\n",
      "[470,     3] train loss: 18.0110175\n",
      "Epoch #470\n",
      "[471,     1] train loss: 30.6850052\n",
      "[471,     3] train loss: 19.5762844\n",
      "Epoch #471\n",
      "[472,     1] train loss: 32.1319427\n",
      "[472,     3] train loss: 18.1662375\n",
      "Epoch #472\n",
      "[473,     1] train loss: 29.4304829\n",
      "[473,     3] train loss: 18.7997100\n",
      "Epoch #473\n",
      "[474,     1] train loss: 27.4499187\n",
      "[474,     3] train loss: 17.4137828\n",
      "Epoch #474\n",
      "[475,     1] train loss: 28.6428490\n",
      "[475,     3] train loss: 19.7573331\n",
      "Epoch #475\n",
      "[476,     1] train loss: 31.3892288\n",
      "[476,     3] train loss: 17.4481424\n",
      "Epoch #476\n",
      "[477,     1] train loss: 26.3257294\n",
      "[477,     3] train loss: 18.1909434\n",
      "Epoch #477\n",
      "[478,     1] train loss: 27.0014496\n",
      "[478,     3] train loss: 18.7795690\n",
      "Epoch #478\n",
      "[479,     1] train loss: 26.5255566\n",
      "[479,     3] train loss: 19.1337058\n",
      "Epoch #479\n",
      "[480,     1] train loss: 32.6991501\n",
      "[480,     3] train loss: 17.4085039\n",
      "Epoch #480\n",
      "[481,     1] train loss: 30.5956497\n",
      "[481,     3] train loss: 18.0023829\n",
      "Epoch #481\n",
      "[482,     1] train loss: 27.7250881\n",
      "[482,     3] train loss: 19.4779167\n",
      "Epoch #482\n",
      "[483,     1] train loss: 27.7055740\n",
      "[483,     3] train loss: 19.3587386\n",
      "Epoch #483\n",
      "[484,     1] train loss: 25.6980629\n",
      "[484,     3] train loss: 20.4671097\n",
      "Epoch #484\n",
      "[485,     1] train loss: 29.7279053\n",
      "[485,     3] train loss: 18.9927006\n",
      "Epoch #485\n",
      "[486,     1] train loss: 25.3858604\n",
      "[486,     3] train loss: 20.7881788\n",
      "Epoch #486\n",
      "[487,     1] train loss: 29.0926056\n",
      "[487,     3] train loss: 18.1514613\n",
      "Epoch #487\n",
      "[488,     1] train loss: 29.2376785\n",
      "[488,     3] train loss: 17.8829104\n",
      "Epoch #488\n",
      "[489,     1] train loss: 29.2696972\n",
      "[489,     3] train loss: 17.6897907\n",
      "Epoch #489\n",
      "[490,     1] train loss: 31.5583420\n",
      "[490,     3] train loss: 16.7650458\n",
      "Epoch #490\n",
      "[491,     1] train loss: 27.6643410\n",
      "[491,     3] train loss: 19.5533854\n",
      "Epoch #491\n",
      "[492,     1] train loss: 30.5119381\n",
      "[492,     3] train loss: 17.1947791\n",
      "Epoch #492\n",
      "[493,     1] train loss: 28.3786488\n",
      "[493,     3] train loss: 19.8216654\n",
      "Epoch #493\n",
      "[494,     1] train loss: 25.2714977\n",
      "[494,     3] train loss: 20.5232569\n",
      "Epoch #494\n",
      "[495,     1] train loss: 30.0415726\n",
      "[495,     3] train loss: 18.9097614\n",
      "Epoch #495\n",
      "[496,     1] train loss: 28.9750824\n",
      "[496,     3] train loss: 18.7299207\n",
      "Epoch #496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[497,     1] train loss: 29.0546112\n",
      "[497,     3] train loss: 18.8042183\n",
      "Epoch #497\n",
      "[498,     1] train loss: 27.4269485\n",
      "[498,     3] train loss: 19.2074114\n",
      "Epoch #498\n",
      "[499,     1] train loss: 27.3134022\n",
      "[499,     3] train loss: 18.9359773\n",
      "Epoch #499\n",
      "[500,     1] train loss: 27.0005932\n",
      "[500,     3] train loss: 19.5210088\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 256\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "        batch_bass_train_raw = batch_bass_train_raw.transpose(0, 1)\n",
    "        # transpose нужен БЫЛ для обмена размерности батча и размерности шагов\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train.size()}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 3,  ..., 4, 4, 2],\n",
       "        [1, 1, 0,  ..., 1, 1, 0],\n",
       "        [2, 3, 4,  ..., 2, 2, 4],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((16,50))\n",
    "\n",
    "batch_drum = drum_train + drum_test + drum_validation\n",
    "batch_bass = bass_train + bass_test + bass_validation\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(bass_outputs.size()[1]):\n",
    "        bass_seq = bass_outputs[:,i]\n",
    "#         bass_seq = batch_bass[:,i]\n",
    "#         print(f\"bass_seq:{bass_seq.size()}\")\n",
    "        bass_output = []\n",
    "        for bass_note in bass_seq:\n",
    "            bass_row = np.eye(1, 36, bass_note - 1)[0]\n",
    "            bass_output.append(bass_row)\n",
    "        bass_output = torch.tensor(bass_output).int().squeeze()\n",
    "#         print(f\"bass_output:{bass_output.size()}\")\n",
    "        \n",
    "#         print(f\"batch_drum:{batch_drum[:,i,:].size()}, bass_output:{bass_output.size()}\")\n",
    "            \n",
    "        img_dnb = np.concatenate((batch_drum[i].image,bass_output), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , batch_drum[i].tempo\n",
    "                                , batch_drum[i].instrument\n",
    "                                , 1\n",
    "                                , batch_drum[i].min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "patterns_file = \"decode_patterns/patterns.pairs.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            limit=1000,\n",
    "                                                            patterns_file=patterns_file,\n",
    "                                                            mono=False)\n",
    "# print(drum[0])\n",
    "# drum, bass = np.array(drum), np.array(bass)\n",
    "# print(drum[0])\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=120, instrument=25, denominator=1, min_note=50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self, bass_height, bass_width):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # save data parameters\n",
    "        self.bass_height = bass_height\n",
    "        self.bass_width = bass_width\n",
    "        self.bass_size = bass_height*bass_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        self.embedding_size = 2 # размер латентного пространства\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 4\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_preembed_layer = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.lstm_embed_layer_mean = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        self.lstm_embed_layer_logvar = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        \n",
    "        self.decoder_layer1 = nn.Linear(self.embedding_size + self.condition_size, 48)\n",
    "        self.decoder_layer2 = nn.Linear(48 + self.condition_size, 512)\n",
    "        self.decoder_layer3 = nn.Linear(512, self.bass_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_preembed_layer(output))\n",
    "        # избавляемся от лишней размерности (embedding_size=1), чтобы получить вектор из lstm\n",
    "        # размером с высоту изображения\n",
    "        output = output.squeeze().transpose(0,1)\n",
    "        mean = self.sigm(self.lstm_embed_layer_mean(output))\n",
    "        logvar = self.sigm(self.lstm_embed_layer_logvar(output))\n",
    "        return mean, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, input, cond):\n",
    "        output = torch.cat((input, cond), axis=1) # добавляем conditioning\n",
    "        output = self.sigm(self.decoder_layer1(output))\n",
    "        output = torch.cat((output, cond), axis=1) # добавляем ещё conditioning\n",
    "        output = self.sigm(self.decoder_layer2(output))\n",
    "        output = self.sigm(self.decoder_layer3(output))\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        images = self.get_images(input)\n",
    "        mean, logvar = self.encoder(images)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        # добавляем conditioning\n",
    "        conditionings = torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "        conditionings = conditionings\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.bass_height, self.bass_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN(data_height, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 3099.8581543\n",
      "[1,     6] train loss: 2411.4665527\n",
      "[1,    11] train loss: 2477.7588298\n",
      "[1,    16] train loss: 2569.6451416\n",
      "[1,    20] train loss: 2507.7742187\n",
      "Epoch #1\n",
      "[2,     1] train loss: 2991.0673828\n",
      "[2,     6] train loss: 2548.4679362\n",
      "[2,    11] train loss: 2421.4735107\n",
      "[2,    16] train loss: 2501.9620768\n",
      "[2,    20] train loss: 2476.5114746\n",
      "Epoch #2\n",
      "[3,     1] train loss: 3037.7915039\n",
      "[3,     6] train loss: 2392.2001546\n",
      "[3,    11] train loss: 2464.6499430\n",
      "[3,    16] train loss: 2546.7259928\n",
      "[3,    20] train loss: 2462.2242676\n",
      "Epoch #3\n",
      "[4,     1] train loss: 3154.8830566\n",
      "[4,     6] train loss: 2470.7625326\n",
      "[4,    11] train loss: 2476.6317952\n",
      "[4,    16] train loss: 2518.8664551\n",
      "[4,    20] train loss: 2444.0926270\n",
      "Epoch #4\n",
      "[5,     1] train loss: 2799.4099121\n",
      "[5,     6] train loss: 2357.0210368\n",
      "[5,    11] train loss: 2497.5878499\n",
      "[5,    16] train loss: 2710.6608887\n",
      "[5,    20] train loss: 2340.6559082\n",
      "Epoch #5\n",
      "[6,     1] train loss: 3173.7436523\n",
      "[6,     6] train loss: 2460.3833822\n",
      "[6,    11] train loss: 2452.3004150\n",
      "[6,    16] train loss: 2587.6835938\n",
      "[6,    20] train loss: 2405.6284180\n",
      "Epoch #6\n",
      "[7,     1] train loss: 3056.2187500\n",
      "[7,     6] train loss: 2437.1362305\n",
      "[7,    11] train loss: 2513.8846029\n",
      "[7,    16] train loss: 2537.1339925\n",
      "[7,    20] train loss: 2543.0982422\n",
      "Epoch #7\n",
      "[8,     1] train loss: 2898.1037598\n",
      "[8,     6] train loss: 2505.9113770\n",
      "[8,    11] train loss: 2447.6154378\n",
      "[8,    16] train loss: 2439.0713298\n",
      "[8,    20] train loss: 2413.0357422\n",
      "Epoch #8\n",
      "[9,     1] train loss: 3215.5988770\n",
      "[9,     6] train loss: 2433.1894124\n",
      "[9,    11] train loss: 2430.9071859\n",
      "[9,    16] train loss: 2569.1843262\n",
      "[9,    20] train loss: 2394.6656738\n",
      "Epoch #9\n",
      "[10,     1] train loss: 2605.6374512\n",
      "[10,     6] train loss: 2433.6525065\n",
      "[10,    11] train loss: 2465.1111654\n",
      "[10,    16] train loss: 2513.4716390\n",
      "[10,    20] train loss: 2424.6612305\n",
      "Epoch #10\n",
      "[11,     1] train loss: 3202.7551270\n",
      "[11,     6] train loss: 2501.1582031\n",
      "[11,    11] train loss: 2447.5736491\n",
      "[11,    16] train loss: 2512.8009033\n",
      "[11,    20] train loss: 2315.0304199\n",
      "Epoch #11\n",
      "[12,     1] train loss: 2800.8205566\n",
      "[12,     6] train loss: 2502.2296549\n",
      "[12,    11] train loss: 2463.3726400\n",
      "[12,    16] train loss: 2469.7224121\n",
      "[12,    20] train loss: 2364.3607422\n",
      "Epoch #12\n",
      "[13,     1] train loss: 2779.9072266\n",
      "[13,     6] train loss: 2512.2244059\n",
      "[13,    11] train loss: 2526.6353760\n",
      "[13,    16] train loss: 2455.1517741\n",
      "[13,    20] train loss: 2360.0055664\n",
      "Epoch #13\n",
      "[14,     1] train loss: 3064.0034180\n",
      "[14,     6] train loss: 2490.2506104\n",
      "[14,    11] train loss: 2485.0808512\n",
      "[14,    16] train loss: 2374.4641927\n",
      "[14,    20] train loss: 2532.3592773\n",
      "Epoch #14\n",
      "[15,     1] train loss: 3252.3115234\n",
      "[15,     6] train loss: 2439.1945801\n",
      "[15,    11] train loss: 2417.6092936\n",
      "[15,    16] train loss: 2452.2835286\n",
      "[15,    20] train loss: 2289.2720215\n",
      "Epoch #15\n",
      "[16,     1] train loss: 2759.3908691\n",
      "[16,     6] train loss: 2517.6334229\n",
      "[16,    11] train loss: 2440.8233643\n",
      "[16,    16] train loss: 2505.8609212\n",
      "[16,    20] train loss: 2372.5289063\n",
      "Epoch #16\n",
      "[17,     1] train loss: 2831.3876953\n",
      "[17,     6] train loss: 2464.8810221\n",
      "[17,    11] train loss: 2477.0578206\n",
      "[17,    16] train loss: 2364.2977295\n",
      "[17,    20] train loss: 2459.5716309\n",
      "Epoch #17\n",
      "[18,     1] train loss: 2747.4392090\n",
      "[18,     6] train loss: 2511.0010173\n",
      "[18,    11] train loss: 2464.8804118\n",
      "[18,    16] train loss: 2497.2839355\n",
      "[18,    20] train loss: 2299.0261230\n",
      "Epoch #18\n",
      "[19,     1] train loss: 2902.1743164\n",
      "[19,     6] train loss: 2394.4025472\n",
      "[19,    11] train loss: 2549.2176107\n",
      "[19,    16] train loss: 2576.6279704\n",
      "[19,    20] train loss: 2319.9942383\n",
      "Epoch #19\n",
      "[20,     1] train loss: 3297.5344238\n",
      "[20,     6] train loss: 2399.5118001\n",
      "[20,    11] train loss: 2441.7158203\n",
      "[20,    16] train loss: 2479.7187093\n",
      "[20,    20] train loss: 2320.5360352\n",
      "Epoch #20\n",
      "[21,     1] train loss: 2751.2351074\n",
      "[21,     6] train loss: 2421.4844971\n",
      "[21,    11] train loss: 2531.4125977\n",
      "[21,    16] train loss: 2497.9326986\n",
      "[21,    20] train loss: 2331.4674316\n",
      "Epoch #21\n",
      "[22,     1] train loss: 2912.2507324\n",
      "[22,     6] train loss: 2343.7261556\n",
      "[22,    11] train loss: 2491.8577067\n",
      "[22,    16] train loss: 2542.6631673\n",
      "[22,    20] train loss: 2379.1070801\n",
      "Epoch #22\n",
      "[23,     1] train loss: 2934.9628906\n",
      "[23,     6] train loss: 2388.0969238\n",
      "[23,    11] train loss: 2503.3446859\n",
      "[23,    16] train loss: 2392.5232340\n",
      "[23,    20] train loss: 2378.7708008\n",
      "Epoch #23\n",
      "[24,     1] train loss: 2675.1948242\n",
      "[24,     6] train loss: 2417.4405518\n",
      "[24,    11] train loss: 2573.6544596\n",
      "[24,    16] train loss: 2394.2836507\n",
      "[24,    20] train loss: 2283.5471191\n",
      "Epoch #24\n",
      "[25,     1] train loss: 2920.5573730\n",
      "[25,     6] train loss: 2391.6184082\n",
      "[25,    11] train loss: 2472.1038005\n",
      "[25,    16] train loss: 2408.6881917\n",
      "[25,    20] train loss: 2193.6031250\n",
      "Epoch #25\n",
      "[26,     1] train loss: 2675.9672852\n",
      "[26,     6] train loss: 2436.8013916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-a205f0f7797a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreconstruction_KL_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_lstm(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]\n",
    "y = mu[:,1]\n",
    "z = dev\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*20, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "batch_drum = drum_train + drum_test + drum_validation\n",
    "batch_drum = drum_validation\n",
    "batch_bass = bass_train + bass_test + bass_validation\n",
    "batch_bass = drum_validation\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)[0]\n",
    "    bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "    \n",
    "    for i in range(len(batch_drum)):\n",
    "            \n",
    "        img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , batch_drum[i].tempo\n",
    "                                , batch_drum[i].instrument\n",
    "                                , 1\n",
    "                                , batch_drum[i].min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 3\n",
    "sample2_id = 49\n",
    "sample1 = drum_validation[sample1_id]\n",
    "sample2 = drum_validation[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images([sample1, sample2]))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_lstm.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , instrument\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

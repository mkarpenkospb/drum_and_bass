{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "train_file = \"decode_patterns/train.tsv\" # обучающая выборка\n",
    "validation_file = \"decode_patterns/validation.tsv\" # валидационная выборка\n",
    "human_file = \"decode_patterns/human.tsv\" # как валидационная, только для ассесмента людей (read as \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            patterns_file=train_file,\n",
    "                                                            mono=False)\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can shuffle train and test set like this:\n",
    "drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "\n",
    "# selecting a validation set\n",
    "drum_validation, bass_validation = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                                                  patterns_file=validation_file,\n",
    "                                                                                  mono=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=96, instrument=27, denominator=4, min_note=45)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self, bass_height, bass_width):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # save data parameters\n",
    "        self.bass_height = bass_height\n",
    "        self.bass_width = bass_width\n",
    "        self.bass_size = bass_height*bass_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        self.embedding_size = 2 # размер латентного пространства\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 4\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_preembed_layer = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.lstm_embed_layer_mean = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        self.lstm_embed_layer_logvar = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        \n",
    "        self.decoder_layer1 = nn.Linear(self.embedding_size + self.condition_size, 48)\n",
    "        self.decoder_layer2 = nn.Linear(48 + self.condition_size, 512)\n",
    "        self.decoder_layer3 = nn.Linear(512, self.bass_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_preembed_layer(output))\n",
    "        # избавляемся от лишней размерности (embedding_size=1), чтобы получить вектор из lstm\n",
    "        # размером с высоту изображения\n",
    "        output = output.squeeze().transpose(0,1)\n",
    "        mean = self.sigm(self.lstm_embed_layer_mean(output))\n",
    "        logvar = self.sigm(self.lstm_embed_layer_logvar(output))\n",
    "        return mean, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, input, cond):\n",
    "        output = torch.cat((input, cond), axis=1) # добавляем conditioning\n",
    "        output = self.sigm(self.decoder_layer1(output))\n",
    "        output = torch.cat((output, cond), axis=1) # добавляем ещё conditioning\n",
    "        output = self.sigm(self.decoder_layer2(output))\n",
    "        output = self.sigm(self.decoder_layer3(output))\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_conditionings(input):\n",
    "        return torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        images = self.get_images(input)\n",
    "        conditionings = self.get_conditionings(input)\n",
    "        \n",
    "        mean, logvar = self.encoder(images)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        # добавляем conditioning\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.bass_height, self.bass_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN(data_height, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "def reconstruction_loss(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 26334.5371094\n",
      "[1,     6] train loss: 12954.3878581\n",
      "[1,    11] train loss: 4990.1977539\n",
      "[1,    16] train loss: 2416.7472738\n",
      "[1,    21] train loss: 1819.0951131\n",
      "[1,    26] train loss: 1681.5126139\n",
      "[1,    31] train loss: 1522.2734578\n",
      "[1,    36] train loss: 1486.1710612\n",
      "[1,    41] train loss: 1785.8728027\n",
      "[1,    46] train loss: 1676.9093424\n",
      "[1,    51] train loss: 1513.7425537\n",
      "[1,    56] train loss: 1535.5562744\n",
      "[1,    61] train loss: 1514.4538167\n",
      "[1,    66] train loss: 1462.1471151\n",
      "[1,    71] train loss: 1535.1810303\n",
      "[1,    76] train loss: 1486.0676473\n",
      "[1,    81] train loss: 1490.1380005\n",
      "[1,    86] train loss: 1487.6771240\n",
      "[1,    91] train loss: 1500.0312093\n",
      "[1,    96] train loss: 1501.1992188\n",
      "[1,   101] train loss: 1475.2120361\n",
      "[1,   106] train loss: 1348.2330526\n",
      "[1,   111] train loss: 1473.6702881\n",
      "[1,   116] train loss: 1484.6652018\n",
      "[1,   121] train loss: 1656.6858927\n",
      "[1,   126] train loss: 1405.2963664\n",
      "[1,   131] train loss: 1434.3000895\n",
      "[1,   136] train loss: 1602.7255452\n",
      "[1,   141] train loss: 1494.1089071\n",
      "[1,   146] train loss: 1392.8754069\n",
      "[1,   151] train loss: 1444.1026611\n",
      "[1,   156] train loss: 1433.3139445\n",
      "[1,   161] train loss: 1433.9879557\n",
      "[1,   166] train loss: 1410.6053467\n",
      "[1,   171] train loss: 1527.4699504\n",
      "[1,   176] train loss: 1601.3773804\n",
      "[1,   181] train loss: 1514.0951538\n",
      "[1,   186] train loss: 1489.6349691\n",
      "[1,   191] train loss: 1474.8002930\n",
      "[1,   196] train loss: 1418.8577271\n",
      "[1,   201] train loss: 1433.1722819\n",
      "[1,   206] train loss: 1483.5591838\n",
      "[1,   211] train loss: 1595.4515991\n",
      "[1,   216] train loss: 1369.8672078\n",
      "[1,   221] train loss: 1590.7568970\n",
      "[1,   226] train loss: 1479.3802490\n",
      "[1,   231] train loss: 1559.2959798\n",
      "[1,   236] train loss: 1527.2254639\n",
      "[1,   241] train loss: 1492.5306396\n",
      "[1,   246] train loss: 1504.0011597\n",
      "[1,   251] train loss: 1534.9096273\n",
      "[1,   256] train loss: 1501.0949300\n",
      "[1,   261] train loss: 1440.9446208\n",
      "[1,   266] train loss: 1470.9011230\n",
      "[1,   271] train loss: 1487.7423910\n",
      "[1,   276] train loss: 1512.0147298\n",
      "[1,   281] train loss: 1330.7336019\n",
      "[1,   286] train loss: 1516.0413208\n",
      "[1,   291] train loss: 1592.7452799\n",
      "[1,   296] train loss: 1475.6148275\n",
      "[1,   301] train loss: 1429.5504964\n",
      "[1,   306] train loss: 1379.0261637\n",
      "[1,   311] train loss: 1554.1641032\n",
      "[1,   316] train loss: 1570.2942708\n",
      "[1,   321] train loss: 1433.3845622\n",
      "[1,   326] train loss: 1549.8485107\n",
      "[1,   331] train loss: 1471.7822266\n",
      "[1,   336] train loss: 1575.9248657\n",
      "[1,   341] train loss: 1548.6158447\n",
      "[1,   346] train loss: 1404.8665365\n",
      "[1,   351] train loss: 1469.0496012\n",
      "[1,   356] train loss: 1392.0640666\n",
      "[1,   361] train loss: 1347.4740397\n",
      "[1,   366] train loss: 1554.7012126\n",
      "[1,   371] train loss: 1423.3617350\n",
      "[1,   376] train loss: 1655.4585368\n",
      "[1,   381] train loss: 1439.0831095\n",
      "[1,   386] train loss: 1605.5840251\n",
      "[1,   391] train loss: 1427.6301270\n",
      "[1,   396] train loss: 1613.7432861\n",
      "[1,   401] train loss: 1627.0312907\n",
      "[1,   406] train loss: 1398.6148885\n",
      "[1,   411] train loss: 1331.7841797\n",
      "[1,   416] train loss: 1427.5515544\n",
      "[1,   421] train loss: 1335.9230143\n",
      "[1,   426] train loss: 1622.7557576\n",
      "[1,   431] train loss: 1389.1551514\n",
      "[1,   436] train loss: 1421.7223104\n",
      "[1,   441] train loss: 1497.6086019\n",
      "[1,   446] train loss: 1468.4320272\n",
      "[1,   451] train loss: 1486.2868449\n",
      "[1,   456] train loss: 1508.6342977\n",
      "[1,   461] train loss: 1558.9867961\n",
      "[1,   466] train loss: 1435.2592367\n",
      "[1,   471] train loss: 1562.9261678\n",
      "[1,   476] train loss: 1450.9680583\n",
      "[1,   481] train loss: 1469.7768758\n",
      "[1,   486] train loss: 1471.3502604\n",
      "[1,   491] train loss: 1494.0095011\n",
      "[1,   496] train loss: 1432.4378459\n",
      "[1,   500] train loss: 1474.1422852\n",
      "#1 reconstruction test loss: 111.13843536376953\n",
      "Epoch #1\n",
      "[2,     1] train loss: 1791.1052246\n",
      "[2,     6] train loss: 1401.1848145\n",
      "[2,    11] train loss: 1415.4805908\n",
      "[2,    16] train loss: 1457.7586873\n",
      "[2,    21] train loss: 1474.6716105\n",
      "[2,    26] train loss: 1419.3128866\n",
      "[2,    31] train loss: 1550.7084961\n",
      "[2,    36] train loss: 1511.7175293\n",
      "[2,    41] train loss: 1391.5943400\n",
      "[2,    46] train loss: 1480.0581868\n",
      "[2,    51] train loss: 1638.6820475\n",
      "[2,    56] train loss: 1501.8569132\n",
      "[2,    61] train loss: 1509.4578654\n",
      "[2,    66] train loss: 1340.1553955\n",
      "[2,    71] train loss: 1598.9028524\n",
      "[2,    76] train loss: 1430.1897786\n",
      "[2,    81] train loss: 1496.8135783\n",
      "[2,    86] train loss: 1536.6453451\n",
      "[2,    91] train loss: 1498.9783936\n",
      "[2,    96] train loss: 1558.6049194\n",
      "[2,   101] train loss: 1442.1765340\n",
      "[2,   106] train loss: 1472.7768555\n",
      "[2,   111] train loss: 1584.4612020\n",
      "[2,   116] train loss: 1489.5909424\n",
      "[2,   121] train loss: 1503.3837687\n",
      "[2,   126] train loss: 1394.7438151\n",
      "[2,   131] train loss: 1545.6440633\n",
      "[2,   136] train loss: 1529.7944743\n",
      "[2,   141] train loss: 1604.6555379\n",
      "[2,   146] train loss: 1433.5372721\n",
      "[2,   151] train loss: 1524.3492025\n",
      "[2,   156] train loss: 1501.8570760\n",
      "[2,   161] train loss: 1440.1993001\n",
      "[2,   166] train loss: 1435.3614299\n",
      "[2,   171] train loss: 1477.1003215\n",
      "[2,   176] train loss: 1447.9671224\n",
      "[2,   181] train loss: 1462.3716838\n",
      "[2,   186] train loss: 1469.4206136\n",
      "[2,   191] train loss: 1506.1743368\n",
      "[2,   196] train loss: 1514.1553141\n",
      "[2,   201] train loss: 1547.4923910\n",
      "[2,   206] train loss: 1502.9769694\n",
      "[2,   211] train loss: 1457.9477743\n",
      "[2,   216] train loss: 1552.6855876\n",
      "[2,   221] train loss: 1557.2667643\n",
      "[2,   226] train loss: 1356.3473918\n",
      "[2,   231] train loss: 1480.1373698\n",
      "[2,   236] train loss: 1389.0904338\n",
      "[2,   241] train loss: 1330.7947591\n",
      "[2,   246] train loss: 1462.2618205\n",
      "[2,   251] train loss: 1333.3184001\n",
      "[2,   256] train loss: 1524.3136597\n",
      "[2,   261] train loss: 1443.1196493\n",
      "[2,   266] train loss: 1455.8874512\n",
      "[2,   271] train loss: 1362.1008708\n",
      "[2,   276] train loss: 1571.9564819\n",
      "[2,   281] train loss: 1451.6628418\n",
      "[2,   286] train loss: 1498.3707072\n",
      "[2,   291] train loss: 1579.2641195\n",
      "[2,   296] train loss: 1425.4530640\n",
      "[2,   301] train loss: 1435.6131592\n",
      "[2,   306] train loss: 1459.4752808\n",
      "[2,   311] train loss: 1619.2543132\n",
      "[2,   316] train loss: 1494.1794027\n",
      "[2,   321] train loss: 1470.7270711\n",
      "[2,   326] train loss: 1440.0591838\n",
      "[2,   331] train loss: 1434.3506877\n",
      "[2,   336] train loss: 1554.8303833\n",
      "[2,   341] train loss: 1486.1859131\n",
      "[2,   346] train loss: 1341.8947754\n",
      "[2,   351] train loss: 1319.2383830\n",
      "[2,   356] train loss: 1438.5954183\n",
      "[2,   361] train loss: 1434.1761068\n",
      "[2,   366] train loss: 1500.9433594\n",
      "[2,   371] train loss: 1450.0542806\n",
      "[2,   376] train loss: 1471.5688883\n",
      "[2,   381] train loss: 1607.3738607\n",
      "[2,   386] train loss: 1518.2664185\n",
      "[2,   391] train loss: 1399.1984253\n",
      "[2,   396] train loss: 1432.5529378\n",
      "[2,   401] train loss: 1519.0370687\n",
      "[2,   406] train loss: 1476.4999797\n",
      "[2,   411] train loss: 1416.5877686\n",
      "[2,   416] train loss: 1422.4142253\n",
      "[2,   421] train loss: 1403.1638997\n",
      "[2,   426] train loss: 1507.6075643\n",
      "[2,   431] train loss: 1517.6510824\n",
      "[2,   436] train loss: 1480.8767090\n",
      "[2,   441] train loss: 1452.7326660\n",
      "[2,   446] train loss: 1458.7650553\n",
      "[2,   451] train loss: 1529.6842855\n",
      "[2,   456] train loss: 1545.5727743\n",
      "[2,   461] train loss: 1509.7351074\n",
      "[2,   466] train loss: 1411.8928833\n",
      "[2,   471] train loss: 1414.0546468\n",
      "[2,   476] train loss: 1541.7017822\n",
      "[2,   481] train loss: 1425.9015706\n",
      "[2,   486] train loss: 1484.9224243\n",
      "[2,   491] train loss: 1604.4782104\n",
      "[2,   496] train loss: 1513.0502319\n",
      "[2,   500] train loss: 1440.9925537\n",
      "#2 reconstruction test loss: 113.42606353759766\n",
      "Epoch #2\n",
      "[3,     1] train loss: 1688.0136719\n",
      "[3,     6] train loss: 1442.5607910\n",
      "[3,    11] train loss: 1484.9599813\n",
      "[3,    16] train loss: 1468.3679606\n",
      "[3,    21] train loss: 1517.9899089\n",
      "[3,    26] train loss: 1467.0112508\n",
      "[3,    31] train loss: 1441.7333984\n",
      "[3,    36] train loss: 1696.3757731\n",
      "[3,    41] train loss: 1453.0852254\n",
      "[3,    46] train loss: 1470.4689331\n",
      "[3,    51] train loss: 1463.0769246\n",
      "[3,    56] train loss: 1508.9123535\n",
      "[3,    61] train loss: 1485.2078247\n",
      "[3,    66] train loss: 1492.8252563\n",
      "[3,    71] train loss: 1490.7020060\n",
      "[3,    76] train loss: 1495.8306071\n",
      "[3,    81] train loss: 1559.1457520\n",
      "[3,    86] train loss: 1523.9399007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b4afccc7973d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreconstruction_KL_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 100\n",
    "batch_size = 16\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_lstm(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "    with torch.no_grad():\n",
    "        batch_bass_test_raw = torch.tensor(list(map(lambda p: p.image, bass_test)), dtype=torch.float)\n",
    "        bass_outputs, _, _ = dnb_lstm(drum_test)\n",
    "        \n",
    "        test_count = len(drum_test)\n",
    "        test_loss = 0\n",
    "        for k in range(test_count):\n",
    "            test_loss += reconstruction_loss(bass_outputs[k], batch_bass_test_raw[k])\n",
    "        print(f\"#{epoch + 1} reconstruction test loss: {test_loss/test_count}\")\n",
    "    \n",
    "\n",
    "#should check accuracy on validation set\n",
    "with torch.no_grad():\n",
    "    batch_bass_validation_raw = torch.tensor(list(map(lambda p: p.image, bass_validation)), dtype=torch.float)\n",
    "    bass_outputs, _, _ = dnb_lstm(drum_test)\n",
    "\n",
    "    validation_count = len(drum_test)\n",
    "    validation_loss = 0\n",
    "    for k in range(validation_count):\n",
    "        validation_loss += reconstruction_loss(bass_outputs[k], batch_bass_validation_raw[k])\n",
    "    print(f\"#{epoch + 1} reconstruction validation loss: {validation_loss/validation_count}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]\n",
    "y = mu[:,1]\n",
    "z = dev\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*20, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "def output_midi(batch_drum, batch_bass, folder):\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_lstm(batch_drum)[0]\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "\n",
    "        for i in range(len(batch_drum)):\n",
    "\n",
    "            img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "            numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                    , batch_drum[i].tempo\n",
    "                                    , batch_drum[i].instrument\n",
    "                                    , 1\n",
    "                                    , batch_drum[i].min_note)\n",
    "            pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "            mid = build_track(pair, tempo=pair.tempo)\n",
    "            mid.save(f\"{folder}/sample{i+1}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим обучающую и валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # если очень надо послушать тренировчную -- лучше её перезагрузить, потому что она перемешивается\n",
    "# drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "#                                                             patterns_file=train_file,\n",
    "#                                                             mono=False)\n",
    "# output_midi(drum_train + drum_test, bass_train + bass_test, \"midi/vae_lstm_fcnn/train\")\n",
    "output_midi(drum_validation, bass_validation, \"midi/vae_lstm_fcnn/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вкусу, выводим тот же результат для кожанных мешков на ассесмент. На самом деле ничем от валидационной выборки не отличается :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hum, bass_hum = data_conversion.make_lstm_dataset_conditioning(height=data_height, patterns_file=human_file, mono=False)\n",
    "output_midi(drum_hum, bass_hum, \"midi/vae_lstm_fcnn/human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 3\n",
    "sample2_id = 49\n",
    "sample1 = drum_validation[sample1_id]\n",
    "sample2 = drum_validation[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images([sample1, sample2]))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_lstm.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , instrument\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/vae_lstm_fcnn/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

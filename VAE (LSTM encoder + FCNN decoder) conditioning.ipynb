{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "train_file = \"decode_patterns/train.tsv\" # обучающая выборка\n",
    "validation_file = \"decode_patterns/validation.tsv\" # валидационная выборка\n",
    "human_file = \"decode_patterns/human.tsv\" # как валидационная, только для ассесмента людей (read as \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            patterns_file=train_file,\n",
    "                                                            mono=False)\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can shuffle train and test set like this:\n",
    "drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "\n",
    "# selecting a validation set\n",
    "drum_validation, bass_validation = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                                                  patterns_file=validation_file,\n",
    "                                                                                  mono=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=96, instrument=27, denominator=4, min_note=45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self, bass_height, bass_width):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # save data parameters\n",
    "        self.bass_height = bass_height\n",
    "        self.bass_width = bass_width\n",
    "        self.bass_size = bass_height*bass_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        self.embedding_size = 4 # размер латентного пространства\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 4\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_preembed_layer = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.lstm_embed_layer_mean = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        self.lstm_embed_layer_logvar = nn.Linear(self.bass_height, self.embedding_size)\n",
    "        \n",
    "        self.decoder_layer1 = nn.Linear(self.embedding_size + self.condition_size, 48)\n",
    "        self.decoder_layer2 = nn.Linear(48 + self.condition_size, 512)\n",
    "        self.decoder_layer3 = nn.Linear(512, self.bass_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_preembed_layer(output))\n",
    "        # избавляемся от лишней размерности (embedding_size=1), чтобы получить вектор из lstm\n",
    "        # размером с высоту изображения\n",
    "        output = output.squeeze().transpose(0,1)\n",
    "        mean = self.sigm(self.lstm_embed_layer_mean(output))\n",
    "        logvar = self.sigm(self.lstm_embed_layer_logvar(output))\n",
    "        return mean, logvar\n",
    "    \n",
    "    # reference:\n",
    "    # https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    # end reference\n",
    "    \n",
    "    def decoder(self, input, cond):\n",
    "        output = torch.cat((input, cond), axis=1) # добавляем conditioning\n",
    "        output = self.sigm(self.decoder_layer1(output))\n",
    "        output = torch.cat((output, cond), axis=1) # добавляем ещё conditioning\n",
    "        output = self.sigm(self.decoder_layer2(output))\n",
    "        output = self.sigm(self.decoder_layer3(output))\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_images(input):\n",
    "        return torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_conditionings(input):\n",
    "        return torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        images = self.get_images(input)\n",
    "        conditionings = self.get_conditionings(input)\n",
    "        \n",
    "        mean, logvar = self.encoder(images)\n",
    "        # генерируем случайную точку в латентном пространстве\n",
    "        result = self.reparameterize(mean, logvar)\n",
    "        # добавляем conditioning\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.bass_height, self.bass_width)), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN(data_height, melody_width)\n",
    "\n",
    "# criterion = nn.MSELoss() # -- с этим всё работает (точнее, работало)\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "\n",
    "# на самом деле, попробуем функцию потерь взять из VAE\n",
    "\n",
    "def reconstruction_loss(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "# Reference: https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def reconstruction_KL_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.forward([drum_validation[16], drum_validation[14], drum_validation[43]])[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 105804.2578125\n",
      "[1,     6] train loss: 52616.0625000\n",
      "[1,    11] train loss: 19987.8354492\n",
      "[1,    16] train loss: 9612.7210286\n",
      "[1,    21] train loss: 7295.7250977\n",
      "[1,    26] train loss: 6666.2264811\n",
      "[1,    31] train loss: 6315.2217611\n",
      "[1,    36] train loss: 6447.3212891\n",
      "[1,    41] train loss: 6156.3768717\n",
      "[1,    46] train loss: 6332.1488444\n",
      "[1,    51] train loss: 5993.4137370\n",
      "[1,    56] train loss: 6313.2171224\n",
      "[1,    61] train loss: 5791.3854980\n",
      "[1,    66] train loss: 5873.3064779\n",
      "[1,    71] train loss: 5633.0908203\n",
      "[1,    76] train loss: 5906.2965495\n",
      "[1,    81] train loss: 5823.3674316\n",
      "[1,    86] train loss: 5739.2776693\n",
      "[1,    91] train loss: 6144.3256836\n",
      "[1,    96] train loss: 6006.3346354\n",
      "[1,   101] train loss: 6118.2699382\n",
      "[1,   106] train loss: 5775.7169596\n",
      "[1,   111] train loss: 5739.2664388\n",
      "[1,   116] train loss: 5896.8026530\n",
      "[1,   121] train loss: 5795.9636230\n",
      "[1,   126] train loss: 6008.3368327\n",
      "[1,   131] train loss: 5963.4971517\n",
      "[1,   136] train loss: 5946.7865397\n",
      "[1,   141] train loss: 5761.5646973\n",
      "[1,   146] train loss: 5763.6054688\n",
      "[1,   151] train loss: 5894.8271484\n",
      "[1,   156] train loss: 5771.7027995\n",
      "[1,   161] train loss: 5958.5048828\n",
      "[1,   166] train loss: 5963.8185221\n",
      "[1,   171] train loss: 5864.9312337\n",
      "[1,   176] train loss: 5819.4897461\n",
      "[1,   181] train loss: 5821.7887370\n",
      "[1,   186] train loss: 6175.3680827\n",
      "[1,   191] train loss: 5979.2219238\n",
      "[1,   196] train loss: 5864.6519368\n",
      "[1,   201] train loss: 5841.7985840\n",
      "[1,   206] train loss: 5730.7201335\n",
      "[1,   211] train loss: 5623.4488932\n",
      "[1,   216] train loss: 5941.0449219\n",
      "[1,   221] train loss: 5912.3224284\n",
      "[1,   226] train loss: 6087.3613281\n",
      "[1,   231] train loss: 5840.2353516\n",
      "[1,   236] train loss: 5953.8293457\n",
      "[1,   241] train loss: 5866.8293457\n",
      "[1,   246] train loss: 5791.4338379\n",
      "[1,   251] train loss: 5923.9441732\n",
      "[1,   256] train loss: 5873.3862305\n",
      "[1,   261] train loss: 5972.1180013\n",
      "[1,   266] train loss: 5850.7896322\n",
      "[1,   271] train loss: 6049.5772298\n",
      "[1,   276] train loss: 6063.4210612\n",
      "[1,   281] train loss: 5868.2506510\n",
      "[1,   286] train loss: 5838.4122721\n",
      "[1,   291] train loss: 5809.2598470\n",
      "[1,   296] train loss: 5881.4780273\n",
      "[1,   301] train loss: 5601.6608073\n",
      "[1,   306] train loss: 6098.8336589\n",
      "[1,   311] train loss: 5994.9745280\n",
      "[1,   316] train loss: 5802.1478678\n",
      "[1,   321] train loss: 5762.3956706\n",
      "[1,   326] train loss: 5830.8358561\n",
      "[1,   331] train loss: 5846.0707194\n",
      "[1,   336] train loss: 5761.7727051\n",
      "[1,   341] train loss: 5745.1037598\n",
      "[1,   346] train loss: 6074.1562500\n",
      "[1,   351] train loss: 5774.1271973\n",
      "[1,   356] train loss: 5830.7692871\n",
      "[1,   361] train loss: 6122.6872559\n",
      "[1,   366] train loss: 5952.1527507\n",
      "[1,   371] train loss: 5923.1340332\n",
      "[1,   376] train loss: 5738.8074544\n",
      "[1,   381] train loss: 5700.2626953\n",
      "[1,   386] train loss: 5935.9489746\n",
      "[1,   391] train loss: 5799.9977214\n",
      "[1,   396] train loss: 6150.8323568\n",
      "[1,   401] train loss: 5882.0742188\n",
      "[1,   406] train loss: 5825.8630371\n",
      "[1,   411] train loss: 5802.9098307\n",
      "[1,   416] train loss: 6044.8491211\n",
      "[1,   421] train loss: 5951.9469401\n",
      "[1,   426] train loss: 5688.5758464\n",
      "[1,   431] train loss: 5874.1747233\n",
      "[1,   436] train loss: 5711.2315267\n",
      "[1,   441] train loss: 5894.9571126\n",
      "[1,   446] train loss: 5747.4160156\n",
      "[1,   451] train loss: 5797.0925293\n",
      "[1,   456] train loss: 5669.9960938\n",
      "[1,   461] train loss: 5623.2885742\n",
      "[1,   466] train loss: 5837.9421387\n",
      "[1,   471] train loss: 5908.1423340\n",
      "[1,   476] train loss: 6089.3579915\n",
      "[1,   481] train loss: 5630.5482585\n",
      "[1,   486] train loss: 5659.0495605\n",
      "[1,   491] train loss: 5851.9778646\n",
      "[1,   496] train loss: 5998.6695150\n",
      "[1,   501] train loss: 5717.5603027\n",
      "[1,   506] train loss: 5897.8250326\n",
      "[1,   511] train loss: 6144.3961589\n",
      "[1,   516] train loss: 5675.4972331\n",
      "[1,   521] train loss: 5962.0361328\n",
      "[1,   526] train loss: 5896.7315267\n",
      "[1,   531] train loss: 5610.6120605\n",
      "[1,   536] train loss: 5871.7491862\n",
      "[1,   541] train loss: 5764.6385905\n",
      "[1,   546] train loss: 5836.0584310\n",
      "[1,   551] train loss: 5924.6082357\n",
      "[1,   556] train loss: 5739.3148600\n",
      "[1,   561] train loss: 5869.7687174\n",
      "[1,   566] train loss: 5677.1534017\n",
      "[1,   571] train loss: 5874.3795573\n",
      "[1,   576] train loss: 5774.5201009\n",
      "[1,   581] train loss: 5900.7797852\n",
      "[1,   586] train loss: 5682.7925618\n",
      "[1,   591] train loss: 5641.5849609\n",
      "[1,   596] train loss: 5837.2673340\n",
      "[1,   601] train loss: 6184.8958333\n",
      "[1,   606] train loss: 5829.8870443\n",
      "[1,   611] train loss: 5767.5576172\n",
      "[1,   616] train loss: 6030.1190592\n",
      "[1,   621] train loss: 6123.7716471\n",
      "[1,   625] train loss: 5779.0167969\n",
      "#1 reconstruction test loss: 109.22672271728516\n",
      "#1 reconstruction validation loss: 110.3812255859375\n",
      "Epoch #1\n",
      "[2,     1] train loss: 6814.8491211\n",
      "[2,     6] train loss: 5554.0087891\n",
      "[2,    11] train loss: 5944.0288900\n",
      "[2,    16] train loss: 5819.7452799\n",
      "[2,    21] train loss: 5897.0247396\n",
      "[2,    26] train loss: 5581.8951009\n",
      "[2,    31] train loss: 6044.2934570\n",
      "[2,    36] train loss: 5633.7081706\n",
      "[2,    41] train loss: 5899.8937988\n",
      "[2,    46] train loss: 5885.2102051\n",
      "[2,    51] train loss: 6048.5451660\n",
      "[2,    56] train loss: 5790.4473470\n",
      "[2,    61] train loss: 5906.9649251\n",
      "[2,    66] train loss: 6231.6835938\n",
      "[2,    71] train loss: 5740.1993815\n",
      "[2,    76] train loss: 5894.6216634\n",
      "[2,    81] train loss: 5701.9531250\n",
      "[2,    86] train loss: 5787.0510254\n",
      "[2,    91] train loss: 5942.6603190\n",
      "[2,    96] train loss: 6054.0768229\n",
      "[2,   101] train loss: 5690.5158691\n",
      "[2,   106] train loss: 5792.3549805\n",
      "[2,   111] train loss: 5632.0049642\n",
      "[2,   116] train loss: 5915.3519694\n",
      "[2,   121] train loss: 6027.7492676\n",
      "[2,   126] train loss: 5987.3038737\n",
      "[2,   131] train loss: 5863.0380859\n",
      "[2,   136] train loss: 5852.2313639\n",
      "[2,   141] train loss: 5971.3737793\n",
      "[2,   146] train loss: 6044.3364258\n",
      "[2,   151] train loss: 5973.2731934\n",
      "[2,   156] train loss: 5690.6303711\n",
      "[2,   161] train loss: 5855.5124512\n",
      "[2,   166] train loss: 6040.4026693\n",
      "[2,   171] train loss: 5679.0883789\n",
      "[2,   176] train loss: 6062.6341146\n",
      "[2,   181] train loss: 5904.7149251\n",
      "[2,   186] train loss: 5891.3567708\n",
      "[2,   191] train loss: 5860.9836426\n",
      "[2,   196] train loss: 5620.6451823\n",
      "[2,   201] train loss: 5984.4485677\n",
      "[2,   206] train loss: 5536.3548177\n",
      "[2,   211] train loss: 6225.7288411\n",
      "[2,   216] train loss: 5863.4117025\n",
      "[2,   221] train loss: 5796.9123535\n",
      "[2,   226] train loss: 5898.2990723\n",
      "[2,   231] train loss: 5681.4543457\n",
      "[2,   236] train loss: 5741.6488444\n",
      "[2,   241] train loss: 5736.0142415\n",
      "[2,   246] train loss: 5705.9060059\n",
      "[2,   251] train loss: 5804.9761556\n",
      "[2,   256] train loss: 5818.4558105\n",
      "[2,   261] train loss: 5802.3964030\n",
      "[2,   266] train loss: 5855.5210775\n",
      "[2,   271] train loss: 5681.6866048\n",
      "[2,   276] train loss: 5968.6965332\n",
      "[2,   281] train loss: 5631.5859375\n",
      "[2,   286] train loss: 6056.5619303\n",
      "[2,   291] train loss: 5980.0934245\n",
      "[2,   296] train loss: 5624.6016439\n",
      "[2,   301] train loss: 5766.4383952\n",
      "[2,   306] train loss: 5728.4896647\n",
      "[2,   311] train loss: 5791.7447917\n",
      "[2,   316] train loss: 5753.2617188\n",
      "[2,   321] train loss: 6123.1884766\n",
      "[2,   326] train loss: 5817.3013509\n",
      "[2,   331] train loss: 5564.2238770\n",
      "[2,   336] train loss: 6073.5096029\n",
      "[2,   341] train loss: 5970.7705892\n",
      "[2,   346] train loss: 5777.9675293\n",
      "[2,   351] train loss: 5751.1737467\n",
      "[2,   356] train loss: 6009.3107910\n",
      "[2,   361] train loss: 5843.7598470\n",
      "[2,   366] train loss: 5900.7121582\n",
      "[2,   371] train loss: 5979.8520508\n",
      "[2,   376] train loss: 5831.3621419\n",
      "[2,   381] train loss: 5708.7133789\n",
      "[2,   386] train loss: 5749.6983236\n",
      "[2,   391] train loss: 5670.5070801\n",
      "[2,   396] train loss: 6076.8994954\n",
      "[2,   401] train loss: 5798.1490885\n",
      "[2,   406] train loss: 5768.0481771\n",
      "[2,   411] train loss: 5659.4339193\n",
      "[2,   416] train loss: 5866.3016764\n",
      "[2,   421] train loss: 5876.8773600\n",
      "[2,   426] train loss: 5698.3705241\n",
      "[2,   431] train loss: 5802.3858236\n",
      "[2,   436] train loss: 5777.0755208\n",
      "[2,   441] train loss: 5873.8407389\n",
      "[2,   446] train loss: 5951.2954102\n",
      "[2,   451] train loss: 5497.8439941\n",
      "[2,   456] train loss: 5852.4320475\n",
      "[2,   461] train loss: 5990.1908366\n",
      "[2,   466] train loss: 6133.1596680\n",
      "[2,   471] train loss: 5748.1321615\n",
      "[2,   476] train loss: 5799.5917969\n",
      "[2,   481] train loss: 5612.1278483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   486] train loss: 5634.8815918\n",
      "[2,   491] train loss: 5629.1914876\n",
      "[2,   496] train loss: 5943.8009440\n",
      "[2,   501] train loss: 5963.3633626\n",
      "[2,   506] train loss: 5850.4785156\n",
      "[2,   511] train loss: 5818.7370605\n",
      "[2,   516] train loss: 5936.6898600\n",
      "[2,   521] train loss: 6008.4722493\n",
      "[2,   526] train loss: 5801.3629557\n",
      "[2,   531] train loss: 6169.0219727\n",
      "[2,   536] train loss: 5915.4682617\n",
      "[2,   541] train loss: 5867.3656413\n",
      "[2,   546] train loss: 5777.4333496\n",
      "[2,   551] train loss: 5614.3936361\n",
      "[2,   556] train loss: 5782.3419596\n",
      "[2,   561] train loss: 5697.6121419\n",
      "[2,   566] train loss: 5710.6334635\n",
      "[2,   571] train loss: 5749.7242839\n",
      "[2,   576] train loss: 5676.2377930\n",
      "[2,   581] train loss: 5799.5055339\n",
      "[2,   586] train loss: 5946.7989095\n",
      "[2,   591] train loss: 6069.9986165\n",
      "[2,   596] train loss: 5887.3849284\n",
      "[2,   601] train loss: 5826.8323568\n",
      "[2,   606] train loss: 5867.7837728\n",
      "[2,   611] train loss: 5689.9566243\n",
      "[2,   616] train loss: 5693.1032715\n",
      "[2,   621] train loss: 5743.9104004\n",
      "[2,   625] train loss: 5675.3859375\n",
      "#2 reconstruction test loss: 110.38884735107422\n",
      "#2 reconstruction validation loss: 110.18426513671875\n",
      "Epoch #2\n",
      "[3,     1] train loss: 6388.7519531\n",
      "[3,     6] train loss: 5959.2836914\n",
      "[3,    11] train loss: 5934.4174805\n",
      "[3,    16] train loss: 5927.3331706\n",
      "[3,    21] train loss: 5913.8634440\n",
      "[3,    26] train loss: 5725.7575684\n",
      "[3,    31] train loss: 5817.1175130\n",
      "[3,    36] train loss: 5889.9087728\n",
      "[3,    41] train loss: 6107.2137044\n",
      "[3,    46] train loss: 5743.2663574\n",
      "[3,    51] train loss: 5703.0159505\n",
      "[3,    56] train loss: 5843.1098633\n",
      "[3,    61] train loss: 5726.3294271\n",
      "[3,    66] train loss: 5897.4020996\n",
      "[3,    71] train loss: 5881.5410970\n",
      "[3,    76] train loss: 5882.3235677\n",
      "[3,    81] train loss: 5901.4315592\n",
      "[3,    86] train loss: 5436.1249186\n",
      "[3,    91] train loss: 5812.7412109\n",
      "[3,    96] train loss: 5736.0293783\n",
      "[3,   101] train loss: 5852.6350098\n",
      "[3,   106] train loss: 5698.7399089\n",
      "[3,   111] train loss: 6193.3073730\n",
      "[3,   116] train loss: 5980.5655111\n",
      "[3,   121] train loss: 5877.0317383\n",
      "[3,   126] train loss: 5945.4886882\n",
      "[3,   131] train loss: 5806.5456543\n",
      "[3,   136] train loss: 5935.6334635\n",
      "[3,   141] train loss: 6011.2170410\n",
      "[3,   146] train loss: 5708.1678874\n",
      "[3,   151] train loss: 5961.2290039\n",
      "[3,   156] train loss: 5927.2475586\n",
      "[3,   161] train loss: 5707.4190267\n",
      "[3,   166] train loss: 5676.6848958\n",
      "[3,   171] train loss: 5822.5858561\n",
      "[3,   176] train loss: 5901.1097819\n",
      "[3,   181] train loss: 5894.0384928\n",
      "[3,   186] train loss: 5927.6356608\n",
      "[3,   191] train loss: 5684.6247559\n",
      "[3,   196] train loss: 5926.5518392\n",
      "[3,   201] train loss: 5545.3147786\n",
      "[3,   206] train loss: 5773.3138835\n",
      "[3,   211] train loss: 5896.6562500\n",
      "[3,   216] train loss: 5686.9970703\n",
      "[3,   221] train loss: 6079.5760091\n",
      "[3,   226] train loss: 5676.8080241\n",
      "[3,   231] train loss: 5947.6446940\n",
      "[3,   236] train loss: 5658.8979492\n",
      "[3,   241] train loss: 5794.8226725\n",
      "[3,   246] train loss: 5909.3553060\n",
      "[3,   251] train loss: 5761.2463379\n",
      "[3,   256] train loss: 5793.1962077\n",
      "[3,   261] train loss: 5810.8762207\n",
      "[3,   266] train loss: 5977.0674642\n",
      "[3,   271] train loss: 5940.5499674\n",
      "[3,   276] train loss: 5787.1742350\n",
      "[3,   281] train loss: 5616.2604167\n",
      "[3,   286] train loss: 5812.3483887\n",
      "[3,   291] train loss: 5785.4320475\n",
      "[3,   296] train loss: 5747.0690918\n",
      "[3,   301] train loss: 5662.4872233\n",
      "[3,   306] train loss: 5739.8258464\n",
      "[3,   311] train loss: 5871.6538900\n",
      "[3,   316] train loss: 5907.4188639\n",
      "[3,   321] train loss: 5734.6936035\n",
      "[3,   326] train loss: 5787.2145996\n",
      "[3,   331] train loss: 5808.1303711\n",
      "[3,   336] train loss: 6014.6129557\n",
      "[3,   341] train loss: 5797.0515137\n",
      "[3,   346] train loss: 5904.6905924\n",
      "[3,   351] train loss: 5583.9057617\n",
      "[3,   356] train loss: 5813.7426758\n",
      "[3,   361] train loss: 6007.4452311\n",
      "[3,   366] train loss: 5934.2588704\n",
      "[3,   371] train loss: 5777.5057780\n",
      "[3,   376] train loss: 5423.7113444\n",
      "[3,   381] train loss: 5964.2845052\n",
      "[3,   386] train loss: 5982.0122884\n",
      "[3,   391] train loss: 5791.4479167\n",
      "[3,   396] train loss: 5748.1437988\n",
      "[3,   401] train loss: 5827.4000651\n",
      "[3,   406] train loss: 5915.9514974\n",
      "[3,   411] train loss: 5907.9725749\n",
      "[3,   416] train loss: 5739.6386719\n",
      "[3,   421] train loss: 5741.2458496\n",
      "[3,   426] train loss: 5968.3352051\n",
      "[3,   431] train loss: 5894.9485677\n",
      "[3,   436] train loss: 5924.2682292\n",
      "[3,   441] train loss: 5817.9505208\n",
      "[3,   446] train loss: 5809.6809082\n",
      "[3,   451] train loss: 5912.8004557\n",
      "[3,   456] train loss: 5992.0530599\n",
      "[3,   461] train loss: 5838.1259766\n",
      "[3,   466] train loss: 5852.4682617\n",
      "[3,   471] train loss: 6022.1420085\n",
      "[3,   476] train loss: 5791.2001953\n",
      "[3,   481] train loss: 5871.5594076\n",
      "[3,   486] train loss: 5653.5732422\n",
      "[3,   491] train loss: 5945.7356771\n",
      "[3,   496] train loss: 6038.9527995\n",
      "[3,   501] train loss: 5693.0417480\n",
      "[3,   506] train loss: 6046.3692220\n",
      "[3,   511] train loss: 6088.3246257\n",
      "[3,   516] train loss: 5663.3576660\n",
      "[3,   521] train loss: 5645.1854655\n",
      "[3,   526] train loss: 5910.9804688\n",
      "[3,   531] train loss: 5638.3745931\n",
      "[3,   536] train loss: 5797.6286621\n",
      "[3,   541] train loss: 5758.6988932\n",
      "[3,   546] train loss: 5620.0617676\n",
      "[3,   551] train loss: 5973.6057129\n",
      "[3,   556] train loss: 5932.7123210\n",
      "[3,   561] train loss: 5875.4360352\n",
      "[3,   566] train loss: 6040.9149577\n",
      "[3,   571] train loss: 6058.2173665\n",
      "[3,   576] train loss: 5657.6916504\n",
      "[3,   581] train loss: 5859.1340332\n",
      "[3,   586] train loss: 6153.2979329\n",
      "[3,   591] train loss: 5815.4235840\n",
      "[3,   596] train loss: 5704.5128581\n",
      "[3,   601] train loss: 6072.5767415\n",
      "[3,   606] train loss: 6290.4403483\n",
      "[3,   611] train loss: 6025.3832194\n",
      "[3,   616] train loss: 5939.6132812\n",
      "[3,   621] train loss: 5424.1625163\n",
      "[3,   625] train loss: 5371.9477539\n",
      "#3 reconstruction test loss: 109.32561492919922\n",
      "#3 reconstruction validation loss: 110.24165344238281\n",
      "Epoch #3\n",
      "[4,     1] train loss: 6627.4560547\n",
      "[4,     6] train loss: 5869.4251302\n",
      "[4,    11] train loss: 5814.5745443\n",
      "[4,    16] train loss: 5881.7521973\n",
      "[4,    21] train loss: 5895.3112793\n",
      "[4,    26] train loss: 5784.9820150\n",
      "[4,    31] train loss: 6071.6290690\n",
      "[4,    36] train loss: 5695.0492350\n",
      "[4,    41] train loss: 5940.7575684\n",
      "[4,    46] train loss: 5886.1062012\n",
      "[4,    51] train loss: 5818.6367188\n",
      "[4,    56] train loss: 5821.0177409\n",
      "[4,    61] train loss: 5870.4117839\n",
      "[4,    66] train loss: 5876.7173665\n",
      "[4,    71] train loss: 5629.5733236\n",
      "[4,    76] train loss: 5849.6588542\n",
      "[4,    81] train loss: 5688.8047689\n",
      "[4,    86] train loss: 5916.4142253\n",
      "[4,    91] train loss: 5564.1988118\n",
      "[4,    96] train loss: 5666.8713379\n",
      "[4,   101] train loss: 5949.2384440\n",
      "[4,   106] train loss: 5730.3754069\n",
      "[4,   111] train loss: 5858.6363118\n",
      "[4,   116] train loss: 5716.3942871\n",
      "[4,   121] train loss: 5566.8705241\n",
      "[4,   126] train loss: 5709.9825033\n",
      "[4,   131] train loss: 5964.6712240\n",
      "[4,   136] train loss: 5866.6790365\n",
      "[4,   141] train loss: 5807.6856283\n",
      "[4,   146] train loss: 5588.5510254\n",
      "[4,   151] train loss: 5812.8538411\n",
      "[4,   156] train loss: 5951.0982259\n",
      "[4,   161] train loss: 5876.6524251\n",
      "[4,   166] train loss: 5942.7369792\n",
      "[4,   171] train loss: 5643.9022624\n",
      "[4,   176] train loss: 5978.9200033\n",
      "[4,   181] train loss: 5943.5555013\n",
      "[4,   186] train loss: 5914.7165527\n",
      "[4,   191] train loss: 5804.5458171\n",
      "[4,   196] train loss: 5858.1076660\n",
      "[4,   201] train loss: 5685.1147461\n",
      "[4,   206] train loss: 5705.4134115\n",
      "[4,   211] train loss: 5819.6069336\n",
      "[4,   216] train loss: 5798.5673828\n",
      "[4,   221] train loss: 5949.9377441\n",
      "[4,   226] train loss: 5839.0239258\n",
      "[4,   231] train loss: 5986.6273600\n",
      "[4,   236] train loss: 5740.1296387\n",
      "[4,   241] train loss: 5849.7527669\n",
      "[4,   246] train loss: 5551.3336589\n",
      "[4,   251] train loss: 5758.9408366\n",
      "[4,   256] train loss: 5767.0327962\n",
      "[4,   261] train loss: 5623.7394206\n",
      "[4,   266] train loss: 5772.5122884\n",
      "[4,   271] train loss: 5830.7562663\n",
      "[4,   276] train loss: 5786.6535645\n",
      "[4,   281] train loss: 5797.7774251\n",
      "[4,   286] train loss: 5885.9584961\n",
      "[4,   291] train loss: 5509.7101237\n",
      "[4,   296] train loss: 5848.4634603\n",
      "[4,   301] train loss: 5908.5234375\n",
      "[4,   306] train loss: 5994.7262370\n",
      "[4,   311] train loss: 6186.6028646\n",
      "[4,   316] train loss: 6031.1150716\n",
      "[4,   321] train loss: 5609.3808594\n",
      "[4,   326] train loss: 5964.7412923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   331] train loss: 5768.1092936\n",
      "[4,   336] train loss: 5873.2028809\n",
      "[4,   341] train loss: 5789.5212402\n",
      "[4,   346] train loss: 5534.6745605\n",
      "[4,   351] train loss: 5893.7458496\n",
      "[4,   356] train loss: 5853.3947754\n",
      "[4,   361] train loss: 5887.3005371\n",
      "[4,   366] train loss: 6021.0426432\n",
      "[4,   371] train loss: 5600.4338379\n",
      "[4,   376] train loss: 5785.1264648\n",
      "[4,   381] train loss: 5880.2336426\n",
      "[4,   386] train loss: 6015.4890137\n",
      "[4,   391] train loss: 5649.4558105\n",
      "[4,   396] train loss: 5859.4031576\n",
      "[4,   401] train loss: 5759.7727865\n",
      "[4,   406] train loss: 5753.1685384\n",
      "[4,   411] train loss: 5949.0105794\n",
      "[4,   416] train loss: 6102.1461589\n",
      "[4,   421] train loss: 5975.7138672\n",
      "[4,   426] train loss: 5827.0171712\n",
      "[4,   431] train loss: 5845.6149089\n",
      "[4,   436] train loss: 5849.1099447\n",
      "[4,   441] train loss: 5854.3510742\n",
      "[4,   446] train loss: 5996.8532715\n",
      "[4,   451] train loss: 5611.0284831\n",
      "[4,   456] train loss: 5566.6844076\n",
      "[4,   461] train loss: 5792.0371908\n",
      "[4,   466] train loss: 5810.4509277\n",
      "[4,   471] train loss: 5847.4058431\n",
      "[4,   476] train loss: 5763.4492188\n",
      "[4,   481] train loss: 5640.8034668\n",
      "[4,   486] train loss: 5731.8636068\n",
      "[4,   491] train loss: 5972.5048828\n",
      "[4,   496] train loss: 5770.8671875\n",
      "[4,   501] train loss: 5838.8086751\n",
      "[4,   506] train loss: 5988.2023926\n",
      "[4,   511] train loss: 5939.8095703\n",
      "[4,   516] train loss: 5855.2918294\n",
      "[4,   521] train loss: 5879.7573242\n",
      "[4,   526] train loss: 5911.9575195\n",
      "[4,   531] train loss: 5591.3251953\n",
      "[4,   536] train loss: 5709.3748372\n",
      "[4,   541] train loss: 5790.7382812\n",
      "[4,   546] train loss: 5887.6229655\n",
      "[4,   551] train loss: 5531.2600098\n",
      "[4,   556] train loss: 5687.3134766\n",
      "[4,   561] train loss: 5919.5281576\n",
      "[4,   566] train loss: 5967.5209147\n",
      "[4,   571] train loss: 5876.3591309\n",
      "[4,   576] train loss: 5885.7993164\n",
      "[4,   581] train loss: 5757.3566081\n",
      "[4,   586] train loss: 6118.4593913\n",
      "[4,   591] train loss: 5867.8684896\n",
      "[4,   596] train loss: 5571.0201009\n",
      "[4,   601] train loss: 5938.4256185\n",
      "[4,   606] train loss: 5708.3688965\n",
      "[4,   611] train loss: 6134.3305664\n",
      "[4,   616] train loss: 5640.9649251\n",
      "[4,   621] train loss: 5809.0115560\n",
      "[4,   625] train loss: 5558.2829102\n",
      "#4 reconstruction test loss: 109.72957611083984\n",
      "#4 reconstruction validation loss: 109.8712158203125\n",
      "Epoch #4\n",
      "[5,     1] train loss: 7812.1386719\n",
      "[5,     6] train loss: 5808.4319661\n",
      "[5,    11] train loss: 5916.8094076\n",
      "[5,    16] train loss: 5751.2739258\n",
      "[5,    21] train loss: 5509.5910645\n",
      "[5,    26] train loss: 5705.4143066\n",
      "[5,    31] train loss: 5933.1700033\n",
      "[5,    36] train loss: 5628.7325033\n",
      "[5,    41] train loss: 5719.9269206\n",
      "[5,    46] train loss: 5797.7478027\n",
      "[5,    51] train loss: 6021.2313639\n",
      "[5,    56] train loss: 5909.9599609\n",
      "[5,    61] train loss: 5787.7016602\n",
      "[5,    66] train loss: 5769.2270508\n",
      "[5,    71] train loss: 5704.5891113\n",
      "[5,    76] train loss: 5891.1475423\n",
      "[5,    81] train loss: 5597.2680664\n",
      "[5,    86] train loss: 5639.8075358\n",
      "[5,    91] train loss: 5706.0257975\n",
      "[5,    96] train loss: 6231.3614909\n",
      "[5,   101] train loss: 5605.1355794\n",
      "[5,   106] train loss: 5780.8228353\n",
      "[5,   111] train loss: 5864.1133626\n",
      "[5,   116] train loss: 5911.5805664\n",
      "[5,   121] train loss: 5932.3752441\n",
      "[5,   126] train loss: 5853.0245768\n",
      "[5,   131] train loss: 5755.3607585\n",
      "[5,   136] train loss: 5829.6620280\n",
      "[5,   141] train loss: 6097.2438965\n",
      "[5,   146] train loss: 5634.9029134\n",
      "[5,   151] train loss: 5723.0490723\n",
      "[5,   156] train loss: 5677.4181315\n",
      "[5,   161] train loss: 5914.5175781\n",
      "[5,   166] train loss: 5756.1609701\n",
      "[5,   171] train loss: 5712.2785645\n",
      "[5,   176] train loss: 5582.0623372\n",
      "[5,   181] train loss: 5929.7486165\n",
      "[5,   186] train loss: 5917.0506999\n",
      "[5,   191] train loss: 6043.7794596\n",
      "[5,   196] train loss: 5846.9078776\n",
      "[5,   201] train loss: 5868.9263509\n",
      "[5,   206] train loss: 5652.3973796\n",
      "[5,   211] train loss: 5886.0446777\n",
      "[5,   216] train loss: 5658.9395345\n",
      "[5,   221] train loss: 5934.9341634\n",
      "[5,   226] train loss: 5944.0209961\n",
      "[5,   231] train loss: 5865.0579427\n",
      "[5,   236] train loss: 5918.7278646\n",
      "[5,   241] train loss: 5777.6818034\n",
      "[5,   246] train loss: 5969.7661947\n",
      "[5,   251] train loss: 5474.6027832\n",
      "[5,   256] train loss: 5771.9732259\n",
      "[5,   261] train loss: 5543.5293783\n",
      "[5,   266] train loss: 6284.0146484\n",
      "[5,   271] train loss: 6042.3012695\n",
      "[5,   276] train loss: 5850.6503092\n",
      "[5,   281] train loss: 6017.5113932\n",
      "[5,   286] train loss: 5946.0374349\n",
      "[5,   291] train loss: 5841.5775553\n",
      "[5,   296] train loss: 5912.0174154\n",
      "[5,   301] train loss: 6029.3914388\n",
      "[5,   306] train loss: 5926.1142578\n",
      "[5,   311] train loss: 5776.4321289\n",
      "[5,   316] train loss: 5873.9427897\n",
      "[5,   321] train loss: 5635.4128418\n",
      "[5,   326] train loss: 5904.6028646\n",
      "[5,   331] train loss: 6020.2813314\n",
      "[5,   336] train loss: 5780.1018066\n",
      "[5,   341] train loss: 6001.8848470\n",
      "[5,   346] train loss: 5616.6433919\n",
      "[5,   351] train loss: 5905.6663411\n",
      "[5,   356] train loss: 5705.1874186\n",
      "[5,   361] train loss: 5934.8355306\n",
      "[5,   366] train loss: 6038.4712728\n",
      "[5,   371] train loss: 5955.4001465\n",
      "[5,   376] train loss: 5964.9443359\n",
      "[5,   381] train loss: 6087.5047201\n",
      "[5,   386] train loss: 5747.8700358\n",
      "[5,   391] train loss: 5673.6950684\n",
      "[5,   396] train loss: 5669.1284180\n",
      "[5,   401] train loss: 5962.3309733\n",
      "[5,   406] train loss: 5810.3651530\n",
      "[5,   411] train loss: 5839.6687012\n",
      "[5,   416] train loss: 5547.0812988\n",
      "[5,   421] train loss: 5820.9302572\n",
      "[5,   426] train loss: 5928.8938802\n",
      "[5,   431] train loss: 5744.6335449\n",
      "[5,   436] train loss: 5848.3378906\n",
      "[5,   441] train loss: 5822.2501628\n",
      "[5,   446] train loss: 5919.8183594\n",
      "[5,   451] train loss: 5807.7226562\n",
      "[5,   456] train loss: 5747.0615234\n",
      "[5,   461] train loss: 5737.2957357\n",
      "[5,   466] train loss: 5801.9020182\n",
      "[5,   471] train loss: 5794.4136556\n",
      "[5,   476] train loss: 5809.4602051\n",
      "[5,   481] train loss: 5965.6843262\n",
      "[5,   486] train loss: 5848.0401204\n",
      "[5,   491] train loss: 5818.8636882\n",
      "[5,   496] train loss: 5792.6530762\n",
      "[5,   501] train loss: 5805.7342122\n",
      "[5,   506] train loss: 5829.5115560\n",
      "[5,   511] train loss: 5909.2750651\n",
      "[5,   516] train loss: 6019.8885091\n",
      "[5,   521] train loss: 5685.4718424\n",
      "[5,   526] train loss: 5818.4553223\n",
      "[5,   531] train loss: 5519.0252279\n",
      "[5,   536] train loss: 5759.4769694\n",
      "[5,   541] train loss: 5683.9508464\n",
      "[5,   546] train loss: 5878.7918294\n",
      "[5,   551] train loss: 5766.4768066\n",
      "[5,   556] train loss: 5654.1752930\n",
      "[5,   561] train loss: 5482.9648438\n",
      "[5,   566] train loss: 5899.3734538\n",
      "[5,   571] train loss: 6047.9353841\n",
      "[5,   576] train loss: 5757.6879069\n",
      "[5,   581] train loss: 5643.8697917\n",
      "[5,   586] train loss: 5882.1329753\n",
      "[5,   591] train loss: 5960.1188965\n",
      "[5,   596] train loss: 5899.3378092\n",
      "[5,   601] train loss: 5869.3064779\n",
      "[5,   606] train loss: 5551.4975586\n",
      "[5,   611] train loss: 5835.5109049\n",
      "[5,   616] train loss: 5720.1721191\n",
      "[5,   621] train loss: 5540.6572266\n",
      "[5,   625] train loss: 5639.1880859\n",
      "#5 reconstruction test loss: 108.80508422851562\n",
      "#5 reconstruction validation loss: 109.83125305175781\n",
      "Epoch #5\n",
      "[6,     1] train loss: 7212.1030273\n",
      "[6,     6] train loss: 6009.1424154\n",
      "[6,    11] train loss: 5933.0380046\n",
      "[6,    16] train loss: 5859.4484049\n",
      "[6,    21] train loss: 5733.3438314\n",
      "[6,    26] train loss: 5670.4497070\n",
      "[6,    31] train loss: 5966.7567546\n",
      "[6,    36] train loss: 5695.8858236\n",
      "[6,    41] train loss: 5909.7373047\n",
      "[6,    46] train loss: 5913.7119141\n",
      "[6,    51] train loss: 5841.7149251\n",
      "[6,    56] train loss: 5813.7046712\n",
      "[6,    61] train loss: 5767.5958659\n",
      "[6,    66] train loss: 5758.7423503\n",
      "[6,    71] train loss: 5918.2385254\n",
      "[6,    76] train loss: 5635.4693197\n",
      "[6,    81] train loss: 5561.2934570\n",
      "[6,    86] train loss: 5833.2689616\n",
      "[6,    91] train loss: 6005.2631836\n",
      "[6,    96] train loss: 6029.5073242\n",
      "[6,   101] train loss: 5473.5419108\n",
      "[6,   106] train loss: 5880.1184082\n",
      "[6,   111] train loss: 5956.9015299\n",
      "[6,   116] train loss: 5915.4995931\n",
      "[6,   121] train loss: 5818.9245605\n",
      "[6,   126] train loss: 5727.1386719\n",
      "[6,   131] train loss: 5888.0264486\n",
      "[6,   136] train loss: 5911.3192546\n",
      "[6,   141] train loss: 5770.1745605\n",
      "[6,   146] train loss: 5687.4265951\n",
      "[6,   151] train loss: 5758.4503581\n",
      "[6,   156] train loss: 6004.1645508\n",
      "[6,   161] train loss: 6061.0078125\n",
      "[6,   166] train loss: 5793.8000488\n",
      "[6,   171] train loss: 5682.2112630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,   176] train loss: 5793.0207520\n",
      "[6,   181] train loss: 5731.3545736\n",
      "[6,   186] train loss: 5716.6023763\n",
      "[6,   191] train loss: 5765.0301107\n",
      "[6,   196] train loss: 5781.4115397\n",
      "[6,   201] train loss: 5815.5520020\n",
      "[6,   206] train loss: 6143.3006999\n",
      "[6,   211] train loss: 5749.3210449\n",
      "[6,   216] train loss: 5620.5642904\n",
      "[6,   221] train loss: 5735.5069987\n",
      "[6,   226] train loss: 5611.2613932\n",
      "[6,   231] train loss: 5718.7594401\n",
      "[6,   236] train loss: 5694.2989095\n",
      "[6,   241] train loss: 5886.6367188\n",
      "[6,   246] train loss: 5941.6740723\n",
      "[6,   251] train loss: 5806.9558919\n",
      "[6,   256] train loss: 5824.2992350\n",
      "[6,   261] train loss: 5689.2380371\n",
      "[6,   266] train loss: 5593.1984049\n",
      "[6,   271] train loss: 5801.4515788\n",
      "[6,   276] train loss: 5679.5946452\n",
      "[6,   281] train loss: 5726.9313965\n",
      "[6,   286] train loss: 5749.4733073\n",
      "[6,   291] train loss: 5675.8621419\n",
      "[6,   296] train loss: 5825.6248372\n",
      "[6,   301] train loss: 5728.3258464\n",
      "[6,   306] train loss: 5728.0295410\n",
      "[6,   311] train loss: 5811.4432780\n",
      "[6,   316] train loss: 5799.5394694\n",
      "[6,   321] train loss: 5765.4505208\n",
      "[6,   326] train loss: 5684.4147949\n",
      "[6,   331] train loss: 5866.8792318\n",
      "[6,   336] train loss: 5715.9920247\n",
      "[6,   341] train loss: 5830.1755371\n",
      "[6,   346] train loss: 5793.7619629\n",
      "[6,   351] train loss: 5654.9216309\n",
      "[6,   356] train loss: 5835.4114583\n",
      "[6,   361] train loss: 5667.3916829\n",
      "[6,   366] train loss: 5977.9868164\n",
      "[6,   371] train loss: 6005.3284505\n",
      "[6,   376] train loss: 5941.6027832\n",
      "[6,   381] train loss: 5526.8657227\n",
      "[6,   386] train loss: 5690.2659505\n",
      "[6,   391] train loss: 5809.1958008\n",
      "[6,   396] train loss: 5739.3046875\n",
      "[6,   401] train loss: 5784.1958822\n",
      "[6,   406] train loss: 5927.2840983\n",
      "[6,   411] train loss: 5726.8258464\n",
      "[6,   416] train loss: 5789.9985352\n",
      "[6,   421] train loss: 5794.1311849\n",
      "[6,   426] train loss: 5736.0049642\n",
      "[6,   431] train loss: 5839.6662598\n",
      "[6,   436] train loss: 5827.6424967\n",
      "[6,   441] train loss: 5691.3581543\n",
      "[6,   446] train loss: 5920.9388835\n",
      "[6,   451] train loss: 5675.7539876\n",
      "[6,   456] train loss: 6030.2030436\n",
      "[6,   461] train loss: 5703.0404460\n",
      "[6,   466] train loss: 6132.1848145\n",
      "[6,   471] train loss: 5674.6341146\n",
      "[6,   476] train loss: 6025.2768555\n",
      "[6,   481] train loss: 5847.7350260\n",
      "[6,   486] train loss: 5650.9781901\n",
      "[6,   491] train loss: 5963.4334310\n",
      "[6,   496] train loss: 5730.3002930\n",
      "[6,   501] train loss: 5758.3918457\n",
      "[6,   506] train loss: 5797.8745117\n",
      "[6,   511] train loss: 5860.6281738\n",
      "[6,   516] train loss: 5802.6066081\n",
      "[6,   521] train loss: 5926.0049642\n",
      "[6,   526] train loss: 5663.9538574\n",
      "[6,   531] train loss: 5732.4501953\n",
      "[6,   536] train loss: 5549.3740234\n",
      "[6,   541] train loss: 5680.5582682\n",
      "[6,   546] train loss: 5846.9949544\n",
      "[6,   551] train loss: 6084.8307292\n",
      "[6,   556] train loss: 5832.1392415\n",
      "[6,   561] train loss: 5786.5987956\n",
      "[6,   566] train loss: 5670.8236491\n",
      "[6,   571] train loss: 6016.1943359\n",
      "[6,   576] train loss: 5855.8684082\n",
      "[6,   581] train loss: 5740.8862305\n",
      "[6,   586] train loss: 5950.4509277\n",
      "[6,   591] train loss: 5912.0192871\n",
      "[6,   596] train loss: 5971.7392578\n",
      "[6,   601] train loss: 5784.6854655\n",
      "[6,   606] train loss: 5966.8968913\n",
      "[6,   611] train loss: 5921.5027669\n",
      "[6,   616] train loss: 5747.9400228\n",
      "[6,   621] train loss: 5755.3540039\n",
      "[6,   625] train loss: 5659.5636719\n",
      "#6 reconstruction test loss: 108.96467590332031\n",
      "#6 reconstruction validation loss: 109.58184814453125\n",
      "Epoch #6\n",
      "[7,     1] train loss: 6333.1757812\n",
      "[7,     6] train loss: 5782.3854167\n",
      "[7,    11] train loss: 5611.4531250\n",
      "[7,    16] train loss: 5703.0961100\n",
      "[7,    21] train loss: 5897.1405436\n",
      "[7,    26] train loss: 5740.2747396\n",
      "[7,    31] train loss: 5547.7154948\n",
      "[7,    36] train loss: 5858.7815755\n",
      "[7,    41] train loss: 5585.8582357\n",
      "[7,    46] train loss: 5761.6920573\n",
      "[7,    51] train loss: 5846.8156738\n",
      "[7,    56] train loss: 5860.0439453\n",
      "[7,    61] train loss: 5833.1335449\n",
      "[7,    66] train loss: 5818.1704102\n",
      "[7,    71] train loss: 5726.2530111\n",
      "[7,    76] train loss: 5603.7214355\n",
      "[7,    81] train loss: 5894.8678385\n",
      "[7,    86] train loss: 5838.1232096\n",
      "[7,    91] train loss: 5649.3133952\n",
      "[7,    96] train loss: 5444.8606771\n",
      "[7,   101] train loss: 5658.0364583\n",
      "[7,   106] train loss: 5548.5852865\n",
      "[7,   111] train loss: 5779.4658203\n",
      "[7,   116] train loss: 5883.1534831\n",
      "[7,   121] train loss: 5717.4501139\n",
      "[7,   126] train loss: 5948.1324870\n",
      "[7,   131] train loss: 5943.8251953\n",
      "[7,   136] train loss: 5722.7141113\n",
      "[7,   141] train loss: 5853.7240397\n",
      "[7,   146] train loss: 5749.9966634\n",
      "[7,   151] train loss: 5805.1277669\n",
      "[7,   156] train loss: 5769.0769043\n",
      "[7,   161] train loss: 5946.8729655\n",
      "[7,   166] train loss: 5697.7515462\n",
      "[7,   171] train loss: 5842.1193034\n",
      "[7,   176] train loss: 5935.2487793\n",
      "[7,   181] train loss: 5950.9501139\n",
      "[7,   186] train loss: 6060.1523438\n",
      "[7,   191] train loss: 5938.8306478\n",
      "[7,   196] train loss: 5855.2609863\n",
      "[7,   201] train loss: 5520.5463053\n",
      "[7,   206] train loss: 6130.7783203\n",
      "[7,   211] train loss: 5902.7233073\n",
      "[7,   216] train loss: 5754.9217122\n",
      "[7,   221] train loss: 5686.4567871\n",
      "[7,   226] train loss: 5597.8920898\n",
      "[7,   231] train loss: 5987.0562337\n",
      "[7,   236] train loss: 5742.7696126\n",
      "[7,   241] train loss: 5793.8797201\n",
      "[7,   246] train loss: 5830.7655436\n",
      "[7,   251] train loss: 6052.5735677\n",
      "[7,   256] train loss: 5946.5547689\n",
      "[7,   261] train loss: 5829.8950195\n",
      "[7,   266] train loss: 5696.0764160\n",
      "[7,   271] train loss: 6041.0220540\n",
      "[7,   276] train loss: 5833.6285807\n",
      "[7,   281] train loss: 5908.7624512\n",
      "[7,   286] train loss: 5690.1210938\n",
      "[7,   291] train loss: 5707.6449382\n",
      "[7,   296] train loss: 5778.5904134\n",
      "[7,   301] train loss: 5645.9975586\n",
      "[7,   306] train loss: 5764.2432454\n",
      "[7,   311] train loss: 5983.8410645\n",
      "[7,   316] train loss: 5923.4842122\n",
      "[7,   321] train loss: 5895.8977865\n",
      "[7,   326] train loss: 5934.4790853\n",
      "[7,   331] train loss: 5762.6382650\n",
      "[7,   336] train loss: 5889.6425781\n",
      "[7,   341] train loss: 5744.3251139\n",
      "[7,   346] train loss: 5746.7450358\n",
      "[7,   351] train loss: 5548.8577474\n",
      "[7,   356] train loss: 5820.9243164\n",
      "[7,   361] train loss: 5783.6353353\n",
      "[7,   366] train loss: 5877.4388835\n",
      "[7,   371] train loss: 5786.4322917\n",
      "[7,   376] train loss: 5760.7286784\n",
      "[7,   381] train loss: 5813.6112467\n",
      "[7,   386] train loss: 5536.3512370\n",
      "[7,   391] train loss: 6098.9399414\n",
      "[7,   396] train loss: 5751.8526204\n",
      "[7,   401] train loss: 5578.9077962\n",
      "[7,   406] train loss: 5756.9772949\n",
      "[7,   411] train loss: 5983.2080892\n",
      "[7,   416] train loss: 5702.8680013\n",
      "[7,   421] train loss: 5724.8128255\n",
      "[7,   426] train loss: 6070.6822917\n",
      "[7,   431] train loss: 6005.5847982\n",
      "[7,   436] train loss: 5675.8062337\n",
      "[7,   441] train loss: 5616.0526530\n",
      "[7,   446] train loss: 5830.7295736\n",
      "[7,   451] train loss: 5852.7381185\n",
      "[7,   456] train loss: 5745.3275553\n",
      "[7,   461] train loss: 5615.3552246\n",
      "[7,   466] train loss: 5697.3614909\n",
      "[7,   471] train loss: 6171.1495768\n",
      "[7,   476] train loss: 5885.2792969\n",
      "[7,   481] train loss: 5756.6286621\n",
      "[7,   486] train loss: 5738.8735352\n",
      "[7,   491] train loss: 6134.7592773\n",
      "[7,   496] train loss: 5839.4302572\n",
      "[7,   501] train loss: 5746.5943197\n",
      "[7,   506] train loss: 5739.4312337\n",
      "[7,   511] train loss: 5816.4866536\n",
      "[7,   516] train loss: 5835.3806152\n",
      "[7,   521] train loss: 5772.0537923\n",
      "[7,   526] train loss: 5958.4940592\n",
      "[7,   531] train loss: 5872.3001302\n",
      "[7,   536] train loss: 5812.4055176\n",
      "[7,   541] train loss: 5929.6925456\n",
      "[7,   546] train loss: 5984.3775228\n",
      "[7,   551] train loss: 5825.8653971\n",
      "[7,   556] train loss: 5631.0363770\n",
      "[7,   561] train loss: 5835.5656738\n",
      "[7,   566] train loss: 5840.0825195\n",
      "[7,   571] train loss: 5869.5118001\n",
      "[7,   576] train loss: 5666.6354980\n",
      "[7,   581] train loss: 5911.8227539\n",
      "[7,   586] train loss: 5743.9951986\n",
      "[7,   591] train loss: 5904.7421875\n",
      "[7,   596] train loss: 5791.0458171\n",
      "[7,   601] train loss: 5746.0725098\n",
      "[7,   606] train loss: 5711.0305176\n",
      "[7,   611] train loss: 5621.6288249\n",
      "[7,   616] train loss: 5697.9409180\n",
      "[7,   621] train loss: 5979.3171387\n",
      "[7,   625] train loss: 5411.8228516\n",
      "#7 reconstruction test loss: 108.32728576660156\n",
      "#7 reconstruction validation loss: 109.4085464477539\n",
      "Epoch #7\n",
      "[8,     1] train loss: 6632.2001953\n",
      "[8,     6] train loss: 5942.0999349\n",
      "[8,    11] train loss: 6043.0300293\n",
      "[8,    16] train loss: 5744.9003092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,    21] train loss: 5806.9367676\n",
      "[8,    26] train loss: 5811.2036947\n",
      "[8,    31] train loss: 5526.9556478\n",
      "[8,    36] train loss: 5974.4786784\n",
      "[8,    41] train loss: 5959.0399577\n",
      "[8,    46] train loss: 5794.4942220\n",
      "[8,    51] train loss: 5671.8584798\n",
      "[8,    56] train loss: 6023.2110189\n",
      "[8,    61] train loss: 5795.3387858\n",
      "[8,    66] train loss: 5634.5518392\n",
      "[8,    71] train loss: 6044.4486491\n",
      "[8,    76] train loss: 5996.4563802\n",
      "[8,    81] train loss: 5671.2041829\n",
      "[8,    86] train loss: 5670.3341471\n",
      "[8,    91] train loss: 5722.0139974\n",
      "[8,    96] train loss: 5990.4619141\n",
      "[8,   101] train loss: 5736.4930827\n",
      "[8,   106] train loss: 5987.0069987\n",
      "[8,   111] train loss: 5703.6520182\n",
      "[8,   116] train loss: 5806.9321289\n",
      "[8,   121] train loss: 5772.9077962\n",
      "[8,   126] train loss: 5737.5008952\n",
      "[8,   131] train loss: 5922.9871419\n",
      "[8,   136] train loss: 5582.4762370\n",
      "[8,   141] train loss: 5652.5767415\n",
      "[8,   146] train loss: 5701.0604655\n",
      "[8,   151] train loss: 5835.1996257\n",
      "[8,   156] train loss: 5805.1630046\n",
      "[8,   161] train loss: 5754.7630208\n",
      "[8,   166] train loss: 5899.6079915\n",
      "[8,   171] train loss: 5862.1115723\n",
      "[8,   176] train loss: 5852.6075033\n",
      "[8,   181] train loss: 6034.6347656\n",
      "[8,   186] train loss: 5511.2667643\n",
      "[8,   191] train loss: 5632.0362142\n",
      "[8,   196] train loss: 5855.1474609\n",
      "[8,   201] train loss: 5787.0458171\n",
      "[8,   206] train loss: 5860.8476562\n",
      "[8,   211] train loss: 5626.3777669\n",
      "[8,   216] train loss: 5798.5686849\n",
      "[8,   221] train loss: 5825.5452474\n",
      "[8,   226] train loss: 5622.1658529\n",
      "[8,   231] train loss: 5803.1577962\n",
      "[8,   236] train loss: 5749.4196777\n",
      "[8,   241] train loss: 5836.7487793\n",
      "[8,   246] train loss: 6023.5083822\n",
      "[8,   251] train loss: 5746.0739746\n",
      "[8,   256] train loss: 5754.1426595\n",
      "[8,   261] train loss: 5641.2181803\n",
      "[8,   266] train loss: 6066.4621582\n",
      "[8,   271] train loss: 5747.6755371\n",
      "[8,   276] train loss: 5708.2286784\n",
      "[8,   281] train loss: 5876.0558268\n",
      "[8,   286] train loss: 5582.3757324\n",
      "[8,   291] train loss: 5852.6519368\n",
      "[8,   296] train loss: 5690.9968262\n",
      "[8,   301] train loss: 6048.5043945\n",
      "[8,   306] train loss: 5729.2901204\n",
      "[8,   311] train loss: 5887.8993327\n",
      "[8,   316] train loss: 5891.9047852\n",
      "[8,   321] train loss: 6116.5789388\n",
      "[8,   326] train loss: 5776.4641927\n",
      "[8,   331] train loss: 5729.0543620\n",
      "[8,   336] train loss: 5520.1945801\n",
      "[8,   341] train loss: 5983.2947591\n",
      "[8,   346] train loss: 5728.7220866\n",
      "[8,   351] train loss: 5841.5287272\n",
      "[8,   356] train loss: 5770.5102539\n",
      "[8,   361] train loss: 5719.3795573\n",
      "[8,   366] train loss: 5713.6027832\n",
      "[8,   371] train loss: 5854.9143880\n",
      "[8,   376] train loss: 5912.4654134\n",
      "[8,   381] train loss: 6101.4295247\n",
      "[8,   386] train loss: 5605.1507161\n",
      "[8,   391] train loss: 5875.9977214\n",
      "[8,   396] train loss: 5589.6555990\n",
      "[8,   401] train loss: 5786.2550456\n",
      "[8,   406] train loss: 5650.3000488\n",
      "[8,   411] train loss: 5504.4715983\n",
      "[8,   416] train loss: 5776.8019206\n",
      "[8,   421] train loss: 5995.4140625\n",
      "[8,   426] train loss: 5908.2080892\n",
      "[8,   431] train loss: 5722.6291504\n",
      "[8,   436] train loss: 5666.9213867\n",
      "[8,   441] train loss: 5697.7573242\n",
      "[8,   446] train loss: 5597.3329264\n",
      "[8,   451] train loss: 5783.9965820\n",
      "[8,   456] train loss: 6061.1243490\n",
      "[8,   461] train loss: 5757.8237305\n",
      "[8,   466] train loss: 5778.5777181\n",
      "[8,   471] train loss: 5692.2961426\n",
      "[8,   476] train loss: 6078.8116862\n",
      "[8,   481] train loss: 5897.3691406\n",
      "[8,   486] train loss: 5648.6757812\n",
      "[8,   491] train loss: 5714.0451660\n",
      "[8,   496] train loss: 5798.9930827\n",
      "[8,   501] train loss: 5968.5957031\n",
      "[8,   506] train loss: 5520.9480794\n",
      "[8,   511] train loss: 6004.0585938\n",
      "[8,   516] train loss: 5839.8061523\n",
      "[8,   521] train loss: 5786.5678711\n",
      "[8,   526] train loss: 5634.2212728\n",
      "[8,   531] train loss: 5932.0436198\n",
      "[8,   536] train loss: 5891.9205729\n",
      "[8,   541] train loss: 5659.3343099\n",
      "[8,   546] train loss: 5487.9755046\n",
      "[8,   551] train loss: 5823.1101888\n",
      "[8,   556] train loss: 5782.4140625\n",
      "[8,   561] train loss: 5566.9974772\n",
      "[8,   566] train loss: 5829.2931315\n",
      "[8,   571] train loss: 5659.3806152\n",
      "[8,   576] train loss: 5868.1664225\n",
      "[8,   581] train loss: 5729.1236979\n",
      "[8,   586] train loss: 5605.2897949\n",
      "[8,   591] train loss: 5584.9120280\n",
      "[8,   596] train loss: 5900.7810059\n",
      "[8,   601] train loss: 5773.8327637\n",
      "[8,   606] train loss: 5829.5550130\n",
      "[8,   611] train loss: 5663.9467773\n",
      "[8,   616] train loss: 5721.0748698\n",
      "[8,   621] train loss: 5778.6986491\n",
      "[8,   625] train loss: 5566.4415039\n",
      "#8 reconstruction test loss: 108.6020278930664\n",
      "#8 reconstruction validation loss: 109.28347778320312\n",
      "Epoch #8\n",
      "[9,     1] train loss: 6398.4799805\n",
      "[9,     6] train loss: 5874.2116699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2c80c8f5815e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreconstruction_KL_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-49a0039857f1>\u001b[0m in \u001b[0;36mreconstruction_KL_loss_function\u001b[1;34m(recon_x, x, mu, logvar)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# https://arxiv.org/abs/1312.6114\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mKLD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBCE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mKLD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 72\n",
    "batch_size = 64\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "\n",
    "def calc_loss_on(drum, bass, description):\n",
    "    with torch.no_grad():\n",
    "        batch_bass_raw = torch.tensor(list(map(lambda p: p.image, bass)), dtype=torch.float)\n",
    "        bass_outputs, _, _ = dnb_lstm(drum)\n",
    "\n",
    "        count = len(drum)\n",
    "        loss = 0\n",
    "        for k in range(count):\n",
    "            loss += reconstruction_loss(bass_outputs[k], batch_bass_raw[k])\n",
    "        print(f\"#{epoch + 1} {description}: {loss/count}\")\n",
    "\n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs, mu, stddev = dnb_lstm(batch_drum_train)\n",
    "        # bass_outputs = bass_outputs.squeeze()\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += reconstruction_KL_loss_function(bass_outputs[i], batch_bass_train_raw[i], mu[i], stddev[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    calc_loss_on(drum_test, bass_test, \"reconstruction test loss\")\n",
    "    calc_loss_on(drum_validation, bass_validation, \"reconstruction validation loss\")\n",
    "    torch.save(dnb_lstm.state_dict(), f\"models/vae_lstm_fcnn_{epoch+1}iter\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение текущего состояния нейросети\n",
    "Сохраим веса модели во внешний файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dnb_lstm.state_dict(), \"models/vae_lstm_fcnn_32iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично происходит загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.load_state_dict(torch.load(\"models/vae_lstm_fcnn_32iter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этап эксплуатации нейросети\n",
    "Посмотрим на результаты, что выдаёт нейросеть на выходе..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)\n",
    "result = bass_outputs[0].squeeze().int()\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, более интересно посмотреть на то, что получилось в латентном пространстве... Неплохо было бы визуализировать точки в латентном пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images(drum_train))\n",
    "    \n",
    "mu, dev = latent_train\n",
    "\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# create data\n",
    "x = mu[:,0]\n",
    "y = mu[:,1]\n",
    "z = dev\n",
    " \n",
    "# use the scatter function\n",
    "plt.scatter(x, y, s=z*20, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "def output_midi(batch_drum, batch_bass, folder):\n",
    "    with torch.no_grad():\n",
    "        bass_outputs = dnb_lstm(batch_drum)[0]\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "\n",
    "        for i in range(len(batch_drum)):\n",
    "\n",
    "            img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "            numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                    , batch_drum[i].tempo\n",
    "                                    , batch_drum[i].instrument\n",
    "                                    , 1\n",
    "                                    , batch_drum[i].min_note)\n",
    "            pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "            mid = build_track(pair, tempo=pair.tempo)\n",
    "            mid.save(f\"{folder}/sample{i+1}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим обучающую и валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # если очень надо послушать тренировчную -- лучше её перезагрузить, потому что она перемешивается\n",
    "# drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "#                                                             patterns_file=train_file,\n",
    "#                                                             mono=False)\n",
    "# output_midi(drum, bass, \"midi/vae_lstm_fcnn/train\")\n",
    "output_midi(drum_validation, bass_validation, \"midi/vae_lstm_fcnn/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вкусу, выводим тот же результат для кожанных мешков на ассесмент. На самом деле ничем от валидационной выборки не отличается :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hum, bass_hum = data_conversion.make_lstm_dataset_conditioning(height=data_height, patterns_file=human_file, mono=False)\n",
    "output_midi(drum_hum, bass_hum, \"midi/vae_lstm_fcnn/human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать градиент от двух базовых партий!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "steps = 10 # количество шагов между семплами\n",
    "sample1_id = 3\n",
    "sample2_id = 49\n",
    "sample1 = drum_validation[sample1_id]\n",
    "sample2 = drum_validation[sample2_id]\n",
    "\n",
    "# вычисляем два вектора в латентном пространстве\n",
    "with torch.no_grad():\n",
    "    latent_train = dnb_lstm.encoder(dnb_lstm.get_images([sample1, sample2]))\n",
    "    mu, dev = latent_train\n",
    "\n",
    "    sample1_latent = mu[0]\n",
    "    sample2_latent = mu[1]\n",
    "    \n",
    "    # пробегаемся линейно по латентному пространству\n",
    "    for step in range(steps + 1):\n",
    "        alpha = step / steps\n",
    "        latent_sample = sample1_latent + (sample2_latent - sample1_latent)*alpha\n",
    "        \n",
    "        # пока что выбираем соответствующую барабанную партию в двоичном виде\n",
    "        drum_sample = sample1\n",
    "        if (alpha >= 0.5):\n",
    "            drum_sample = sample2\n",
    "            \n",
    "        # а параметры для кондишнинга -- линейно\n",
    "        tempo = sample1.tempo + (sample2.tempo - sample1.tempo) * alpha\n",
    "        instrument = sample1.instrument + (sample2.instrument - sample1.instrument) * alpha\n",
    "        \n",
    "        # декодируем линейную комбинацию\n",
    "        conditionings = torch.tensor([tempo, instrument]).float()\n",
    "        upsample = dnb_lstm.decoder(latent_sample.unsqueeze(dim=0), conditionings.unsqueeze(dim=0))\n",
    "        upsample =  upsample.view((data_height, melody_width))\n",
    "        upsample = ((upsample.squeeze() + 1) / 2 > 0.55)\n",
    "        \n",
    "        \n",
    "        # сохраняем в файл\n",
    "        img_dnb = np.concatenate((drum_sample.image,upsample), axis=1)\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , tempo\n",
    "                                , instrument\n",
    "                                , 1\n",
    "                                , drum_sample.min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/vae_lstm_fcnn/grad/gradient{step}.mid\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

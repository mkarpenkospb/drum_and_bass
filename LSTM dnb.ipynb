{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM на оригинальном датасете\n",
    "\n",
    "Попробуем сделать модель LSTM, похожую на ту, что описана в соседнем Notebook, но для нашей текущей задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет с помощью DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по аналогии с предыдущим примером:\n",
    "# первая строка -- ожидаемый выходной сигнал, вторая -- входные данные :)\n",
    "# dataset_train = torch.tensor([\n",
    "#     [[1,3,1,4,2,3,3,2], [1,0,0,1,8,4,7,2], [2,4,2,3,2,5,6,3]],\n",
    "#     [[2,2,1,1,4,3,4,2], [2,0,4,2,2,4,2,8], [3,5,3,4,6,1,1,2]],\n",
    "# ], dtype=torch.float)\n",
    "\n",
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset(height=64, limit=1000, patterns_file=\"decode_patterns/patterns.pairs.tsv\")\n",
    "\n",
    "\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    L = len(drum)\n",
    "    idx = np.arange(L) < p*L\n",
    "    np.random.shuffle(idx)\n",
    "    yield A[idx]\n",
    "    yield B[idx]\n",
    "    yield A[np.logical_not(idx)]\n",
    "    yield B[np.logical_not(idx)]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([384, 64, 36]),\n",
       " torch.Size([384, 64, 14]),\n",
       " torch.Size([96, 64, 36]),\n",
       " torch.Size([96, 64, 14]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_train.shape, drum_train.shape, bass_test.shape, drum_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем определить модель LSTM как конечный автомат\n",
    "class DrumNBassLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumNBassLSTM, self).__init__()\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.hidden_size = 36\n",
    "        self.layer_count = 8\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.layer_count)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (64, 32, 14)\n",
    "        # то есть 64 отсчёта, тридцать два примера (минибатч), 14 значение в каждом (барабанная партия)\n",
    "        output, _ = self.lstm(input)\n",
    "        # пробуем превратить это в классификацию (для NLLLoss)\n",
    "#         output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBassLSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 0.0136524\n",
      "[1,     6] train loss: 0.0115293\n",
      "[1,    11] train loss: 0.0128308\n",
      "[1,    16] train loss: 0.0124043\n",
      "[1,    20] train loss: 0.0107828\n",
      "Epoch #1\n",
      "[2,     1] train loss: 0.0133313\n",
      "[2,     6] train loss: 0.0115083\n",
      "[2,    11] train loss: 0.0128082\n",
      "[2,    16] train loss: 0.0119504\n",
      "[2,    20] train loss: 0.0111531\n",
      "Epoch #2\n",
      "[3,     1] train loss: 0.0129065\n",
      "[3,     6] train loss: 0.0113117\n",
      "[3,    11] train loss: 0.0130923\n",
      "[3,    16] train loss: 0.0115865\n",
      "[3,    20] train loss: 0.0108517\n",
      "Epoch #3\n",
      "[4,     1] train loss: 0.0132091\n",
      "[4,     6] train loss: 0.0112073\n",
      "[4,    11] train loss: 0.0135270\n",
      "[4,    16] train loss: 0.0116406\n",
      "[4,    20] train loss: 0.0105689\n",
      "Epoch #4\n",
      "[5,     1] train loss: 0.0134345\n",
      "[5,     6] train loss: 0.0112862\n",
      "[5,    11] train loss: 0.0126832\n",
      "[5,    16] train loss: 0.0117854\n",
      "[5,    20] train loss: 0.0109123\n",
      "Epoch #5\n",
      "[6,     1] train loss: 0.0132339\n",
      "[6,     6] train loss: 0.0112988\n",
      "[6,    11] train loss: 0.0133221\n",
      "[6,    16] train loss: 0.0114451\n",
      "[6,    20] train loss: 0.0103454\n",
      "Epoch #6\n",
      "[7,     1] train loss: 0.0131240\n",
      "[7,     6] train loss: 0.0111529\n",
      "[7,    11] train loss: 0.0123848\n",
      "[7,    16] train loss: 0.0117665\n",
      "[7,    20] train loss: 0.0107278\n",
      "Epoch #7\n",
      "[8,     1] train loss: 0.0129415\n",
      "[8,     6] train loss: 0.0107932\n",
      "[8,    11] train loss: 0.0130731\n",
      "[8,    16] train loss: 0.0113827\n",
      "[8,    20] train loss: 0.0106618\n",
      "Epoch #8\n",
      "[9,     1] train loss: 0.0130877\n",
      "[9,     6] train loss: 0.0108091\n",
      "[9,    11] train loss: 0.0125259\n",
      "[9,    16] train loss: 0.0114951\n",
      "[9,    20] train loss: 0.0105840\n",
      "Epoch #9\n",
      "[10,     1] train loss: 0.0128993\n",
      "[10,     6] train loss: 0.0110629\n",
      "[10,    11] train loss: 0.0126701\n",
      "[10,    16] train loss: 0.0115569\n",
      "[10,    20] train loss: 0.0105723\n",
      "Epoch #10\n",
      "[11,     1] train loss: 0.0124197\n",
      "[11,     6] train loss: 0.0110309\n",
      "[11,    11] train loss: 0.0125160\n",
      "[11,    16] train loss: 0.0110614\n",
      "[11,    20] train loss: 0.0105092\n",
      "Epoch #11\n",
      "[12,     1] train loss: 0.0130418\n",
      "[12,     6] train loss: 0.0111985\n",
      "[12,    11] train loss: 0.0122196\n",
      "[12,    16] train loss: 0.0113924\n",
      "[12,    20] train loss: 0.0105581\n",
      "Epoch #12\n",
      "[13,     1] train loss: 0.0131961\n",
      "[13,     6] train loss: 0.0106176\n",
      "[13,    11] train loss: 0.0120047\n",
      "[13,    16] train loss: 0.0116377\n",
      "[13,    20] train loss: 0.0102797\n",
      "Epoch #13\n",
      "[14,     1] train loss: 0.0128118\n",
      "[14,     6] train loss: 0.0108323\n",
      "[14,    11] train loss: 0.0122881\n",
      "[14,    16] train loss: 0.0110717\n",
      "[14,    20] train loss: 0.0104025\n",
      "Epoch #14\n",
      "[15,     1] train loss: 0.0126723\n",
      "[15,     6] train loss: 0.0107028\n",
      "[15,    11] train loss: 0.0118758\n",
      "[15,    16] train loss: 0.0116319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-43cd8a538663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# loss = criterion(bass_outputs, batch_bass_train.long())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 300\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "    drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "    bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "    drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        drum_train = torch.tensor(drum_train, dtype=torch.float)\n",
    "        bass_train = torch.tensor(bass_train, dtype=torch.float)\n",
    "        drum_test = torch.tensor(drum_test, dtype=torch.float)\n",
    "        bass_test = torch.tensor(bass_test, dtype=torch.float)\n",
    "        \n",
    "    examples_count = drum_train.size()[0]\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size,:,:].transpose(0,1)\n",
    "        # transpose нужен для обмена размерности батча и размерности шагов\n",
    "        # print(f\"i:{i}, batch_drum_train:{batch_drum_train.size()}, batch_bass_train:{batch_bass_train.size()}\")\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train)\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train.size()}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_drum_train = drum_train[:1,:,:].transpose(0,1)\n",
    "batch_bass_train = bass_train[:1,:,:].transpose(0,1)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    bass_outputs = dnb_lstm(batch_drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33.7636, 30.3495, 30.0864, 35.6125, 31.8778, 31.1510, 35.1207, 33.9974,\n",
       "        30.9092, 29.5502, 29.6476, 33.2156, 30.5292, 31.5056, 29.9812, 33.4993,\n",
       "        32.6440, 30.6445, 32.0401, 32.3189, 32.1354, 34.6041, 31.7607, 34.7456,\n",
       "        34.5019, 27.9688, 31.6641, 35.0432, 31.7548, 31.8498, 31.6478, 32.1741,\n",
       "        32.9007, 28.7890, 29.7973, 32.2894])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((bass_outputs.squeeze() + 1) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "patterns_file = \"decode_patterns/patterns.pairs.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            limit=1000,\n",
    "                                                            patterns_file=patterns_file,\n",
    "                                                            mono=False)\n",
    "# print(drum[0])\n",
    "# drum, bass = np.array(drum), np.array(bass)\n",
    "# print(drum[0])\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=480, instrument=26, denominator=4, min_note=67)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self, bass_height, bass_width):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # save data parameters\n",
    "        self.bass_height = bass_height\n",
    "        self.bass_width = bass_width\n",
    "        self.bass_size = bass_height*bass_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        self.embedding_size = 1 # размер латентного пространства (на каждый отсчёт)\n",
    "                                # ЛУЧШЕ НЕ МЕНЯТЬ ЭТОТ ПАРАМЕТР С 1, придётся переписывать код!\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 26\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_embed_layer = nn.Linear(self.lstm_hidden_size, self.embedding_size)\n",
    "        \n",
    "        self.decoder_layer1 = nn.Linear(self.bass_height + self.condition_size, 4)\n",
    "        self.decoder_layer2 = nn.Linear(4, 48)\n",
    "        self.decoder_layer3 = nn.Linear(48, 384)\n",
    "        self.decoder_layer4 = nn.Linear(384 + self.condition_size, self.bass_size // 2)\n",
    "        self.decoder_layer5 = nn.Linear(self.bass_size // 2, self.bass_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_embed_layer(output))\n",
    "        return output\n",
    "    \n",
    "    def decoder(self, input, cond):\n",
    "        output = torch.cat((input, cond), axis=1) # добавляем conditioning\n",
    "        output = self.sigm(self.decoder_layer1(output))\n",
    "        output = self.sigm(self.decoder_layer2(output))\n",
    "        output = self.sigm(self.decoder_layer3(output))\n",
    "        output = torch.cat((output, cond), axis=1) # добавляем ещё conditioning\n",
    "        output = self.sigm(self.decoder_layer4(output))\n",
    "        output = self.sigm(self.decoder_layer5(output))\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input):\n",
    "        images = torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float)\n",
    "        result = self.encoder(images)\n",
    "        # избавляемся от лишней размерности (embedding_size=1), чтобы получить вектор из lstm\n",
    "        # размером с высоту изображения\n",
    "        result = result.squeeze().transpose(0,1)\n",
    "        # добавляем conditioning\n",
    "        conditionings = torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "        conditionings = conditionings\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.bass_height, self.bass_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN(data_height, melody_width)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.forward([drum_validation[16], drum_validation[14], drum_validation[43]]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 0.0102417\n",
      "[1,     6] train loss: 0.0074905\n",
      "[1,    11] train loss: 0.0078503\n",
      "[1,    16] train loss: 0.0076752\n",
      "[1,    20] train loss: 0.0080323\n",
      "Epoch #1\n",
      "[2,     1] train loss: 0.0086733\n",
      "[2,     6] train loss: 0.0080712\n",
      "[2,    11] train loss: 0.0077730\n",
      "[2,    16] train loss: 0.0079698\n",
      "[2,    20] train loss: 0.0073607\n",
      "Epoch #2\n",
      "[3,     1] train loss: 0.0095062\n",
      "[3,     6] train loss: 0.0078498\n",
      "[3,    11] train loss: 0.0074179\n",
      "[3,    16] train loss: 0.0082021\n",
      "[3,    20] train loss: 0.0075152\n",
      "Epoch #3\n",
      "[4,     1] train loss: 0.0093757\n",
      "[4,     6] train loss: 0.0074535\n",
      "[4,    11] train loss: 0.0075755\n",
      "[4,    16] train loss: 0.0078045\n",
      "[4,    20] train loss: 0.0078242\n",
      "Epoch #4\n",
      "[5,     1] train loss: 0.0088216\n",
      "[5,     6] train loss: 0.0076878\n",
      "[5,    11] train loss: 0.0078152\n",
      "[5,    16] train loss: 0.0075238\n",
      "[5,    20] train loss: 0.0081606\n",
      "Epoch #5\n",
      "[6,     1] train loss: 0.0094923\n",
      "[6,     6] train loss: 0.0074507\n",
      "[6,    11] train loss: 0.0075557\n",
      "[6,    16] train loss: 0.0080658\n",
      "[6,    20] train loss: 0.0077383\n",
      "Epoch #6\n",
      "[7,     1] train loss: 0.0096517\n",
      "[7,     6] train loss: 0.0077523\n",
      "[7,    11] train loss: 0.0080206\n",
      "[7,    16] train loss: 0.0080061\n",
      "[7,    20] train loss: 0.0070580\n",
      "Epoch #7\n",
      "[8,     1] train loss: 0.0098071\n",
      "[8,     6] train loss: 0.0080955\n",
      "[8,    11] train loss: 0.0077302\n",
      "[8,    16] train loss: 0.0077618\n",
      "[8,    20] train loss: 0.0074724\n",
      "Epoch #8\n",
      "[9,     1] train loss: 0.0090899\n",
      "[9,     6] train loss: 0.0076067\n",
      "[9,    11] train loss: 0.0079872\n",
      "[9,    16] train loss: 0.0076379\n",
      "[9,    20] train loss: 0.0078175\n",
      "Epoch #9\n",
      "[10,     1] train loss: 0.0088109\n",
      "[10,     6] train loss: 0.0079141\n",
      "[10,    11] train loss: 0.0074498\n",
      "[10,    16] train loss: 0.0078166\n",
      "[10,    20] train loss: 0.0074963\n",
      "Epoch #10\n",
      "[11,     1] train loss: 0.0096563\n",
      "[11,     6] train loss: 0.0076668\n",
      "[11,    11] train loss: 0.0078773\n",
      "[11,    16] train loss: 0.0078210\n",
      "[11,    20] train loss: 0.0075364\n",
      "Epoch #11\n",
      "[12,     1] train loss: 0.0096382\n",
      "[12,     6] train loss: 0.0075426\n",
      "[12,    11] train loss: 0.0080888\n",
      "[12,    16] train loss: 0.0076973\n",
      "[12,    20] train loss: 0.0071952\n",
      "Epoch #12\n",
      "[13,     1] train loss: 0.0100998\n",
      "[13,     6] train loss: 0.0073499\n",
      "[13,    11] train loss: 0.0078324\n",
      "[13,    16] train loss: 0.0079817\n",
      "[13,    20] train loss: 0.0078785\n",
      "Epoch #13\n",
      "[14,     1] train loss: 0.0083697\n",
      "[14,     6] train loss: 0.0075338\n",
      "[14,    11] train loss: 0.0077958\n",
      "[14,    16] train loss: 0.0074923\n",
      "[14,    20] train loss: 0.0081352\n",
      "Epoch #14\n",
      "[15,     1] train loss: 0.0092695\n",
      "[15,     6] train loss: 0.0077500\n",
      "[15,    11] train loss: 0.0077969\n",
      "[15,    16] train loss: 0.0075122\n",
      "[15,    20] train loss: 0.0077310\n",
      "Epoch #15\n",
      "[16,     1] train loss: 0.0094652\n",
      "[16,     6] train loss: 0.0078843\n",
      "[16,    11] train loss: 0.0078060\n",
      "[16,    16] train loss: 0.0076965\n",
      "[16,    20] train loss: 0.0078155\n",
      "Epoch #16\n",
      "[17,     1] train loss: 0.0095007\n",
      "[17,     6] train loss: 0.0077269\n",
      "[17,    11] train loss: 0.0080049\n",
      "[17,    16] train loss: 0.0076070\n",
      "[17,    20] train loss: 0.0071164\n",
      "Epoch #17\n",
      "[18,     1] train loss: 0.0100835\n",
      "[18,     6] train loss: 0.0073819\n",
      "[18,    11] train loss: 0.0077390\n",
      "[18,    16] train loss: 0.0079204\n",
      "[18,    20] train loss: 0.0078182\n",
      "Epoch #18\n",
      "[19,     1] train loss: 0.0088163\n",
      "[19,     6] train loss: 0.0077440\n",
      "[19,    11] train loss: 0.0079508\n",
      "[19,    16] train loss: 0.0076832\n",
      "[19,    20] train loss: 0.0074932\n",
      "Epoch #19\n",
      "[20,     1] train loss: 0.0095238\n",
      "[20,     6] train loss: 0.0077082\n",
      "[20,    11] train loss: 0.0078359\n",
      "[20,    16] train loss: 0.0079167\n",
      "[20,    20] train loss: 0.0074759\n",
      "Epoch #20\n",
      "[21,     1] train loss: 0.0089272\n",
      "[21,     6] train loss: 0.0076272\n",
      "[21,    11] train loss: 0.0076751\n",
      "[21,    16] train loss: 0.0076557\n",
      "[21,    20] train loss: 0.0077538\n",
      "Epoch #21\n",
      "[22,     1] train loss: 0.0088528\n",
      "[22,     6] train loss: 0.0077563\n",
      "[22,    11] train loss: 0.0077150\n",
      "[22,    16] train loss: 0.0079179\n",
      "[22,    20] train loss: 0.0071276\n",
      "Epoch #22\n",
      "[23,     1] train loss: 0.0095623\n",
      "[23,     6] train loss: 0.0084346\n",
      "[23,    11] train loss: 0.0071303\n",
      "[23,    16] train loss: 0.0080033\n",
      "[23,    20] train loss: 0.0069756\n",
      "Epoch #23\n",
      "[24,     1] train loss: 0.0102027\n",
      "[24,     6] train loss: 0.0078076\n",
      "[24,    11] train loss: 0.0075336\n",
      "[24,    16] train loss: 0.0077830\n",
      "[24,    20] train loss: 0.0072736\n",
      "Epoch #24\n",
      "[25,     1] train loss: 0.0078701\n",
      "[25,     6] train loss: 0.0075679\n",
      "[25,    11] train loss: 0.0082176\n",
      "[25,    16] train loss: 0.0080110\n",
      "[25,    20] train loss: 0.0072905\n",
      "Epoch #25\n",
      "[26,     1] train loss: 0.0102000\n",
      "[26,     6] train loss: 0.0075470\n",
      "[26,    11] train loss: 0.0076651\n",
      "[26,    16] train loss: 0.0075846\n",
      "[26,    20] train loss: 0.0071734\n",
      "Epoch #26\n",
      "[27,     1] train loss: 0.0087745\n",
      "[27,     6] train loss: 0.0076350\n",
      "[27,    11] train loss: 0.0078658\n",
      "[27,    16] train loss: 0.0077463\n",
      "[27,    20] train loss: 0.0077750\n",
      "Epoch #27\n",
      "[28,     1] train loss: 0.0101195\n",
      "[28,     6] train loss: 0.0080581\n",
      "[28,    11] train loss: 0.0076240\n",
      "[28,    16] train loss: 0.0079252\n",
      "[28,    20] train loss: 0.0072464\n",
      "Epoch #28\n",
      "[29,     1] train loss: 0.0085936\n",
      "[29,     6] train loss: 0.0076987\n",
      "[29,    11] train loss: 0.0078115\n",
      "[29,    16] train loss: 0.0077590\n",
      "[29,    20] train loss: 0.0077066\n",
      "Epoch #29\n",
      "[30,     1] train loss: 0.0087287\n",
      "[30,     6] train loss: 0.0074908\n",
      "[30,    11] train loss: 0.0080000\n",
      "[30,    16] train loss: 0.0077887\n",
      "[30,    20] train loss: 0.0070643\n",
      "Epoch #30\n",
      "[31,     1] train loss: 0.0091034\n",
      "[31,     6] train loss: 0.0078099\n",
      "[31,    11] train loss: 0.0080929\n",
      "[31,    16] train loss: 0.0078952\n",
      "[31,    20] train loss: 0.0071118\n",
      "Epoch #31\n",
      "[32,     1] train loss: 0.0103467\n",
      "[32,     6] train loss: 0.0076248\n",
      "[32,    11] train loss: 0.0079006\n",
      "[32,    16] train loss: 0.0075145\n",
      "[32,    20] train loss: 0.0074650\n",
      "Epoch #32\n",
      "[33,     1] train loss: 0.0087710\n",
      "[33,     6] train loss: 0.0077429\n",
      "[33,    11] train loss: 0.0076506\n",
      "[33,    16] train loss: 0.0080301\n",
      "[33,    20] train loss: 0.0073409\n",
      "Epoch #33\n",
      "[34,     1] train loss: 0.0101394\n",
      "[34,     6] train loss: 0.0078435\n",
      "[34,    11] train loss: 0.0075407\n",
      "[34,    16] train loss: 0.0076374\n",
      "[34,    20] train loss: 0.0073171\n",
      "Epoch #34\n",
      "[35,     1] train loss: 0.0083372\n",
      "[35,     6] train loss: 0.0077822\n",
      "[35,    11] train loss: 0.0076858\n",
      "[35,    16] train loss: 0.0078208\n",
      "[35,    20] train loss: 0.0076778\n",
      "Epoch #35\n",
      "[36,     1] train loss: 0.0082227\n",
      "[36,     6] train loss: 0.0075331\n",
      "[36,    11] train loss: 0.0078504\n",
      "[36,    16] train loss: 0.0075992\n",
      "[36,    20] train loss: 0.0073802\n",
      "Epoch #36\n",
      "[37,     1] train loss: 0.0092330\n",
      "[37,     6] train loss: 0.0077679\n",
      "[37,    11] train loss: 0.0076276\n",
      "[37,    16] train loss: 0.0074310\n",
      "[37,    20] train loss: 0.0074785\n",
      "Epoch #37\n",
      "[38,     1] train loss: 0.0097640\n",
      "[38,     6] train loss: 0.0077182\n",
      "[38,    11] train loss: 0.0078468\n",
      "[38,    16] train loss: 0.0075527\n",
      "[38,    20] train loss: 0.0072680\n",
      "Epoch #38\n",
      "[39,     1] train loss: 0.0089407\n",
      "[39,     6] train loss: 0.0079188\n",
      "[39,    11] train loss: 0.0074238\n",
      "[39,    16] train loss: 0.0073947\n",
      "[39,    20] train loss: 0.0075801\n",
      "Epoch #39\n",
      "[40,     1] train loss: 0.0087073\n",
      "[40,     6] train loss: 0.0078833\n",
      "[40,    11] train loss: 0.0074597\n",
      "[40,    16] train loss: 0.0077490\n",
      "[40,    20] train loss: 0.0072917\n",
      "Epoch #40\n",
      "[41,     1] train loss: 0.0081893\n",
      "[41,     6] train loss: 0.0075819\n",
      "[41,    11] train loss: 0.0073193\n",
      "[41,    16] train loss: 0.0078563\n",
      "[41,    20] train loss: 0.0078902\n",
      "Epoch #41\n",
      "[42,     1] train loss: 0.0091183\n",
      "[42,     6] train loss: 0.0075496\n",
      "[42,    11] train loss: 0.0076884\n",
      "[42,    16] train loss: 0.0077534\n",
      "[42,    20] train loss: 0.0070378\n",
      "Epoch #42\n",
      "[43,     1] train loss: 0.0087873\n",
      "[43,     6] train loss: 0.0080529\n",
      "[43,    11] train loss: 0.0074694\n",
      "[43,    16] train loss: 0.0077118\n",
      "[43,    20] train loss: 0.0074972\n",
      "Epoch #43\n",
      "[44,     1] train loss: 0.0091475\n",
      "[44,     6] train loss: 0.0076633\n",
      "[44,    11] train loss: 0.0077741\n",
      "[44,    16] train loss: 0.0078999\n",
      "[44,    20] train loss: 0.0073075\n",
      "Epoch #44\n",
      "[45,     1] train loss: 0.0087210\n",
      "[45,     6] train loss: 0.0076873\n",
      "[45,    11] train loss: 0.0079238\n",
      "[45,    16] train loss: 0.0078104\n",
      "[45,    20] train loss: 0.0070270\n",
      "Epoch #45\n",
      "[46,     1] train loss: 0.0084941\n",
      "[46,     6] train loss: 0.0074762\n",
      "[46,    11] train loss: 0.0077352\n",
      "[46,    16] train loss: 0.0081383\n",
      "[46,    20] train loss: 0.0073461\n",
      "Epoch #46\n",
      "[47,     1] train loss: 0.0083117\n",
      "[47,     6] train loss: 0.0076959\n",
      "[47,    11] train loss: 0.0081724\n",
      "[47,    16] train loss: 0.0078979\n",
      "[47,    20] train loss: 0.0072011\n",
      "Epoch #47\n",
      "[48,     1] train loss: 0.0097816\n",
      "[48,     6] train loss: 0.0075907\n",
      "[48,    11] train loss: 0.0082712\n",
      "[48,    16] train loss: 0.0072288\n",
      "[48,    20] train loss: 0.0075993\n",
      "Epoch #48\n",
      "[49,     1] train loss: 0.0093832\n",
      "[49,     6] train loss: 0.0078321\n",
      "[49,    11] train loss: 0.0073954\n",
      "[49,    16] train loss: 0.0074167\n",
      "[49,    20] train loss: 0.0077764\n",
      "Epoch #49\n",
      "[50,     1] train loss: 0.0090132\n",
      "[50,     6] train loss: 0.0078677\n",
      "[50,    11] train loss: 0.0074335\n",
      "[50,    16] train loss: 0.0080246\n",
      "[50,    20] train loss: 0.0073480\n",
      "Epoch #50\n",
      "[51,     1] train loss: 0.0089947\n",
      "[51,     6] train loss: 0.0075635\n",
      "[51,    11] train loss: 0.0076766\n",
      "[51,    16] train loss: 0.0074593\n",
      "[51,    20] train loss: 0.0074147\n",
      "Epoch #51\n",
      "[52,     1] train loss: 0.0092966\n",
      "[52,     6] train loss: 0.0072632\n",
      "[52,    11] train loss: 0.0075393\n",
      "[52,    16] train loss: 0.0081302\n",
      "[52,    20] train loss: 0.0076755\n",
      "Epoch #52\n",
      "[53,     1] train loss: 0.0091646\n",
      "[53,     6] train loss: 0.0077970\n",
      "[53,    11] train loss: 0.0072611\n",
      "[53,    16] train loss: 0.0074589\n",
      "[53,    20] train loss: 0.0076450\n",
      "Epoch #53\n",
      "[54,     1] train loss: 0.0093191\n",
      "[54,     6] train loss: 0.0072737\n",
      "[54,    11] train loss: 0.0082178\n",
      "[54,    16] train loss: 0.0072712\n",
      "[54,    20] train loss: 0.0076284\n",
      "Epoch #54\n",
      "[55,     1] train loss: 0.0091837\n",
      "[55,     6] train loss: 0.0073336\n",
      "[55,    11] train loss: 0.0074810\n",
      "[55,    16] train loss: 0.0077486\n",
      "[55,    20] train loss: 0.0074315\n",
      "Epoch #55\n",
      "[56,     1] train loss: 0.0092750\n",
      "[56,     6] train loss: 0.0078539\n",
      "[56,    11] train loss: 0.0072599\n",
      "[56,    16] train loss: 0.0077784\n",
      "[56,    20] train loss: 0.0075683\n",
      "Epoch #56\n",
      "[57,     1] train loss: 0.0076656\n",
      "[57,     6] train loss: 0.0078765\n",
      "[57,    11] train loss: 0.0074716\n",
      "[57,    16] train loss: 0.0077185\n",
      "[57,    20] train loss: 0.0075881\n",
      "Epoch #57\n",
      "[58,     1] train loss: 0.0088407\n",
      "[58,     6] train loss: 0.0077510\n",
      "[58,    11] train loss: 0.0077309\n",
      "[58,    16] train loss: 0.0074446\n",
      "[58,    20] train loss: 0.0075776\n",
      "Epoch #58\n",
      "[59,     1] train loss: 0.0088739\n",
      "[59,     6] train loss: 0.0074721\n",
      "[59,    11] train loss: 0.0075218\n",
      "[59,    16] train loss: 0.0077269\n",
      "[59,    20] train loss: 0.0073385\n",
      "Epoch #59\n",
      "[60,     1] train loss: 0.0087964\n",
      "[60,     6] train loss: 0.0079892\n",
      "[60,    11] train loss: 0.0077480\n",
      "[60,    16] train loss: 0.0076519\n",
      "[60,    20] train loss: 0.0070317\n",
      "Epoch #60\n",
      "[61,     1] train loss: 0.0092869\n",
      "[61,     6] train loss: 0.0080215\n",
      "[61,    11] train loss: 0.0070096\n",
      "[61,    16] train loss: 0.0079627\n",
      "[61,    20] train loss: 0.0069839\n",
      "Epoch #61\n",
      "[62,     1] train loss: 0.0093264\n",
      "[62,     6] train loss: 0.0074692\n",
      "[62,    11] train loss: 0.0078307\n",
      "[62,    16] train loss: 0.0071977\n",
      "[62,    20] train loss: 0.0073372\n",
      "Epoch #62\n",
      "[63,     1] train loss: 0.0101920\n",
      "[63,     6] train loss: 0.0074581\n",
      "[63,    11] train loss: 0.0077782\n",
      "[63,    16] train loss: 0.0075082\n",
      "[63,    20] train loss: 0.0071926\n",
      "Epoch #63\n",
      "[64,     1] train loss: 0.0093607\n",
      "[64,     6] train loss: 0.0075020\n",
      "[64,    11] train loss: 0.0071398\n",
      "[64,    16] train loss: 0.0079804\n",
      "[64,    20] train loss: 0.0078498\n",
      "Epoch #64\n",
      "[65,     1] train loss: 0.0082171\n",
      "[65,     6] train loss: 0.0071246\n",
      "[65,    11] train loss: 0.0078192\n",
      "[65,    16] train loss: 0.0076018\n",
      "[65,    20] train loss: 0.0077464\n",
      "Epoch #65\n",
      "[66,     1] train loss: 0.0080116\n",
      "[66,     6] train loss: 0.0079671\n",
      "[66,    11] train loss: 0.0074527\n",
      "[66,    16] train loss: 0.0073935\n",
      "[66,    20] train loss: 0.0076845\n",
      "Epoch #66\n",
      "[67,     1] train loss: 0.0091953\n",
      "[67,     6] train loss: 0.0073250\n",
      "[67,    11] train loss: 0.0075253\n",
      "[67,    16] train loss: 0.0076496\n",
      "[67,    20] train loss: 0.0075267\n",
      "Epoch #67\n",
      "[68,     1] train loss: 0.0082901\n",
      "[68,     6] train loss: 0.0081163\n",
      "[68,    11] train loss: 0.0076275\n",
      "[68,    16] train loss: 0.0074481\n",
      "[68,    20] train loss: 0.0075089\n",
      "Epoch #68\n",
      "[69,     1] train loss: 0.0085397\n",
      "[69,     6] train loss: 0.0074142\n",
      "[69,    11] train loss: 0.0076033\n",
      "[69,    16] train loss: 0.0077330\n",
      "[69,    20] train loss: 0.0076160\n",
      "Epoch #69\n",
      "[70,     1] train loss: 0.0094719\n",
      "[70,     6] train loss: 0.0075625\n",
      "[70,    11] train loss: 0.0078747\n",
      "[70,    16] train loss: 0.0078409\n",
      "[70,    20] train loss: 0.0070332\n",
      "Epoch #70\n",
      "[71,     1] train loss: 0.0088878\n",
      "[71,     6] train loss: 0.0076952\n",
      "[71,    11] train loss: 0.0076750\n",
      "[71,    16] train loss: 0.0075750\n",
      "[71,    20] train loss: 0.0068686\n",
      "Epoch #71\n",
      "[72,     1] train loss: 0.0089675\n",
      "[72,     6] train loss: 0.0081544\n",
      "[72,    11] train loss: 0.0072152\n",
      "[72,    16] train loss: 0.0073811\n",
      "[72,    20] train loss: 0.0070600\n",
      "Epoch #72\n",
      "[73,     1] train loss: 0.0096023\n",
      "[73,     6] train loss: 0.0074458\n",
      "[73,    11] train loss: 0.0071520\n",
      "[73,    16] train loss: 0.0076613\n",
      "[73,    20] train loss: 0.0074822\n",
      "Epoch #73\n",
      "[74,     1] train loss: 0.0090961\n",
      "[74,     6] train loss: 0.0082468\n",
      "[74,    11] train loss: 0.0073995\n",
      "[74,    16] train loss: 0.0074011\n",
      "[74,    20] train loss: 0.0073844\n",
      "Epoch #74\n",
      "[75,     1] train loss: 0.0095150\n",
      "[75,     6] train loss: 0.0075827\n",
      "[75,    11] train loss: 0.0077768\n",
      "[75,    16] train loss: 0.0073892\n",
      "[75,    20] train loss: 0.0071923\n",
      "Epoch #75\n",
      "[76,     1] train loss: 0.0096156\n",
      "[76,     6] train loss: 0.0077397\n",
      "[76,    11] train loss: 0.0073539\n",
      "[76,    16] train loss: 0.0073488\n",
      "[76,    20] train loss: 0.0074810\n",
      "Epoch #76\n",
      "[77,     1] train loss: 0.0097390\n",
      "[77,     6] train loss: 0.0072768\n",
      "[77,    11] train loss: 0.0077246\n",
      "[77,    16] train loss: 0.0079552\n",
      "[77,    20] train loss: 0.0070143\n",
      "Epoch #77\n",
      "[78,     1] train loss: 0.0089916\n",
      "[78,     6] train loss: 0.0080708\n",
      "[78,    11] train loss: 0.0077837\n",
      "[78,    16] train loss: 0.0074122\n",
      "[78,    20] train loss: 0.0068401\n",
      "Epoch #78\n",
      "[79,     1] train loss: 0.0086470\n",
      "[79,     6] train loss: 0.0076239\n",
      "[79,    11] train loss: 0.0076053\n",
      "[79,    16] train loss: 0.0076648\n",
      "[79,    20] train loss: 0.0073828\n",
      "Epoch #79\n",
      "[80,     1] train loss: 0.0097487\n",
      "[80,     6] train loss: 0.0077021\n",
      "[80,    11] train loss: 0.0080918\n",
      "[80,    16] train loss: 0.0073494\n",
      "[80,    20] train loss: 0.0070257\n",
      "Epoch #80\n",
      "[81,     1] train loss: 0.0098826\n",
      "[81,     6] train loss: 0.0073834\n",
      "[81,    11] train loss: 0.0072343\n",
      "[81,    16] train loss: 0.0078929\n",
      "[81,    20] train loss: 0.0071492\n",
      "Epoch #81\n",
      "[82,     1] train loss: 0.0105198\n",
      "[82,     6] train loss: 0.0073928\n",
      "[82,    11] train loss: 0.0073150\n",
      "[82,    16] train loss: 0.0078989\n",
      "[82,    20] train loss: 0.0069842\n",
      "Epoch #82\n",
      "[83,     1] train loss: 0.0089001\n",
      "[83,     6] train loss: 0.0074431\n",
      "[83,    11] train loss: 0.0074955\n",
      "[83,    16] train loss: 0.0075821\n",
      "[83,    20] train loss: 0.0072286\n",
      "Epoch #83\n",
      "[84,     1] train loss: 0.0099345\n",
      "[84,     6] train loss: 0.0074448\n",
      "[84,    11] train loss: 0.0076439\n",
      "[84,    16] train loss: 0.0080687\n",
      "[84,    20] train loss: 0.0067896\n",
      "Epoch #84\n",
      "[85,     1] train loss: 0.0097103\n",
      "[85,     6] train loss: 0.0073406\n",
      "[85,    11] train loss: 0.0076460\n",
      "[85,    16] train loss: 0.0075010\n",
      "[85,    20] train loss: 0.0075749\n",
      "Epoch #85\n",
      "[86,     1] train loss: 0.0096245\n",
      "[86,     6] train loss: 0.0076141\n",
      "[86,    11] train loss: 0.0078429\n",
      "[86,    16] train loss: 0.0077452\n",
      "[86,    20] train loss: 0.0071438\n",
      "Epoch #86\n",
      "[87,     1] train loss: 0.0093036\n",
      "[87,     6] train loss: 0.0071085\n",
      "[87,    11] train loss: 0.0075840\n",
      "[87,    16] train loss: 0.0077814\n",
      "[87,    20] train loss: 0.0071832\n",
      "Epoch #87\n",
      "[88,     1] train loss: 0.0089236\n",
      "[88,     6] train loss: 0.0076205\n",
      "[88,    11] train loss: 0.0076768\n",
      "[88,    16] train loss: 0.0075118\n",
      "[88,    20] train loss: 0.0073762\n",
      "Epoch #88\n",
      "[89,     1] train loss: 0.0085593\n",
      "[89,     6] train loss: 0.0072753\n",
      "[89,    11] train loss: 0.0079602\n",
      "[89,    16] train loss: 0.0074986\n",
      "[89,    20] train loss: 0.0075675\n",
      "Epoch #89\n",
      "[90,     1] train loss: 0.0077134\n",
      "[90,     6] train loss: 0.0075121\n",
      "[90,    11] train loss: 0.0072358\n",
      "[90,    16] train loss: 0.0077721\n",
      "[90,    20] train loss: 0.0074943\n",
      "Epoch #90\n",
      "[91,     1] train loss: 0.0082884\n",
      "[91,     6] train loss: 0.0074900\n",
      "[91,    11] train loss: 0.0074420\n",
      "[91,    16] train loss: 0.0080082\n",
      "[91,    20] train loss: 0.0070381\n",
      "Epoch #91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92,     1] train loss: 0.0086269\n",
      "[92,     6] train loss: 0.0073707\n",
      "[92,    11] train loss: 0.0075555\n",
      "[92,    16] train loss: 0.0075661\n",
      "[92,    20] train loss: 0.0073888\n",
      "Epoch #92\n",
      "[93,     1] train loss: 0.0092102\n",
      "[93,     6] train loss: 0.0078935\n",
      "[93,    11] train loss: 0.0072491\n",
      "[93,    16] train loss: 0.0074746\n",
      "[93,    20] train loss: 0.0069276\n",
      "Epoch #93\n",
      "[94,     1] train loss: 0.0090808\n",
      "[94,     6] train loss: 0.0073179\n",
      "[94,    11] train loss: 0.0078295\n",
      "[94,    16] train loss: 0.0073628\n",
      "[94,    20] train loss: 0.0071301\n",
      "Epoch #94\n",
      "[95,     1] train loss: 0.0098413\n",
      "[95,     6] train loss: 0.0077642\n",
      "[95,    11] train loss: 0.0071500\n",
      "[95,    16] train loss: 0.0077389\n",
      "[95,    20] train loss: 0.0071822\n",
      "Epoch #95\n",
      "[96,     1] train loss: 0.0089188\n",
      "[96,     6] train loss: 0.0077673\n",
      "[96,    11] train loss: 0.0074610\n",
      "[96,    16] train loss: 0.0077332\n",
      "[96,    20] train loss: 0.0070490\n",
      "Epoch #96\n",
      "[97,     1] train loss: 0.0086806\n",
      "[97,     6] train loss: 0.0076206\n",
      "[97,    11] train loss: 0.0074148\n",
      "[97,    16] train loss: 0.0076655\n",
      "[97,    20] train loss: 0.0074292\n",
      "Epoch #97\n",
      "[98,     1] train loss: 0.0087287\n",
      "[98,     6] train loss: 0.0075532\n",
      "[98,    11] train loss: 0.0070563\n",
      "[98,    16] train loss: 0.0077024\n",
      "[98,    20] train loss: 0.0075459\n",
      "Epoch #98\n",
      "[99,     1] train loss: 0.0085593\n",
      "[99,     6] train loss: 0.0077320\n",
      "[99,    11] train loss: 0.0073590\n",
      "[99,    16] train loss: 0.0075148\n",
      "[99,    20] train loss: 0.0072247\n",
      "Epoch #99\n",
      "[100,     1] train loss: 0.0101352\n",
      "[100,     6] train loss: 0.0074961\n",
      "[100,    11] train loss: 0.0075352\n",
      "[100,    16] train loss: 0.0076202\n",
      "[100,    20] train loss: 0.0071981\n",
      "Epoch #100\n",
      "[101,     1] train loss: 0.0079631\n",
      "[101,     6] train loss: 0.0072164\n",
      "[101,    11] train loss: 0.0075796\n",
      "[101,    16] train loss: 0.0076172\n",
      "[101,    20] train loss: 0.0073930\n",
      "Epoch #101\n",
      "[102,     1] train loss: 0.0086202\n",
      "[102,     6] train loss: 0.0076150\n",
      "[102,    11] train loss: 0.0078921\n",
      "[102,    16] train loss: 0.0076613\n",
      "[102,    20] train loss: 0.0075198\n",
      "Epoch #102\n",
      "[103,     1] train loss: 0.0089979\n",
      "[103,     6] train loss: 0.0077168\n",
      "[103,    11] train loss: 0.0072408\n",
      "[103,    16] train loss: 0.0075453\n",
      "[103,    20] train loss: 0.0074311\n",
      "Epoch #103\n",
      "[104,     1] train loss: 0.0095160\n",
      "[104,     6] train loss: 0.0078139\n",
      "[104,    11] train loss: 0.0074443\n",
      "[104,    16] train loss: 0.0069947\n",
      "[104,    20] train loss: 0.0080480\n",
      "Epoch #104\n",
      "[105,     1] train loss: 0.0114429\n",
      "[105,     6] train loss: 0.0076976\n",
      "[105,    11] train loss: 0.0072331\n",
      "[105,    16] train loss: 0.0073883\n",
      "[105,    20] train loss: 0.0073010\n",
      "Epoch #105\n",
      "[106,     1] train loss: 0.0076929\n",
      "[106,     6] train loss: 0.0074104\n",
      "[106,    11] train loss: 0.0079291\n",
      "[106,    16] train loss: 0.0079007\n",
      "[106,    20] train loss: 0.0068381\n",
      "Epoch #106\n",
      "[107,     1] train loss: 0.0085730\n",
      "[107,     6] train loss: 0.0078645\n",
      "[107,    11] train loss: 0.0079207\n",
      "[107,    16] train loss: 0.0076660\n",
      "[107,    20] train loss: 0.0069645\n",
      "Epoch #107\n",
      "[108,     1] train loss: 0.0096978\n",
      "[108,     6] train loss: 0.0077270\n",
      "[108,    11] train loss: 0.0075548\n",
      "[108,    16] train loss: 0.0072229\n",
      "[108,    20] train loss: 0.0072470\n",
      "Epoch #108\n",
      "[109,     1] train loss: 0.0082682\n",
      "[109,     6] train loss: 0.0072703\n",
      "[109,    11] train loss: 0.0075527\n",
      "[109,    16] train loss: 0.0080836\n",
      "[109,    20] train loss: 0.0068151\n",
      "Epoch #109\n",
      "[110,     1] train loss: 0.0091243\n",
      "[110,     6] train loss: 0.0074201\n",
      "[110,    11] train loss: 0.0076486\n",
      "[110,    16] train loss: 0.0074745\n",
      "[110,    20] train loss: 0.0071934\n",
      "Epoch #110\n",
      "[111,     1] train loss: 0.0092341\n",
      "[111,     6] train loss: 0.0080850\n",
      "[111,    11] train loss: 0.0072053\n",
      "[111,    16] train loss: 0.0073945\n",
      "[111,    20] train loss: 0.0070758\n",
      "Epoch #111\n",
      "[112,     1] train loss: 0.0086940\n",
      "[112,     6] train loss: 0.0078401\n",
      "[112,    11] train loss: 0.0070910\n",
      "[112,    16] train loss: 0.0075793\n",
      "[112,    20] train loss: 0.0074297\n",
      "Epoch #112\n",
      "[113,     1] train loss: 0.0079388\n",
      "[113,     6] train loss: 0.0078979\n",
      "[113,    11] train loss: 0.0073620\n",
      "[113,    16] train loss: 0.0076620\n",
      "[113,    20] train loss: 0.0070563\n",
      "Epoch #113\n",
      "[114,     1] train loss: 0.0076266\n",
      "[114,     6] train loss: 0.0077292\n",
      "[114,    11] train loss: 0.0074749\n",
      "[114,    16] train loss: 0.0076894\n",
      "[114,    20] train loss: 0.0067026\n",
      "Epoch #114\n",
      "[115,     1] train loss: 0.0080400\n",
      "[115,     6] train loss: 0.0076783\n",
      "[115,    11] train loss: 0.0075371\n",
      "[115,    16] train loss: 0.0073964\n",
      "[115,    20] train loss: 0.0073852\n",
      "Epoch #115\n",
      "[116,     1] train loss: 0.0098830\n",
      "[116,     6] train loss: 0.0076497\n",
      "[116,    11] train loss: 0.0075470\n",
      "[116,    16] train loss: 0.0075737\n",
      "[116,    20] train loss: 0.0066984\n",
      "Epoch #116\n",
      "[117,     1] train loss: 0.0081804\n",
      "[117,     6] train loss: 0.0074785\n",
      "[117,    11] train loss: 0.0077177\n",
      "[117,    16] train loss: 0.0079286\n",
      "[117,    20] train loss: 0.0070967\n",
      "Epoch #117\n",
      "[118,     1] train loss: 0.0089662\n",
      "[118,     6] train loss: 0.0071718\n",
      "[118,    11] train loss: 0.0083814\n",
      "[118,    16] train loss: 0.0073794\n",
      "[118,    20] train loss: 0.0069166\n",
      "Epoch #118\n",
      "[119,     1] train loss: 0.0092268\n",
      "[119,     6] train loss: 0.0077303\n",
      "[119,    11] train loss: 0.0075990\n",
      "[119,    16] train loss: 0.0075221\n",
      "[119,    20] train loss: 0.0071171\n",
      "Epoch #119\n",
      "[120,     1] train loss: 0.0095202\n",
      "[120,     6] train loss: 0.0071375\n",
      "[120,    11] train loss: 0.0077425\n",
      "[120,    16] train loss: 0.0075074\n",
      "[120,    20] train loss: 0.0072979\n",
      "Epoch #120\n",
      "[121,     1] train loss: 0.0081367\n",
      "[121,     6] train loss: 0.0073301\n",
      "[121,    11] train loss: 0.0074313\n",
      "[121,    16] train loss: 0.0075292\n",
      "[121,    20] train loss: 0.0073227\n",
      "Epoch #121\n",
      "[122,     1] train loss: 0.0080171\n",
      "[122,     6] train loss: 0.0074236\n",
      "[122,    11] train loss: 0.0079925\n",
      "[122,    16] train loss: 0.0070373\n",
      "[122,    20] train loss: 0.0072962\n",
      "Epoch #122\n",
      "[123,     1] train loss: 0.0097725\n",
      "[123,     6] train loss: 0.0077731\n",
      "[123,    11] train loss: 0.0075272\n",
      "[123,    16] train loss: 0.0075737\n",
      "[123,    20] train loss: 0.0071830\n",
      "Epoch #123\n",
      "[124,     1] train loss: 0.0090285\n",
      "[124,     6] train loss: 0.0075349\n",
      "[124,    11] train loss: 0.0079976\n",
      "[124,    16] train loss: 0.0072366\n",
      "[124,    20] train loss: 0.0066444\n",
      "Epoch #124\n",
      "[125,     1] train loss: 0.0093282\n",
      "[125,     6] train loss: 0.0075326\n",
      "[125,    11] train loss: 0.0072523\n",
      "[125,    16] train loss: 0.0073282\n",
      "[125,    20] train loss: 0.0074963\n",
      "Epoch #125\n",
      "[126,     1] train loss: 0.0087282\n",
      "[126,     6] train loss: 0.0074667\n",
      "[126,    11] train loss: 0.0075293\n",
      "[126,    16] train loss: 0.0077824\n",
      "[126,    20] train loss: 0.0069509\n",
      "Epoch #126\n",
      "[127,     1] train loss: 0.0086960\n",
      "[127,     6] train loss: 0.0071049\n",
      "[127,    11] train loss: 0.0077416\n",
      "[127,    16] train loss: 0.0074509\n",
      "[127,    20] train loss: 0.0071077\n",
      "Epoch #127\n",
      "[128,     1] train loss: 0.0094249\n",
      "[128,     6] train loss: 0.0075450\n",
      "[128,    11] train loss: 0.0074704\n",
      "[128,    16] train loss: 0.0070612\n",
      "[128,    20] train loss: 0.0072719\n",
      "Epoch #128\n",
      "[129,     1] train loss: 0.0087842\n",
      "[129,     6] train loss: 0.0074562\n",
      "[129,    11] train loss: 0.0073624\n",
      "[129,    16] train loss: 0.0071784\n",
      "[129,    20] train loss: 0.0076293\n",
      "Epoch #129\n",
      "[130,     1] train loss: 0.0094946\n",
      "[130,     6] train loss: 0.0071157\n",
      "[130,    11] train loss: 0.0074720\n",
      "[130,    16] train loss: 0.0077123\n",
      "[130,    20] train loss: 0.0071512\n",
      "Epoch #130\n",
      "[131,     1] train loss: 0.0085088\n",
      "[131,     6] train loss: 0.0074230\n",
      "[131,    11] train loss: 0.0075639\n",
      "[131,    16] train loss: 0.0073559\n",
      "[131,    20] train loss: 0.0072922\n",
      "Epoch #131\n",
      "[132,     1] train loss: 0.0088431\n",
      "[132,     6] train loss: 0.0071836\n",
      "[132,    11] train loss: 0.0076365\n",
      "[132,    16] train loss: 0.0075288\n",
      "[132,    20] train loss: 0.0070898\n",
      "Epoch #132\n",
      "[133,     1] train loss: 0.0084321\n",
      "[133,     6] train loss: 0.0075266\n",
      "[133,    11] train loss: 0.0077262\n",
      "[133,    16] train loss: 0.0072032\n",
      "[133,    20] train loss: 0.0072978\n",
      "Epoch #133\n",
      "[134,     1] train loss: 0.0085218\n",
      "[134,     6] train loss: 0.0070885\n",
      "[134,    11] train loss: 0.0072906\n",
      "[134,    16] train loss: 0.0075973\n",
      "[134,    20] train loss: 0.0075279\n",
      "Epoch #134\n",
      "[135,     1] train loss: 0.0088551\n",
      "[135,     6] train loss: 0.0071909\n",
      "[135,    11] train loss: 0.0077155\n",
      "[135,    16] train loss: 0.0079401\n",
      "[135,    20] train loss: 0.0068806\n",
      "Epoch #135\n",
      "[136,     1] train loss: 0.0075893\n",
      "[136,     6] train loss: 0.0079724\n",
      "[136,    11] train loss: 0.0077238\n",
      "[136,    16] train loss: 0.0071156\n",
      "[136,    20] train loss: 0.0070611\n",
      "Epoch #136\n",
      "[137,     1] train loss: 0.0083133\n",
      "[137,     6] train loss: 0.0075011\n",
      "[137,    11] train loss: 0.0076895\n",
      "[137,    16] train loss: 0.0070150\n",
      "[137,    20] train loss: 0.0077777\n",
      "Epoch #137\n",
      "[138,     1] train loss: 0.0084644\n",
      "[138,     6] train loss: 0.0074112\n",
      "[138,    11] train loss: 0.0074842\n",
      "[138,    16] train loss: 0.0072986\n",
      "[138,    20] train loss: 0.0074478\n",
      "Epoch #138\n",
      "[139,     1] train loss: 0.0087431\n",
      "[139,     6] train loss: 0.0073790\n",
      "[139,    11] train loss: 0.0075038\n",
      "[139,    16] train loss: 0.0076518\n",
      "[139,    20] train loss: 0.0073505\n",
      "Epoch #139\n",
      "[140,     1] train loss: 0.0087201\n",
      "[140,     6] train loss: 0.0072548\n",
      "[140,    11] train loss: 0.0077459\n",
      "[140,    16] train loss: 0.0075128\n",
      "[140,    20] train loss: 0.0071296\n",
      "Epoch #140\n",
      "[141,     1] train loss: 0.0082807\n",
      "[141,     6] train loss: 0.0072866\n",
      "[141,    11] train loss: 0.0074861\n",
      "[141,    16] train loss: 0.0077322\n",
      "[141,    20] train loss: 0.0072046\n",
      "Epoch #141\n",
      "[142,     1] train loss: 0.0089017\n",
      "[142,     6] train loss: 0.0076806\n",
      "[142,    11] train loss: 0.0077335\n",
      "[142,    16] train loss: 0.0072321\n",
      "[142,    20] train loss: 0.0068963\n",
      "Epoch #142\n",
      "[143,     1] train loss: 0.0087460\n",
      "[143,     6] train loss: 0.0071831\n",
      "[143,    11] train loss: 0.0078601\n",
      "[143,    16] train loss: 0.0074881\n",
      "[143,    20] train loss: 0.0074526\n",
      "Epoch #143\n",
      "[144,     1] train loss: 0.0085718\n",
      "[144,     6] train loss: 0.0075960\n",
      "[144,    11] train loss: 0.0073191\n",
      "[144,    16] train loss: 0.0073604\n",
      "[144,    20] train loss: 0.0076593\n",
      "Epoch #144\n",
      "[145,     1] train loss: 0.0080428\n",
      "[145,     6] train loss: 0.0076950\n",
      "[145,    11] train loss: 0.0074634\n",
      "[145,    16] train loss: 0.0075291\n",
      "[145,    20] train loss: 0.0071336\n",
      "Epoch #145\n",
      "[146,     1] train loss: 0.0100273\n",
      "[146,     6] train loss: 0.0075387\n",
      "[146,    11] train loss: 0.0072763\n",
      "[146,    16] train loss: 0.0075817\n",
      "[146,    20] train loss: 0.0069777\n",
      "Epoch #146\n",
      "[147,     1] train loss: 0.0082575\n",
      "[147,     6] train loss: 0.0076631\n",
      "[147,    11] train loss: 0.0073685\n",
      "[147,    16] train loss: 0.0074131\n",
      "[147,    20] train loss: 0.0071486\n",
      "Epoch #147\n",
      "[148,     1] train loss: 0.0100152\n",
      "[148,     6] train loss: 0.0072535\n",
      "[148,    11] train loss: 0.0075576\n",
      "[148,    16] train loss: 0.0072649\n",
      "[148,    20] train loss: 0.0076172\n",
      "Epoch #148\n",
      "[149,     1] train loss: 0.0067143\n",
      "[149,     6] train loss: 0.0075251\n",
      "[149,    11] train loss: 0.0071851\n",
      "[149,    16] train loss: 0.0081380\n",
      "[149,    20] train loss: 0.0072652\n",
      "Epoch #149\n",
      "[150,     1] train loss: 0.0092860\n",
      "[150,     6] train loss: 0.0071287\n",
      "[150,    11] train loss: 0.0068801\n",
      "[150,    16] train loss: 0.0072677\n",
      "[150,    20] train loss: 0.0077386\n",
      "Epoch #150\n",
      "[151,     1] train loss: 0.0090546\n",
      "[151,     6] train loss: 0.0076352\n",
      "[151,    11] train loss: 0.0072883\n",
      "[151,    16] train loss: 0.0074949\n",
      "[151,    20] train loss: 0.0069241\n",
      "Epoch #151\n",
      "[152,     1] train loss: 0.0086780\n",
      "[152,     6] train loss: 0.0072251\n",
      "[152,    11] train loss: 0.0076412\n",
      "[152,    16] train loss: 0.0077170\n",
      "[152,    20] train loss: 0.0074570\n",
      "Epoch #152\n",
      "[153,     1] train loss: 0.0090276\n",
      "[153,     6] train loss: 0.0071735\n",
      "[153,    11] train loss: 0.0075362\n",
      "[153,    16] train loss: 0.0073999\n",
      "[153,    20] train loss: 0.0072967\n",
      "Epoch #153\n",
      "[154,     1] train loss: 0.0080543\n",
      "[154,     6] train loss: 0.0075550\n",
      "[154,    11] train loss: 0.0072989\n",
      "[154,    16] train loss: 0.0078920\n",
      "[154,    20] train loss: 0.0070837\n",
      "Epoch #154\n",
      "[155,     1] train loss: 0.0092449\n",
      "[155,     6] train loss: 0.0073887\n",
      "[155,    11] train loss: 0.0073929\n",
      "[155,    16] train loss: 0.0076386\n",
      "[155,    20] train loss: 0.0073832\n",
      "Epoch #155\n",
      "[156,     1] train loss: 0.0082905\n",
      "[156,     6] train loss: 0.0075548\n",
      "[156,    11] train loss: 0.0075422\n",
      "[156,    16] train loss: 0.0074918\n",
      "[156,    20] train loss: 0.0069214\n",
      "Epoch #156\n",
      "[157,     1] train loss: 0.0083232\n",
      "[157,     6] train loss: 0.0076736\n",
      "[157,    11] train loss: 0.0077973\n",
      "[157,    16] train loss: 0.0074086\n",
      "[157,    20] train loss: 0.0069656\n",
      "Epoch #157\n",
      "[158,     1] train loss: 0.0108626\n",
      "[158,     6] train loss: 0.0072959\n",
      "[158,    11] train loss: 0.0072273\n",
      "[158,    16] train loss: 0.0075976\n",
      "[158,    20] train loss: 0.0073837\n",
      "Epoch #158\n",
      "[159,     1] train loss: 0.0084792\n",
      "[159,     6] train loss: 0.0070658\n",
      "[159,    11] train loss: 0.0075542\n",
      "[159,    16] train loss: 0.0074886\n",
      "[159,    20] train loss: 0.0074835\n",
      "Epoch #159\n",
      "[160,     1] train loss: 0.0090248\n",
      "[160,     6] train loss: 0.0072071\n",
      "[160,    11] train loss: 0.0073436\n",
      "[160,    16] train loss: 0.0074401\n",
      "[160,    20] train loss: 0.0070172\n",
      "Epoch #160\n",
      "[161,     1] train loss: 0.0084572\n",
      "[161,     6] train loss: 0.0075856\n",
      "[161,    11] train loss: 0.0076096\n",
      "[161,    16] train loss: 0.0076491\n",
      "[161,    20] train loss: 0.0069239\n",
      "Epoch #161\n",
      "[162,     1] train loss: 0.0091320\n",
      "[162,     6] train loss: 0.0078090\n",
      "[162,    11] train loss: 0.0075718\n",
      "[162,    16] train loss: 0.0070341\n",
      "[162,    20] train loss: 0.0075870\n",
      "Epoch #162\n",
      "[163,     1] train loss: 0.0090951\n",
      "[163,     6] train loss: 0.0074984\n",
      "[163,    11] train loss: 0.0069301\n",
      "[163,    16] train loss: 0.0078511\n",
      "[163,    20] train loss: 0.0069503\n",
      "Epoch #163\n",
      "[164,     1] train loss: 0.0079622\n",
      "[164,     6] train loss: 0.0074937\n",
      "[164,    11] train loss: 0.0077524\n",
      "[164,    16] train loss: 0.0073282\n",
      "[164,    20] train loss: 0.0070664\n",
      "Epoch #164\n",
      "[165,     1] train loss: 0.0094752\n",
      "[165,     6] train loss: 0.0078131\n",
      "[165,    11] train loss: 0.0074859\n",
      "[165,    16] train loss: 0.0073459\n",
      "[165,    20] train loss: 0.0068893\n",
      "Epoch #165\n",
      "[166,     1] train loss: 0.0088382\n",
      "[166,     6] train loss: 0.0074562\n",
      "[166,    11] train loss: 0.0076188\n",
      "[166,    16] train loss: 0.0072599\n",
      "[166,    20] train loss: 0.0071479\n",
      "Epoch #166\n",
      "[167,     1] train loss: 0.0082302\n",
      "[167,     6] train loss: 0.0072985\n",
      "[167,    11] train loss: 0.0073417\n",
      "[167,    16] train loss: 0.0079216\n",
      "[167,    20] train loss: 0.0070868\n",
      "Epoch #167\n",
      "[168,     1] train loss: 0.0079737\n",
      "[168,     6] train loss: 0.0076290\n",
      "[168,    11] train loss: 0.0072019\n",
      "[168,    16] train loss: 0.0071015\n",
      "[168,    20] train loss: 0.0076108\n",
      "Epoch #168\n",
      "[169,     1] train loss: 0.0083225\n",
      "[169,     6] train loss: 0.0076156\n",
      "[169,    11] train loss: 0.0074175\n",
      "[169,    16] train loss: 0.0076260\n",
      "[169,    20] train loss: 0.0068778\n",
      "Epoch #169\n",
      "[170,     1] train loss: 0.0084720\n",
      "[170,     6] train loss: 0.0071648\n",
      "[170,    11] train loss: 0.0070910\n",
      "[170,    16] train loss: 0.0075969\n",
      "[170,    20] train loss: 0.0075832\n",
      "Epoch #170\n",
      "[171,     1] train loss: 0.0096479\n",
      "[171,     6] train loss: 0.0075851\n",
      "[171,    11] train loss: 0.0073538\n",
      "[171,    16] train loss: 0.0073418\n",
      "[171,    20] train loss: 0.0067092\n",
      "Epoch #171\n",
      "[172,     1] train loss: 0.0085040\n",
      "[172,     6] train loss: 0.0073728\n",
      "[172,    11] train loss: 0.0072260\n",
      "[172,    16] train loss: 0.0078092\n",
      "[172,    20] train loss: 0.0070395\n",
      "Epoch #172\n",
      "[173,     1] train loss: 0.0084829\n",
      "[173,     6] train loss: 0.0070786\n",
      "[173,    11] train loss: 0.0073671\n",
      "[173,    16] train loss: 0.0073453\n",
      "[173,    20] train loss: 0.0074657\n",
      "Epoch #173\n",
      "[174,     1] train loss: 0.0090357\n",
      "[174,     6] train loss: 0.0070822\n",
      "[174,    11] train loss: 0.0075693\n",
      "[174,    16] train loss: 0.0078371\n",
      "[174,    20] train loss: 0.0070543\n",
      "Epoch #174\n",
      "[175,     1] train loss: 0.0095653\n",
      "[175,     6] train loss: 0.0075897\n",
      "[175,    11] train loss: 0.0075441\n",
      "[175,    16] train loss: 0.0073079\n",
      "[175,    20] train loss: 0.0066516\n",
      "Epoch #175\n",
      "[176,     1] train loss: 0.0086297\n",
      "[176,     6] train loss: 0.0072085\n",
      "[176,    11] train loss: 0.0071648\n",
      "[176,    16] train loss: 0.0080326\n",
      "[176,    20] train loss: 0.0074030\n",
      "Epoch #176\n",
      "[177,     1] train loss: 0.0077322\n",
      "[177,     6] train loss: 0.0079413\n",
      "[177,    11] train loss: 0.0069544\n",
      "[177,    16] train loss: 0.0076037\n",
      "[177,    20] train loss: 0.0073363\n",
      "Epoch #177\n",
      "[178,     1] train loss: 0.0080175\n",
      "[178,     6] train loss: 0.0076899\n",
      "[178,    11] train loss: 0.0076870\n",
      "[178,    16] train loss: 0.0073559\n",
      "[178,    20] train loss: 0.0072861\n",
      "Epoch #178\n",
      "[179,     1] train loss: 0.0079693\n",
      "[179,     6] train loss: 0.0077117\n",
      "[179,    11] train loss: 0.0073439\n",
      "[179,    16] train loss: 0.0074606\n",
      "[179,    20] train loss: 0.0070906\n",
      "Epoch #179\n",
      "[180,     1] train loss: 0.0095589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180,     6] train loss: 0.0072911\n",
      "[180,    11] train loss: 0.0071435\n",
      "[180,    16] train loss: 0.0072528\n",
      "[180,    20] train loss: 0.0074458\n",
      "Epoch #180\n",
      "[181,     1] train loss: 0.0092639\n",
      "[181,     6] train loss: 0.0073161\n",
      "[181,    11] train loss: 0.0074286\n",
      "[181,    16] train loss: 0.0073835\n",
      "[181,    20] train loss: 0.0072694\n",
      "Epoch #181\n",
      "[182,     1] train loss: 0.0089949\n",
      "[182,     6] train loss: 0.0074788\n",
      "[182,    11] train loss: 0.0071559\n",
      "[182,    16] train loss: 0.0074741\n",
      "[182,    20] train loss: 0.0072702\n",
      "Epoch #182\n",
      "[183,     1] train loss: 0.0095482\n",
      "[183,     6] train loss: 0.0070294\n",
      "[183,    11] train loss: 0.0077539\n",
      "[183,    16] train loss: 0.0073521\n",
      "[183,    20] train loss: 0.0070433\n",
      "Epoch #183\n",
      "[184,     1] train loss: 0.0099987\n",
      "[184,     6] train loss: 0.0075542\n",
      "[184,    11] train loss: 0.0070770\n",
      "[184,    16] train loss: 0.0072769\n",
      "[184,    20] train loss: 0.0072837\n",
      "Epoch #184\n",
      "[185,     1] train loss: 0.0080695\n",
      "[185,     6] train loss: 0.0071651\n",
      "[185,    11] train loss: 0.0075241\n",
      "[185,    16] train loss: 0.0075449\n",
      "[185,    20] train loss: 0.0071094\n",
      "Epoch #185\n",
      "[186,     1] train loss: 0.0084111\n",
      "[186,     6] train loss: 0.0072730\n",
      "[186,    11] train loss: 0.0073007\n",
      "[186,    16] train loss: 0.0073861\n",
      "[186,    20] train loss: 0.0071587\n",
      "Epoch #186\n",
      "[187,     1] train loss: 0.0082839\n",
      "[187,     6] train loss: 0.0072611\n",
      "[187,    11] train loss: 0.0076920\n",
      "[187,    16] train loss: 0.0073313\n",
      "[187,    20] train loss: 0.0069026\n",
      "Epoch #187\n",
      "[188,     1] train loss: 0.0083490\n",
      "[188,     6] train loss: 0.0074882\n",
      "[188,    11] train loss: 0.0073783\n",
      "[188,    16] train loss: 0.0077043\n",
      "[188,    20] train loss: 0.0069992\n",
      "Epoch #188\n",
      "[189,     1] train loss: 0.0084291\n",
      "[189,     6] train loss: 0.0073465\n",
      "[189,    11] train loss: 0.0075088\n",
      "[189,    16] train loss: 0.0075985\n",
      "[189,    20] train loss: 0.0071832\n",
      "Epoch #189\n",
      "[190,     1] train loss: 0.0085164\n",
      "[190,     6] train loss: 0.0074888\n",
      "[190,    11] train loss: 0.0069191\n",
      "[190,    16] train loss: 0.0075750\n",
      "[190,    20] train loss: 0.0072324\n",
      "Epoch #190\n",
      "[191,     1] train loss: 0.0088355\n",
      "[191,     6] train loss: 0.0072667\n",
      "[191,    11] train loss: 0.0073182\n",
      "[191,    16] train loss: 0.0075374\n",
      "[191,    20] train loss: 0.0074158\n",
      "Epoch #191\n",
      "[192,     1] train loss: 0.0090219\n",
      "[192,     6] train loss: 0.0070585\n",
      "[192,    11] train loss: 0.0070503\n",
      "[192,    16] train loss: 0.0077899\n",
      "[192,    20] train loss: 0.0073904\n",
      "Epoch #192\n",
      "[193,     1] train loss: 0.0076392\n",
      "[193,     6] train loss: 0.0076244\n",
      "[193,    11] train loss: 0.0070381\n",
      "[193,    16] train loss: 0.0074477\n",
      "[193,    20] train loss: 0.0071694\n",
      "Epoch #193\n",
      "[194,     1] train loss: 0.0086925\n",
      "[194,     6] train loss: 0.0071406\n",
      "[194,    11] train loss: 0.0076581\n",
      "[194,    16] train loss: 0.0073269\n",
      "[194,    20] train loss: 0.0071661\n",
      "Epoch #194\n",
      "[195,     1] train loss: 0.0079659\n",
      "[195,     6] train loss: 0.0078731\n",
      "[195,    11] train loss: 0.0068052\n",
      "[195,    16] train loss: 0.0077333\n",
      "[195,    20] train loss: 0.0070072\n",
      "Epoch #195\n",
      "[196,     1] train loss: 0.0092870\n",
      "[196,     6] train loss: 0.0075259\n",
      "[196,    11] train loss: 0.0071204\n",
      "[196,    16] train loss: 0.0080015\n",
      "[196,    20] train loss: 0.0070413\n",
      "Epoch #196\n",
      "[197,     1] train loss: 0.0086650\n",
      "[197,     6] train loss: 0.0074700\n",
      "[197,    11] train loss: 0.0070312\n",
      "[197,    16] train loss: 0.0071672\n",
      "[197,    20] train loss: 0.0073828\n",
      "Epoch #197\n",
      "[198,     1] train loss: 0.0085266\n",
      "[198,     6] train loss: 0.0075499\n",
      "[198,    11] train loss: 0.0072916\n",
      "[198,    16] train loss: 0.0073771\n",
      "[198,    20] train loss: 0.0071477\n",
      "Epoch #198\n",
      "[199,     1] train loss: 0.0091841\n",
      "[199,     6] train loss: 0.0070946\n",
      "[199,    11] train loss: 0.0072635\n",
      "[199,    16] train loss: 0.0076190\n",
      "[199,    20] train loss: 0.0075879\n",
      "Epoch #199\n",
      "[200,     1] train loss: 0.0081601\n",
      "[200,     6] train loss: 0.0074542\n",
      "[200,    11] train loss: 0.0075924\n",
      "[200,    16] train loss: 0.0070952\n",
      "[200,    20] train loss: 0.0071122\n",
      "Epoch #200\n",
      "[201,     1] train loss: 0.0087256\n",
      "[201,     6] train loss: 0.0072293\n",
      "[201,    11] train loss: 0.0076772\n",
      "[201,    16] train loss: 0.0070411\n",
      "[201,    20] train loss: 0.0073603\n",
      "Epoch #201\n",
      "[202,     1] train loss: 0.0086860\n",
      "[202,     6] train loss: 0.0075914\n",
      "[202,    11] train loss: 0.0072156\n",
      "[202,    16] train loss: 0.0074786\n",
      "[202,    20] train loss: 0.0072150\n",
      "Epoch #202\n",
      "[203,     1] train loss: 0.0084578\n",
      "[203,     6] train loss: 0.0073435\n",
      "[203,    11] train loss: 0.0073935\n",
      "[203,    16] train loss: 0.0077196\n",
      "[203,    20] train loss: 0.0071236\n",
      "Epoch #203\n",
      "[204,     1] train loss: 0.0083966\n",
      "[204,     6] train loss: 0.0070655\n",
      "[204,    11] train loss: 0.0077069\n",
      "[204,    16] train loss: 0.0076153\n",
      "[204,    20] train loss: 0.0070349\n",
      "Epoch #204\n",
      "[205,     1] train loss: 0.0083409\n",
      "[205,     6] train loss: 0.0074657\n",
      "[205,    11] train loss: 0.0073069\n",
      "[205,    16] train loss: 0.0077216\n",
      "[205,    20] train loss: 0.0073055\n",
      "Epoch #205\n",
      "[206,     1] train loss: 0.0093082\n",
      "[206,     6] train loss: 0.0072939\n",
      "[206,    11] train loss: 0.0075662\n",
      "[206,    16] train loss: 0.0072023\n",
      "[206,    20] train loss: 0.0068855\n",
      "Epoch #206\n",
      "[207,     1] train loss: 0.0076006\n",
      "[207,     6] train loss: 0.0071379\n",
      "[207,    11] train loss: 0.0075486\n",
      "[207,    16] train loss: 0.0073354\n",
      "[207,    20] train loss: 0.0073526\n",
      "Epoch #207\n",
      "[208,     1] train loss: 0.0081150\n",
      "[208,     6] train loss: 0.0078622\n",
      "[208,    11] train loss: 0.0072263\n",
      "[208,    16] train loss: 0.0075659\n",
      "[208,    20] train loss: 0.0072296\n",
      "Epoch #208\n",
      "[209,     1] train loss: 0.0093551\n",
      "[209,     6] train loss: 0.0072078\n",
      "[209,    11] train loss: 0.0072239\n",
      "[209,    16] train loss: 0.0073750\n",
      "[209,    20] train loss: 0.0072385\n",
      "Epoch #209\n",
      "[210,     1] train loss: 0.0080519\n",
      "[210,     6] train loss: 0.0072394\n",
      "[210,    11] train loss: 0.0073123\n",
      "[210,    16] train loss: 0.0075244\n",
      "[210,    20] train loss: 0.0069146\n",
      "Epoch #210\n",
      "[211,     1] train loss: 0.0083007\n",
      "[211,     6] train loss: 0.0073868\n",
      "[211,    11] train loss: 0.0072494\n",
      "[211,    16] train loss: 0.0078981\n",
      "[211,    20] train loss: 0.0069195\n",
      "Epoch #211\n",
      "[212,     1] train loss: 0.0091261\n",
      "[212,     6] train loss: 0.0073629\n",
      "[212,    11] train loss: 0.0074080\n",
      "[212,    16] train loss: 0.0072812\n",
      "[212,    20] train loss: 0.0073294\n",
      "Epoch #212\n",
      "[213,     1] train loss: 0.0078904\n",
      "[213,     6] train loss: 0.0071917\n",
      "[213,    11] train loss: 0.0079642\n",
      "[213,    16] train loss: 0.0073362\n",
      "[213,    20] train loss: 0.0072027\n",
      "Epoch #213\n",
      "[214,     1] train loss: 0.0091971\n",
      "[214,     6] train loss: 0.0073423\n",
      "[214,    11] train loss: 0.0075939\n",
      "[214,    16] train loss: 0.0075001\n",
      "[214,    20] train loss: 0.0068953\n",
      "Epoch #214\n",
      "[215,     1] train loss: 0.0081917\n",
      "[215,     6] train loss: 0.0074547\n",
      "[215,    11] train loss: 0.0075790\n",
      "[215,    16] train loss: 0.0070498\n",
      "[215,    20] train loss: 0.0074347\n",
      "Epoch #215\n",
      "[216,     1] train loss: 0.0094249\n",
      "[216,     6] train loss: 0.0074774\n",
      "[216,    11] train loss: 0.0073995\n",
      "[216,    16] train loss: 0.0072776\n",
      "[216,    20] train loss: 0.0071771\n",
      "Epoch #216\n",
      "[217,     1] train loss: 0.0090856\n",
      "[217,     6] train loss: 0.0072975\n",
      "[217,    11] train loss: 0.0070515\n",
      "[217,    16] train loss: 0.0073405\n",
      "[217,    20] train loss: 0.0072331\n",
      "Epoch #217\n",
      "[218,     1] train loss: 0.0079152\n",
      "[218,     6] train loss: 0.0074529\n",
      "[218,    11] train loss: 0.0071409\n",
      "[218,    16] train loss: 0.0074140\n",
      "[218,    20] train loss: 0.0069921\n",
      "Epoch #218\n",
      "[219,     1] train loss: 0.0083045\n",
      "[219,     6] train loss: 0.0076831\n",
      "[219,    11] train loss: 0.0072695\n",
      "[219,    16] train loss: 0.0073354\n",
      "[219,    20] train loss: 0.0069093\n",
      "Epoch #219\n",
      "[220,     1] train loss: 0.0086949\n",
      "[220,     6] train loss: 0.0075922\n",
      "[220,    11] train loss: 0.0069309\n",
      "[220,    16] train loss: 0.0076690\n",
      "[220,    20] train loss: 0.0071147\n",
      "Epoch #220\n",
      "[221,     1] train loss: 0.0083255\n",
      "[221,     6] train loss: 0.0073961\n",
      "[221,    11] train loss: 0.0071249\n",
      "[221,    16] train loss: 0.0075774\n",
      "[221,    20] train loss: 0.0070370\n",
      "Epoch #221\n",
      "[222,     1] train loss: 0.0077473\n",
      "[222,     6] train loss: 0.0075725\n",
      "[222,    11] train loss: 0.0071917\n",
      "[222,    16] train loss: 0.0073502\n",
      "[222,    20] train loss: 0.0068701\n",
      "Epoch #222\n",
      "[223,     1] train loss: 0.0087726\n",
      "[223,     6] train loss: 0.0071367\n",
      "[223,    11] train loss: 0.0072197\n",
      "[223,    16] train loss: 0.0074942\n",
      "[223,    20] train loss: 0.0073704\n",
      "Epoch #223\n",
      "[224,     1] train loss: 0.0092239\n",
      "[224,     6] train loss: 0.0070355\n",
      "[224,    11] train loss: 0.0073454\n",
      "[224,    16] train loss: 0.0076643\n",
      "[224,    20] train loss: 0.0072430\n",
      "Epoch #224\n",
      "[225,     1] train loss: 0.0083331\n",
      "[225,     6] train loss: 0.0075340\n",
      "[225,    11] train loss: 0.0072404\n",
      "[225,    16] train loss: 0.0075339\n",
      "[225,    20] train loss: 0.0070285\n",
      "Epoch #225\n",
      "[226,     1] train loss: 0.0078362\n",
      "[226,     6] train loss: 0.0073823\n",
      "[226,    11] train loss: 0.0069888\n",
      "[226,    16] train loss: 0.0071542\n",
      "[226,    20] train loss: 0.0076234\n",
      "Epoch #226\n",
      "[227,     1] train loss: 0.0087164\n",
      "[227,     6] train loss: 0.0074203\n",
      "[227,    11] train loss: 0.0073655\n",
      "[227,    16] train loss: 0.0073657\n",
      "[227,    20] train loss: 0.0068126\n",
      "Epoch #227\n",
      "[228,     1] train loss: 0.0090182\n",
      "[228,     6] train loss: 0.0068964\n",
      "[228,    11] train loss: 0.0072570\n",
      "[228,    16] train loss: 0.0074330\n",
      "[228,    20] train loss: 0.0073881\n",
      "Epoch #228\n",
      "[229,     1] train loss: 0.0081624\n",
      "[229,     6] train loss: 0.0073995\n",
      "[229,    11] train loss: 0.0071714\n",
      "[229,    16] train loss: 0.0073253\n",
      "[229,    20] train loss: 0.0070386\n",
      "Epoch #229\n",
      "[230,     1] train loss: 0.0092688\n",
      "[230,     6] train loss: 0.0070325\n",
      "[230,    11] train loss: 0.0067302\n",
      "[230,    16] train loss: 0.0074286\n",
      "[230,    20] train loss: 0.0073875\n",
      "Epoch #230\n",
      "[231,     1] train loss: 0.0086705\n",
      "[231,     6] train loss: 0.0069429\n",
      "[231,    11] train loss: 0.0076436\n",
      "[231,    16] train loss: 0.0072958\n",
      "[231,    20] train loss: 0.0070970\n",
      "Epoch #231\n",
      "[232,     1] train loss: 0.0082588\n",
      "[232,     6] train loss: 0.0070565\n",
      "[232,    11] train loss: 0.0076541\n",
      "[232,    16] train loss: 0.0074303\n",
      "[232,    20] train loss: 0.0070023\n",
      "Epoch #232\n",
      "[233,     1] train loss: 0.0082147\n",
      "[233,     6] train loss: 0.0073928\n",
      "[233,    11] train loss: 0.0073887\n",
      "[233,    16] train loss: 0.0072813\n",
      "[233,    20] train loss: 0.0069388\n",
      "Epoch #233\n",
      "[234,     1] train loss: 0.0089549\n",
      "[234,     6] train loss: 0.0069561\n",
      "[234,    11] train loss: 0.0071995\n",
      "[234,    16] train loss: 0.0073955\n",
      "[234,    20] train loss: 0.0071483\n",
      "Epoch #234\n",
      "[235,     1] train loss: 0.0085672\n",
      "[235,     6] train loss: 0.0071936\n",
      "[235,    11] train loss: 0.0075496\n",
      "[235,    16] train loss: 0.0074312\n",
      "[235,    20] train loss: 0.0068548\n",
      "Epoch #235\n",
      "[236,     1] train loss: 0.0090719\n",
      "[236,     6] train loss: 0.0071154\n",
      "[236,    11] train loss: 0.0075627\n",
      "[236,    16] train loss: 0.0069923\n",
      "[236,    20] train loss: 0.0073050\n",
      "Epoch #236\n",
      "[237,     1] train loss: 0.0086545\n",
      "[237,     6] train loss: 0.0070201\n",
      "[237,    11] train loss: 0.0073764\n",
      "[237,    16] train loss: 0.0077807\n",
      "[237,    20] train loss: 0.0066816\n",
      "Epoch #237\n",
      "[238,     1] train loss: 0.0079574\n",
      "[238,     6] train loss: 0.0071747\n",
      "[238,    11] train loss: 0.0069845\n",
      "[238,    16] train loss: 0.0075624\n",
      "[238,    20] train loss: 0.0071077\n",
      "Epoch #238\n",
      "[239,     1] train loss: 0.0090136\n",
      "[239,     6] train loss: 0.0071451\n",
      "[239,    11] train loss: 0.0076803\n",
      "[239,    16] train loss: 0.0074070\n",
      "[239,    20] train loss: 0.0071762\n",
      "Epoch #239\n",
      "[240,     1] train loss: 0.0089388\n",
      "[240,     6] train loss: 0.0073801\n",
      "[240,    11] train loss: 0.0072496\n",
      "[240,    16] train loss: 0.0075530\n",
      "[240,    20] train loss: 0.0066400\n",
      "Epoch #240\n",
      "[241,     1] train loss: 0.0079775\n",
      "[241,     6] train loss: 0.0072533\n",
      "[241,    11] train loss: 0.0076153\n",
      "[241,    16] train loss: 0.0075665\n",
      "[241,    20] train loss: 0.0068526\n",
      "Epoch #241\n",
      "[242,     1] train loss: 0.0090086\n",
      "[242,     6] train loss: 0.0071672\n",
      "[242,    11] train loss: 0.0075646\n",
      "[242,    16] train loss: 0.0075578\n",
      "[242,    20] train loss: 0.0068655\n",
      "Epoch #242\n",
      "[243,     1] train loss: 0.0079864\n",
      "[243,     6] train loss: 0.0073296\n",
      "[243,    11] train loss: 0.0072827\n",
      "[243,    16] train loss: 0.0073529\n",
      "[243,    20] train loss: 0.0071197\n",
      "Epoch #243\n",
      "[244,     1] train loss: 0.0101496\n",
      "[244,     6] train loss: 0.0074689\n",
      "[244,    11] train loss: 0.0074380\n",
      "[244,    16] train loss: 0.0071986\n",
      "[244,    20] train loss: 0.0069443\n",
      "Epoch #244\n",
      "[245,     1] train loss: 0.0099503\n",
      "[245,     6] train loss: 0.0072352\n",
      "[245,    11] train loss: 0.0077584\n",
      "[245,    16] train loss: 0.0071594\n",
      "[245,    20] train loss: 0.0065572\n",
      "Epoch #245\n",
      "[246,     1] train loss: 0.0096027\n",
      "[246,     6] train loss: 0.0071512\n",
      "[246,    11] train loss: 0.0072405\n",
      "[246,    16] train loss: 0.0076476\n",
      "[246,    20] train loss: 0.0070159\n",
      "Epoch #246\n",
      "[247,     1] train loss: 0.0088598\n",
      "[247,     6] train loss: 0.0071024\n",
      "[247,    11] train loss: 0.0072669\n",
      "[247,    16] train loss: 0.0072935\n",
      "[247,    20] train loss: 0.0070178\n",
      "Epoch #247\n",
      "[248,     1] train loss: 0.0096188\n",
      "[248,     6] train loss: 0.0072353\n",
      "[248,    11] train loss: 0.0073664\n",
      "[248,    16] train loss: 0.0070230\n",
      "[248,    20] train loss: 0.0070061\n",
      "Epoch #248\n",
      "[249,     1] train loss: 0.0092625\n",
      "[249,     6] train loss: 0.0074626\n",
      "[249,    11] train loss: 0.0068753\n",
      "[249,    16] train loss: 0.0073529\n",
      "[249,    20] train loss: 0.0071174\n",
      "Epoch #249\n",
      "[250,     1] train loss: 0.0095188\n",
      "[250,     6] train loss: 0.0070871\n",
      "[250,    11] train loss: 0.0073663\n",
      "[250,    16] train loss: 0.0074475\n",
      "[250,    20] train loss: 0.0072187\n",
      "Epoch #250\n",
      "[251,     1] train loss: 0.0084161\n",
      "[251,     6] train loss: 0.0072652\n",
      "[251,    11] train loss: 0.0070769\n",
      "[251,    16] train loss: 0.0074378\n",
      "[251,    20] train loss: 0.0075732\n",
      "Epoch #251\n",
      "[252,     1] train loss: 0.0095527\n",
      "[252,     6] train loss: 0.0070241\n",
      "[252,    11] train loss: 0.0074342\n",
      "[252,    16] train loss: 0.0076445\n",
      "[252,    20] train loss: 0.0070263\n",
      "Epoch #252\n",
      "[253,     1] train loss: 0.0076205\n",
      "[253,     6] train loss: 0.0071014\n",
      "[253,    11] train loss: 0.0072570\n",
      "[253,    16] train loss: 0.0073831\n",
      "[253,    20] train loss: 0.0073956\n",
      "Epoch #253\n",
      "[254,     1] train loss: 0.0075251\n",
      "[254,     6] train loss: 0.0072980\n",
      "[254,    11] train loss: 0.0074261\n",
      "[254,    16] train loss: 0.0073223\n",
      "[254,    20] train loss: 0.0069248\n",
      "Epoch #254\n",
      "[255,     1] train loss: 0.0094626\n",
      "[255,     6] train loss: 0.0069487\n",
      "[255,    11] train loss: 0.0071748\n",
      "[255,    16] train loss: 0.0074306\n",
      "[255,    20] train loss: 0.0068381\n",
      "Epoch #255\n",
      "[256,     1] train loss: 0.0085368\n",
      "[256,     6] train loss: 0.0072031\n",
      "[256,    11] train loss: 0.0070833\n",
      "[256,    16] train loss: 0.0069766\n",
      "[256,    20] train loss: 0.0078993\n",
      "Epoch #256\n",
      "[257,     1] train loss: 0.0084180\n",
      "[257,     6] train loss: 0.0071819\n",
      "[257,    11] train loss: 0.0077910\n",
      "[257,    16] train loss: 0.0074633\n",
      "[257,    20] train loss: 0.0071095\n",
      "Epoch #257\n",
      "[258,     1] train loss: 0.0076843\n",
      "[258,     6] train loss: 0.0075951\n",
      "[258,    11] train loss: 0.0072050\n",
      "[258,    16] train loss: 0.0078257\n",
      "[258,    20] train loss: 0.0064768\n",
      "Epoch #258\n",
      "[259,     1] train loss: 0.0079775\n",
      "[259,     6] train loss: 0.0070018\n",
      "[259,    11] train loss: 0.0076645\n",
      "[259,    16] train loss: 0.0074906\n",
      "[259,    20] train loss: 0.0071365\n",
      "Epoch #259\n",
      "[260,     1] train loss: 0.0088424\n",
      "[260,     6] train loss: 0.0070362\n",
      "[260,    11] train loss: 0.0071714\n",
      "[260,    16] train loss: 0.0073504\n",
      "[260,    20] train loss: 0.0072483\n",
      "Epoch #260\n",
      "[261,     1] train loss: 0.0087399\n",
      "[261,     6] train loss: 0.0070272\n",
      "[261,    11] train loss: 0.0070248\n",
      "[261,    16] train loss: 0.0072356\n",
      "[261,    20] train loss: 0.0074837\n",
      "Epoch #261\n",
      "[262,     1] train loss: 0.0075641\n",
      "[262,     6] train loss: 0.0073993\n",
      "[262,    11] train loss: 0.0072178\n",
      "[262,    16] train loss: 0.0072910\n",
      "[262,    20] train loss: 0.0070956\n",
      "Epoch #262\n",
      "[263,     1] train loss: 0.0087580\n",
      "[263,     6] train loss: 0.0068924\n",
      "[263,    11] train loss: 0.0070653\n",
      "[263,    16] train loss: 0.0072135\n",
      "[263,    20] train loss: 0.0076949\n",
      "Epoch #263\n",
      "[264,     1] train loss: 0.0084217\n",
      "[264,     6] train loss: 0.0073117\n",
      "[264,    11] train loss: 0.0073447\n",
      "[264,    16] train loss: 0.0074109\n",
      "[264,    20] train loss: 0.0070598\n",
      "Epoch #264\n",
      "[265,     1] train loss: 0.0086255\n",
      "[265,     6] train loss: 0.0070083\n",
      "[265,    11] train loss: 0.0068879\n",
      "[265,    16] train loss: 0.0078224\n",
      "[265,    20] train loss: 0.0069146\n",
      "Epoch #265\n",
      "[266,     1] train loss: 0.0099624\n",
      "[266,     6] train loss: 0.0070967\n",
      "[266,    11] train loss: 0.0073111\n",
      "[266,    16] train loss: 0.0073449\n",
      "[266,    20] train loss: 0.0069827\n",
      "Epoch #266\n",
      "[267,     1] train loss: 0.0095776\n",
      "[267,     6] train loss: 0.0070731\n",
      "[267,    11] train loss: 0.0075643\n",
      "[267,    16] train loss: 0.0073912\n",
      "[267,    20] train loss: 0.0066799\n",
      "Epoch #267\n",
      "[268,     1] train loss: 0.0076765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[268,     6] train loss: 0.0071230\n",
      "[268,    11] train loss: 0.0071273\n",
      "[268,    16] train loss: 0.0074668\n",
      "[268,    20] train loss: 0.0077720\n",
      "Epoch #268\n",
      "[269,     1] train loss: 0.0087029\n",
      "[269,     6] train loss: 0.0070597\n",
      "[269,    11] train loss: 0.0077330\n",
      "[269,    16] train loss: 0.0072506\n",
      "[269,    20] train loss: 0.0071090\n",
      "Epoch #269\n",
      "[270,     1] train loss: 0.0086701\n",
      "[270,     6] train loss: 0.0071551\n",
      "[270,    11] train loss: 0.0075750\n",
      "[270,    16] train loss: 0.0073999\n",
      "[270,    20] train loss: 0.0070893\n",
      "Epoch #270\n",
      "[271,     1] train loss: 0.0084086\n",
      "[271,     6] train loss: 0.0069778\n",
      "[271,    11] train loss: 0.0073704\n",
      "[271,    16] train loss: 0.0075994\n",
      "[271,    20] train loss: 0.0071537\n",
      "Epoch #271\n",
      "[272,     1] train loss: 0.0081101\n",
      "[272,     6] train loss: 0.0073887\n",
      "[272,    11] train loss: 0.0069931\n",
      "[272,    16] train loss: 0.0074004\n",
      "[272,    20] train loss: 0.0072733\n",
      "Epoch #272\n",
      "[273,     1] train loss: 0.0091347\n",
      "[273,     6] train loss: 0.0072603\n",
      "[273,    11] train loss: 0.0070348\n",
      "[273,    16] train loss: 0.0072119\n",
      "[273,    20] train loss: 0.0070991\n",
      "Epoch #273\n",
      "[274,     1] train loss: 0.0078376\n",
      "[274,     6] train loss: 0.0071852\n",
      "[274,    11] train loss: 0.0072295\n",
      "[274,    16] train loss: 0.0075337\n",
      "[274,    20] train loss: 0.0073224\n",
      "Epoch #274\n",
      "[275,     1] train loss: 0.0089812\n",
      "[275,     6] train loss: 0.0073927\n",
      "[275,    11] train loss: 0.0070094\n",
      "[275,    16] train loss: 0.0069886\n",
      "[275,    20] train loss: 0.0077135\n",
      "Epoch #275\n",
      "[276,     1] train loss: 0.0078142\n",
      "[276,     6] train loss: 0.0071511\n",
      "[276,    11] train loss: 0.0073609\n",
      "[276,    16] train loss: 0.0070283\n",
      "[276,    20] train loss: 0.0070920\n",
      "Epoch #276\n",
      "[277,     1] train loss: 0.0095858\n",
      "[277,     6] train loss: 0.0074449\n",
      "[277,    11] train loss: 0.0074812\n",
      "[277,    16] train loss: 0.0071153\n",
      "[277,    20] train loss: 0.0068569\n",
      "Epoch #277\n",
      "[278,     1] train loss: 0.0090972\n",
      "[278,     6] train loss: 0.0070749\n",
      "[278,    11] train loss: 0.0074753\n",
      "[278,    16] train loss: 0.0074317\n",
      "[278,    20] train loss: 0.0070795\n",
      "Epoch #278\n",
      "[279,     1] train loss: 0.0083126\n",
      "[279,     6] train loss: 0.0067612\n",
      "[279,    11] train loss: 0.0071016\n",
      "[279,    16] train loss: 0.0072316\n",
      "[279,    20] train loss: 0.0074797\n",
      "Epoch #279\n",
      "[280,     1] train loss: 0.0089316\n",
      "[280,     6] train loss: 0.0071744\n",
      "[280,    11] train loss: 0.0068018\n",
      "[280,    16] train loss: 0.0076203\n",
      "[280,    20] train loss: 0.0070694\n",
      "Epoch #280\n",
      "[281,     1] train loss: 0.0081608\n",
      "[281,     6] train loss: 0.0072831\n",
      "[281,    11] train loss: 0.0070796\n",
      "[281,    16] train loss: 0.0073752\n",
      "[281,    20] train loss: 0.0071493\n",
      "Epoch #281\n",
      "[282,     1] train loss: 0.0092902\n",
      "[282,     6] train loss: 0.0069238\n",
      "[282,    11] train loss: 0.0076014\n",
      "[282,    16] train loss: 0.0072669\n",
      "[282,    20] train loss: 0.0069042\n",
      "Epoch #282\n",
      "[283,     1] train loss: 0.0088369\n",
      "[283,     6] train loss: 0.0077285\n",
      "[283,    11] train loss: 0.0073144\n",
      "[283,    16] train loss: 0.0071798\n",
      "[283,    20] train loss: 0.0068421\n",
      "Epoch #283\n",
      "[284,     1] train loss: 0.0084783\n",
      "[284,     6] train loss: 0.0079731\n",
      "[284,    11] train loss: 0.0069964\n",
      "[284,    16] train loss: 0.0069261\n",
      "[284,    20] train loss: 0.0071355\n",
      "Epoch #284\n",
      "[285,     1] train loss: 0.0089386\n",
      "[285,     6] train loss: 0.0071969\n",
      "[285,    11] train loss: 0.0070352\n",
      "[285,    16] train loss: 0.0074331\n",
      "[285,    20] train loss: 0.0069932\n",
      "Epoch #285\n",
      "[286,     1] train loss: 0.0079813\n",
      "[286,     6] train loss: 0.0067350\n",
      "[286,    11] train loss: 0.0076280\n",
      "[286,    16] train loss: 0.0078582\n",
      "[286,    20] train loss: 0.0069296\n",
      "Epoch #286\n",
      "[287,     1] train loss: 0.0084740\n",
      "[287,     6] train loss: 0.0072674\n",
      "[287,    11] train loss: 0.0076735\n",
      "[287,    16] train loss: 0.0071279\n",
      "[287,    20] train loss: 0.0067490\n",
      "Epoch #287\n",
      "[288,     1] train loss: 0.0091571\n",
      "[288,     6] train loss: 0.0073566\n",
      "[288,    11] train loss: 0.0068462\n",
      "[288,    16] train loss: 0.0076312\n",
      "[288,    20] train loss: 0.0068698\n",
      "Epoch #288\n",
      "[289,     1] train loss: 0.0081342\n",
      "[289,     6] train loss: 0.0071156\n",
      "[289,    11] train loss: 0.0071619\n",
      "[289,    16] train loss: 0.0074777\n",
      "[289,    20] train loss: 0.0071354\n",
      "Epoch #289\n",
      "[290,     1] train loss: 0.0093982\n",
      "[290,     6] train loss: 0.0072388\n",
      "[290,    11] train loss: 0.0074035\n",
      "[290,    16] train loss: 0.0074686\n",
      "[290,    20] train loss: 0.0068565\n",
      "Epoch #290\n",
      "[291,     1] train loss: 0.0086991\n",
      "[291,     6] train loss: 0.0072739\n",
      "[291,    11] train loss: 0.0072153\n",
      "[291,    16] train loss: 0.0070478\n",
      "[291,    20] train loss: 0.0073491\n",
      "Epoch #291\n",
      "[292,     1] train loss: 0.0080498\n",
      "[292,     6] train loss: 0.0066813\n",
      "[292,    11] train loss: 0.0074116\n",
      "[292,    16] train loss: 0.0071031\n",
      "[292,    20] train loss: 0.0074456\n",
      "Epoch #292\n",
      "[293,     1] train loss: 0.0083260\n",
      "[293,     6] train loss: 0.0070963\n",
      "[293,    11] train loss: 0.0071898\n",
      "[293,    16] train loss: 0.0074200\n",
      "[293,    20] train loss: 0.0071032\n",
      "Epoch #293\n",
      "[294,     1] train loss: 0.0086123\n",
      "[294,     6] train loss: 0.0070635\n",
      "[294,    11] train loss: 0.0072695\n",
      "[294,    16] train loss: 0.0072883\n",
      "[294,    20] train loss: 0.0066198\n",
      "Epoch #294\n",
      "[295,     1] train loss: 0.0087303\n",
      "[295,     6] train loss: 0.0070447\n",
      "[295,    11] train loss: 0.0072633\n",
      "[295,    16] train loss: 0.0071213\n",
      "[295,    20] train loss: 0.0071491\n",
      "Epoch #295\n",
      "[296,     1] train loss: 0.0088065\n",
      "[296,     6] train loss: 0.0069002\n",
      "[296,    11] train loss: 0.0074562\n",
      "[296,    16] train loss: 0.0072209\n",
      "[296,    20] train loss: 0.0068859\n",
      "Epoch #296\n",
      "[297,     1] train loss: 0.0083298\n",
      "[297,     6] train loss: 0.0068808\n",
      "[297,    11] train loss: 0.0070774\n",
      "[297,    16] train loss: 0.0070610\n",
      "[297,    20] train loss: 0.0075452\n",
      "Epoch #297\n",
      "[298,     1] train loss: 0.0083262\n",
      "[298,     6] train loss: 0.0069441\n",
      "[298,    11] train loss: 0.0072934\n",
      "[298,    16] train loss: 0.0072629\n",
      "[298,    20] train loss: 0.0070551\n",
      "Epoch #298\n",
      "[299,     1] train loss: 0.0085201\n",
      "[299,     6] train loss: 0.0075867\n",
      "[299,    11] train loss: 0.0070759\n",
      "[299,    16] train loss: 0.0072783\n",
      "[299,    20] train loss: 0.0068301\n",
      "Epoch #299\n",
      "[300,     1] train loss: 0.0087490\n",
      "[300,     6] train loss: 0.0074669\n",
      "[300,    11] train loss: 0.0073353\n",
      "[300,    16] train loss: 0.0075960\n",
      "[300,    20] train loss: 0.0067520\n",
      "Epoch #300\n",
      "[301,     1] train loss: 0.0082941\n",
      "[301,     6] train loss: 0.0070783\n",
      "[301,    11] train loss: 0.0072004\n",
      "[301,    16] train loss: 0.0075335\n",
      "[301,    20] train loss: 0.0073774\n",
      "Epoch #301\n",
      "[302,     1] train loss: 0.0093372\n",
      "[302,     6] train loss: 0.0073565\n",
      "[302,    11] train loss: 0.0071136\n",
      "[302,    16] train loss: 0.0071047\n",
      "[302,    20] train loss: 0.0070688\n",
      "Epoch #302\n",
      "[303,     1] train loss: 0.0088630\n",
      "[303,     6] train loss: 0.0065992\n",
      "[303,    11] train loss: 0.0070064\n",
      "[303,    16] train loss: 0.0070968\n",
      "[303,    20] train loss: 0.0076315\n",
      "Epoch #303\n",
      "[304,     1] train loss: 0.0098567\n",
      "[304,     6] train loss: 0.0071576\n",
      "[304,    11] train loss: 0.0072939\n",
      "[304,    16] train loss: 0.0074241\n",
      "[304,    20] train loss: 0.0068493\n",
      "Epoch #304\n",
      "[305,     1] train loss: 0.0092952\n",
      "[305,     6] train loss: 0.0073893\n",
      "[305,    11] train loss: 0.0066913\n",
      "[305,    16] train loss: 0.0074211\n",
      "[305,    20] train loss: 0.0071142\n",
      "Epoch #305\n",
      "[306,     1] train loss: 0.0083330\n",
      "[306,     6] train loss: 0.0075445\n",
      "[306,    11] train loss: 0.0068115\n",
      "[306,    16] train loss: 0.0075970\n",
      "[306,    20] train loss: 0.0071399\n",
      "Epoch #306\n",
      "[307,     1] train loss: 0.0080075\n",
      "[307,     6] train loss: 0.0071973\n",
      "[307,    11] train loss: 0.0069028\n",
      "[307,    16] train loss: 0.0075899\n",
      "[307,    20] train loss: 0.0070457\n",
      "Epoch #307\n",
      "[308,     1] train loss: 0.0093226\n",
      "[308,     6] train loss: 0.0074834\n",
      "[308,    11] train loss: 0.0070368\n",
      "[308,    16] train loss: 0.0072560\n",
      "[308,    20] train loss: 0.0070121\n",
      "Epoch #308\n",
      "[309,     1] train loss: 0.0083859\n",
      "[309,     6] train loss: 0.0070106\n",
      "[309,    11] train loss: 0.0072220\n",
      "[309,    16] train loss: 0.0071676\n",
      "[309,    20] train loss: 0.0072595\n",
      "Epoch #309\n",
      "[310,     1] train loss: 0.0088954\n",
      "[310,     6] train loss: 0.0071434\n",
      "[310,    11] train loss: 0.0071886\n",
      "[310,    16] train loss: 0.0075420\n",
      "[310,    20] train loss: 0.0065084\n",
      "Epoch #310\n",
      "[311,     1] train loss: 0.0085498\n",
      "[311,     6] train loss: 0.0073435\n",
      "[311,    11] train loss: 0.0069647\n",
      "[311,    16] train loss: 0.0074882\n",
      "[311,    20] train loss: 0.0072723\n",
      "Epoch #311\n",
      "[312,     1] train loss: 0.0083441\n",
      "[312,     6] train loss: 0.0073987\n",
      "[312,    11] train loss: 0.0072591\n",
      "[312,    16] train loss: 0.0073580\n",
      "[312,    20] train loss: 0.0069389\n",
      "Epoch #312\n",
      "[313,     1] train loss: 0.0085914\n",
      "[313,     6] train loss: 0.0072711\n",
      "[313,    11] train loss: 0.0074178\n",
      "[313,    16] train loss: 0.0075239\n",
      "[313,    20] train loss: 0.0068258\n",
      "Epoch #313\n",
      "[314,     1] train loss: 0.0079001\n",
      "[314,     6] train loss: 0.0069938\n",
      "[314,    11] train loss: 0.0069738\n",
      "[314,    16] train loss: 0.0075983\n",
      "[314,    20] train loss: 0.0071666\n",
      "Epoch #314\n",
      "[315,     1] train loss: 0.0090807\n",
      "[315,     6] train loss: 0.0072166\n",
      "[315,    11] train loss: 0.0073304\n",
      "[315,    16] train loss: 0.0074047\n",
      "[315,    20] train loss: 0.0070727\n",
      "Epoch #315\n",
      "[316,     1] train loss: 0.0099377\n",
      "[316,     6] train loss: 0.0070696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-37015b56e6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# loss = criterion(bass_outputs, batch_bass_train.long())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbass_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "#         batch_bass_train_raw = batch_bass_train_raw.transpose(0, 1)\n",
    "        # transpose нужен БЫЛ для обмена размерности батча и размерности шагов\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "batch_drum = drum_train + drum_test + drum_validation\n",
    "batch_bass = bass_train + bass_test + bass_validation\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(len(batch_drum)):\n",
    "        bass_outputs = dnb_lstm(batch_drum)\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.7).int()\n",
    "#         print(f\"i:{i}\")\n",
    "#         print(bass_outputs[i].shape)\n",
    "#         print(batch_drum[i].image.shape)\n",
    "            \n",
    "        img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , batch_drum[i].tempo\n",
    "                                , batch_drum[i].instrument\n",
    "                                , 1\n",
    "                                , batch_drum[i].min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдения:\n",
    "1. Нейронная сеть выдаёт по одной барабанной партии одну басовую партию. Следовательно, страдает разнообразие генерируемой музыки\n",
    "2. (следствие из 1) В датасете (скорее всего) имеются типичные ритмы, которым соответствуют абсолютно разные мелодии на разных инструментах. Это сильно сбивает с толку нейросеть. Как следствие, на уникальных ритмах нейросеть переобучается, а на типичных -- не понимает, какую мелодию сгенерировать\n",
    "\n",
    "Предложения:\n",
    "1. Придумать способ разделить пары (каким-то образом) в данной ситуации, возможно добавить жанр или ещё что..\n",
    "2. Разметить музыку тональностями -- это может упростить обучение нейросети\n",
    "3. Добавить случайность в латентное пространство"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация полифонической музыки с кондишнингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем torch и numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также пользовательский импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decode_patterns import data_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_height = 64\n",
    "drum_width = 14\n",
    "melody_width = 36\n",
    "data_width = drum_width + melody_width\n",
    "data_size = data_height*data_width\n",
    "patterns_file = \"decode_patterns/patterns.pairs.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "drum, bass = data_conversion.make_lstm_dataset_conditioning(height=data_height,\n",
    "                                                            limit=1000,\n",
    "                                                            patterns_file=patterns_file,\n",
    "                                                            mono=False)\n",
    "# print(drum[0])\n",
    "# drum, bass = np.array(drum), np.array(bass)\n",
    "# print(drum[0])\n",
    "\n",
    "# define shuffling of dataset\n",
    "def shuffle(A, B, p=0.8):\n",
    "    # take 80% to training, other to testing\n",
    "    AB = list(zip(A, B))\n",
    "    L = len(AB)\n",
    "    pivot = int(p*L)\n",
    "    random.shuffle(AB)\n",
    "    yield [p[0] for p in AB[:pivot]]\n",
    "    yield [p[1] for p in AB[:pivot]]\n",
    "    yield [p[0] for p in AB[pivot:]]\n",
    "    yield [p[1] for p in AB[pivot:]]\n",
    "    \n",
    "    \n",
    "# we can select here a validation set\n",
    "drum, bass, drum_validation, bass_validation = shuffle(drum, bass)\n",
    "    \n",
    "# and we can shuffle train and test set like this:\n",
    "# drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumpyImage(image=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), tempo=480, instrument=3, denominator=1, min_note=56)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель определим в самом простом варианте, который только можно себе представить -- как в примере с конечным автоматом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LSTM\n",
    "# Decoder = FCNN\n",
    "class DrumNBass_LSTM_to_FCNN(nn.Module):\n",
    "    def __init__(self, bass_height, bass_width):\n",
    "        super(DrumNBass_LSTM_to_FCNN, self).__init__()\n",
    "        # save data parameters\n",
    "        self.bass_height = bass_height\n",
    "        self.bass_width = bass_width\n",
    "        self.bass_size = bass_height*bass_width\n",
    "        self.condition_size = 2 # размер подмешиваемого conditioning\n",
    "        self.embedding_size = 1 # размер латентного пространства (на каждый отсчёт)\n",
    "                                # ЛУЧШЕ НЕ МЕНЯТЬ ЭТОТ ПАРАМЕТР С 1, придётся переписывать код!\n",
    "        # one input neuron, one output neuron, one layer in LSTM block\n",
    "        self.input_size = 14\n",
    "        self.lstm_hidden_size = 6\n",
    "        self.lstm_layer_count = 1\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, self.lstm_layer_count)\n",
    "        self.lstm_embed_layer = nn.Linear(self.lstm_hidden_size, self.embedding_size)\n",
    "        \n",
    "        self.decoder_layer1 = nn.Linear(self.bass_height + self.condition_size, 4)\n",
    "        self.decoder_layer2 = nn.Linear(4, 48)\n",
    "        self.decoder_layer3 = nn.Linear(48 + self.condition_size, 512)\n",
    "        self.decoder_layer4 = nn.Linear(512, self.bass_size)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        # пусть в input у нас приходит вектор размерности (32, 128, 14)\n",
    "        # где имеется 32 примера (минибатч) по 128 отсчётов, 14 значений в каждом (барабанная партия)\n",
    "        # Тогда его надо транспонировать в размерность (128, 32, 14)\n",
    "        input = input.transpose(0,1)\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.sigm(self.lstm_embed_layer(output))\n",
    "        return output\n",
    "    \n",
    "    def decoder(self, input, cond):\n",
    "        output = torch.cat((input, cond), axis=1) # добавляем conditioning\n",
    "        output = self.sigm(self.decoder_layer1(output))\n",
    "        output = self.sigm(self.decoder_layer2(output))\n",
    "        output = torch.cat((output, cond), axis=1) # добавляем ещё conditioning\n",
    "        output = self.sigm(self.decoder_layer3(output))\n",
    "        output = self.sigm(self.decoder_layer4(output))\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input):\n",
    "        images = torch.tensor(list(map(lambda p: p.image, input)), dtype=torch.float)\n",
    "        result = self.encoder(images)\n",
    "        # избавляемся от лишней размерности (embedding_size=1), чтобы получить вектор из lstm\n",
    "        # размером с высоту изображения\n",
    "        result = result.squeeze().transpose(0,1)\n",
    "        # добавляем conditioning\n",
    "        conditionings = torch.tensor(list(map(lambda p: [p.tempo, p.instrument], input)), dtype=torch.float)\n",
    "        conditionings = conditionings\n",
    "        result = self.decoder(result, conditionings)\n",
    "        return result.view((-1, self.bass_height, self.bass_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# часть обучения\n",
    "dnb_lstm = DrumNBass_LSTM_to_FCNN(data_height, melody_width)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# оценим также и разнообразие мелодии по её.. дисперсии?)\n",
    "# def melody_variety(melody):\n",
    "#     return 1/(1 + (melody.sum(axis=2) > 1).int())\n",
    "    \n",
    "# criterion = nn.NLLLoss() # -- этот товарищ требует, чтобы LSTM выдавал классы,\n",
    "# criterion = nn.CrossEntropyLoss() # и этот тоже\n",
    "# (числа от 0 до C-1), но как всё-таки его заставить это делать?...\n",
    "# optimizer = optim.SGD(dnb_lstm.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(dnb_lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель форвардится на один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnb_lstm.forward([drum_validation[16], drum_validation[14], drum_validation[43]]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 36)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bass_validation[16].image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найденные баги и их решения:\n",
    "\n",
    "https://stackoverflow.com/questions/56741087/how-to-fix-runtimeerror-expected-object-of-scalar-type-float-but-got-scalar-typ\n",
    "\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "https://stackoverflow.com/questions/56243672/expected-target-size-50-88-got-torch-size50-288-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_every_epoch is on\n",
      "Epoch #0\n",
      "[1,     1] train loss: 0.0089300\n",
      "[1,     6] train loss: 0.0079169\n",
      "[1,    11] train loss: 0.0079742\n",
      "[1,    16] train loss: 0.0081572\n",
      "[1,    20] train loss: 0.0074607\n",
      "Epoch #1\n",
      "[2,     1] train loss: 0.0101166\n",
      "[2,     6] train loss: 0.0080170\n",
      "[2,    11] train loss: 0.0083068\n",
      "[2,    16] train loss: 0.0080580\n",
      "[2,    20] train loss: 0.0074972\n",
      "Epoch #2\n",
      "[3,     1] train loss: 0.0096462\n",
      "[3,     6] train loss: 0.0078541\n",
      "[3,    11] train loss: 0.0077455\n",
      "[3,    16] train loss: 0.0085988\n",
      "[3,    20] train loss: 0.0077867\n",
      "Epoch #3\n",
      "[4,     1] train loss: 0.0099310\n",
      "[4,     6] train loss: 0.0079874\n",
      "[4,    11] train loss: 0.0082058\n",
      "[4,    16] train loss: 0.0076893\n",
      "[4,    20] train loss: 0.0073712\n",
      "Epoch #4\n",
      "[5,     1] train loss: 0.0085562\n",
      "[5,     6] train loss: 0.0080213\n",
      "[5,    11] train loss: 0.0078968\n",
      "[5,    16] train loss: 0.0077679\n",
      "[5,    20] train loss: 0.0079249\n",
      "Epoch #5\n",
      "[6,     1] train loss: 0.0098034\n",
      "[6,     6] train loss: 0.0082818\n",
      "[6,    11] train loss: 0.0079613\n",
      "[6,    16] train loss: 0.0083078\n",
      "[6,    20] train loss: 0.0074077\n",
      "Epoch #6\n",
      "[7,     1] train loss: 0.0098332\n",
      "[7,     6] train loss: 0.0077070\n",
      "[7,    11] train loss: 0.0081653\n",
      "[7,    16] train loss: 0.0081206\n",
      "[7,    20] train loss: 0.0076324\n",
      "Epoch #7\n",
      "[8,     1] train loss: 0.0093011\n",
      "[8,     6] train loss: 0.0081149\n",
      "[8,    11] train loss: 0.0076985\n",
      "[8,    16] train loss: 0.0082686\n",
      "[8,    20] train loss: 0.0077176\n",
      "Epoch #8\n",
      "[9,     1] train loss: 0.0089077\n",
      "[9,     6] train loss: 0.0079963\n",
      "[9,    11] train loss: 0.0078849\n",
      "[9,    16] train loss: 0.0080470\n",
      "[9,    20] train loss: 0.0080809\n",
      "Epoch #9\n",
      "[10,     1] train loss: 0.0088215\n",
      "[10,     6] train loss: 0.0080257\n",
      "[10,    11] train loss: 0.0078563\n",
      "[10,    16] train loss: 0.0082800\n",
      "[10,    20] train loss: 0.0076722\n",
      "Epoch #10\n",
      "[11,     1] train loss: 0.0096744\n",
      "[11,     6] train loss: 0.0076018\n",
      "[11,    11] train loss: 0.0078831\n",
      "[11,    16] train loss: 0.0076608\n",
      "[11,    20] train loss: 0.0077737\n",
      "Epoch #11\n",
      "[12,     1] train loss: 0.0107045\n",
      "[12,     6] train loss: 0.0077394\n",
      "[12,    11] train loss: 0.0076234\n",
      "[12,    16] train loss: 0.0081675\n",
      "[12,    20] train loss: 0.0076918\n",
      "Epoch #12\n",
      "[13,     1] train loss: 0.0101962\n",
      "[13,     6] train loss: 0.0081240\n",
      "[13,    11] train loss: 0.0078308\n",
      "[13,    16] train loss: 0.0078572\n",
      "[13,    20] train loss: 0.0073937\n",
      "Epoch #13\n",
      "[14,     1] train loss: 0.0083957\n",
      "[14,     6] train loss: 0.0083405\n",
      "[14,    11] train loss: 0.0082945\n",
      "[14,    16] train loss: 0.0078195\n",
      "[14,    20] train loss: 0.0075060\n",
      "Epoch #14\n",
      "[15,     1] train loss: 0.0088038\n",
      "[15,     6] train loss: 0.0077132\n",
      "[15,    11] train loss: 0.0081311\n",
      "[15,    16] train loss: 0.0080816\n",
      "[15,    20] train loss: 0.0072789\n",
      "Epoch #15\n",
      "[16,     1] train loss: 0.0097826\n",
      "[16,     6] train loss: 0.0081373\n",
      "[16,    11] train loss: 0.0079764\n",
      "[16,    16] train loss: 0.0076241\n",
      "[16,    20] train loss: 0.0079025\n",
      "Epoch #16\n",
      "[17,     1] train loss: 0.0082155\n",
      "[17,     6] train loss: 0.0080321\n",
      "[17,    11] train loss: 0.0078593\n",
      "[17,    16] train loss: 0.0078008\n",
      "[17,    20] train loss: 0.0076697\n",
      "Epoch #17\n",
      "[18,     1] train loss: 0.0101635\n",
      "[18,     6] train loss: 0.0077581\n",
      "[18,    11] train loss: 0.0081438\n",
      "[18,    16] train loss: 0.0078232\n",
      "[18,    20] train loss: 0.0077879\n",
      "Epoch #18\n",
      "[19,     1] train loss: 0.0086042\n",
      "[19,     6] train loss: 0.0076708\n",
      "[19,    11] train loss: 0.0083615\n",
      "[19,    16] train loss: 0.0075906\n",
      "[19,    20] train loss: 0.0076598\n",
      "Epoch #19\n",
      "[20,     1] train loss: 0.0101799\n",
      "[20,     6] train loss: 0.0080749\n",
      "[20,    11] train loss: 0.0076195\n",
      "[20,    16] train loss: 0.0079360\n",
      "[20,    20] train loss: 0.0076776\n",
      "Epoch #20\n",
      "[21,     1] train loss: 0.0092720\n",
      "[21,     6] train loss: 0.0077402\n",
      "[21,    11] train loss: 0.0082950\n",
      "[21,    16] train loss: 0.0073540\n",
      "[21,    20] train loss: 0.0081699\n",
      "Epoch #21\n",
      "[22,     1] train loss: 0.0102368\n",
      "[22,     6] train loss: 0.0077917\n",
      "[22,    11] train loss: 0.0081406\n",
      "[22,    16] train loss: 0.0077677\n",
      "[22,    20] train loss: 0.0077268\n",
      "Epoch #22\n",
      "[23,     1] train loss: 0.0088713\n",
      "[23,     6] train loss: 0.0078347\n",
      "[23,    11] train loss: 0.0078281\n",
      "[23,    16] train loss: 0.0081165\n",
      "[23,    20] train loss: 0.0075821\n",
      "Epoch #23\n",
      "[24,     1] train loss: 0.0095117\n",
      "[24,     6] train loss: 0.0080623\n",
      "[24,    11] train loss: 0.0072503\n",
      "[24,    16] train loss: 0.0080807\n",
      "[24,    20] train loss: 0.0076120\n",
      "Epoch #24\n",
      "[25,     1] train loss: 0.0093347\n",
      "[25,     6] train loss: 0.0077715\n",
      "[25,    11] train loss: 0.0081693\n",
      "[25,    16] train loss: 0.0075887\n",
      "[25,    20] train loss: 0.0074689\n",
      "Epoch #25\n",
      "[26,     1] train loss: 0.0091573\n",
      "[26,     6] train loss: 0.0078117\n",
      "[26,    11] train loss: 0.0080397\n",
      "[26,    16] train loss: 0.0077899\n",
      "[26,    20] train loss: 0.0072094\n",
      "Epoch #26\n",
      "[27,     1] train loss: 0.0087879\n",
      "[27,     6] train loss: 0.0079764\n",
      "[27,    11] train loss: 0.0078680\n",
      "[27,    16] train loss: 0.0077989\n",
      "[27,    20] train loss: 0.0077034\n",
      "Epoch #27\n",
      "[28,     1] train loss: 0.0093518\n",
      "[28,     6] train loss: 0.0081122\n",
      "[28,    11] train loss: 0.0077966\n",
      "[28,    16] train loss: 0.0079289\n",
      "[28,    20] train loss: 0.0075007\n",
      "Epoch #28\n",
      "[29,     1] train loss: 0.0095062\n",
      "[29,     6] train loss: 0.0079385\n",
      "[29,    11] train loss: 0.0079094\n",
      "[29,    16] train loss: 0.0077574\n",
      "[29,    20] train loss: 0.0073787\n",
      "Epoch #29\n",
      "[30,     1] train loss: 0.0085287\n",
      "[30,     6] train loss: 0.0080277\n",
      "[30,    11] train loss: 0.0076636\n",
      "[30,    16] train loss: 0.0079176\n",
      "[30,    20] train loss: 0.0076593\n",
      "Epoch #30\n",
      "[31,     1] train loss: 0.0082420\n",
      "[31,     6] train loss: 0.0080458\n",
      "[31,    11] train loss: 0.0082744\n",
      "[31,    16] train loss: 0.0078189\n",
      "[31,    20] train loss: 0.0071463\n",
      "Epoch #31\n",
      "[32,     1] train loss: 0.0102165\n",
      "[32,     6] train loss: 0.0077872\n",
      "[32,    11] train loss: 0.0079097\n",
      "[32,    16] train loss: 0.0079223\n",
      "[32,    20] train loss: 0.0076388\n",
      "Epoch #32\n",
      "[33,     1] train loss: 0.0093711\n",
      "[33,     6] train loss: 0.0082679\n",
      "[33,    11] train loss: 0.0077234\n",
      "[33,    16] train loss: 0.0075675\n",
      "[33,    20] train loss: 0.0073854\n",
      "Epoch #33\n",
      "[34,     1] train loss: 0.0091272\n",
      "[34,     6] train loss: 0.0079630\n",
      "[34,    11] train loss: 0.0078105\n",
      "[34,    16] train loss: 0.0079669\n",
      "[34,    20] train loss: 0.0072131\n",
      "Epoch #34\n",
      "[35,     1] train loss: 0.0091285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-37015b56e6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mbatch_bass_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbass_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexamples_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mexamples_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mbatch_bass_train_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bass_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#         batch_bass_train_raw = batch_bass_train_raw.transpose(0, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# transpose нужен БЫЛ для обмена размерности батча и размерности шагов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 500\n",
    "batch_size = 32\n",
    "shuffle_every_epoch = True\n",
    "    \n",
    "if shuffle_every_epoch:\n",
    "    print(f\"shuffle_every_epoch is on\")\n",
    "else:\n",
    "    print(f\"shuffle_every_epoch is off\")\n",
    "    # shuffle train and test set:\n",
    "    drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    if shuffle_every_epoch:\n",
    "        # shuffle train and test set:\n",
    "        drum_train, bass_train, drum_test, bass_test = shuffle(drum, bass)\n",
    "        \n",
    "    examples_count = len(drum_train)\n",
    "    examples_id = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    runnint_count = 0\n",
    "    batch_id = 0\n",
    "    while examples_id < examples_count:\n",
    "        batch_drum_train = drum_train[examples_id:examples_id + batch_size]\n",
    "        batch_bass_train = bass_train[examples_id:examples_id + batch_size]\n",
    "        \n",
    "        batch_bass_train_raw = torch.tensor(list(map(lambda p: p.image, batch_bass_train)), dtype=torch.float)\n",
    "#         batch_bass_train_raw = batch_bass_train_raw.transpose(0, 1)\n",
    "        # transpose нужен БЫЛ для обмена размерности батча и размерности шагов\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        bass_outputs = dnb_lstm(batch_drum_train).squeeze()\n",
    "#         bass_outputs = bass_outputs.reshape(bass_outputs.size()[0], -1)\n",
    "#         batch_bass_train = batch_bass_train.reshape(batch_bass_train.size()[0], -1)\n",
    "#         print(f\"bass_outputs:{bass_outputs.size()} batch_bass_train: {batch_bass_train}\")\n",
    "#         print(f\"bass_outputs:{bass_outputs} batch_bass_train: {batch_bass_train}\")\n",
    "        \n",
    "        # loss = criterion(bass_outputs, batch_bass_train.long())\n",
    "        loss = criterion(bass_outputs, batch_bass_train_raw)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        runnint_count += 1\n",
    "        period = 5\n",
    "        if batch_id % period == 0 or examples_id + batch_size >= examples_count:\n",
    "            print('[%d, %5d] train loss: %.7f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / runnint_count))\n",
    "            running_loss = 0.0\n",
    "            runnint_count = 1\n",
    "            \n",
    "        # update batch info\n",
    "        examples_id += batch_size\n",
    "        batch_id += 1\n",
    "        \n",
    "    # here we can insert measure error on test set\n",
    "\n",
    "#should check accuracy on validation set\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(drum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bass_outputs.squeeze().int()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сохранить результаты работы сети. На anaconda нет mido, поэтому сохраняем результаты работы просто в массивчик npy... Однако, как альтернатива, его можно поставить чере pip в conda:\n",
    "https://github.com/mido/mido/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "from decode_patterns.data_conversion import build_track, DrumMelodyPair, NumpyImage, Converter\n",
    "\n",
    "\n",
    "converter = Converter((data_height, data_width))\n",
    "\n",
    "batch_drum = drum_train + drum_test + drum_validation\n",
    "batch_drum = drum_validation\n",
    "batch_bass = bass_train + bass_test + bass_validation\n",
    "batch_bass = drum_validation\n",
    "with torch.no_grad():\n",
    "    bass_outputs = dnb_lstm(batch_drum)\n",
    "    bass_outputs = bass_outputs.squeeze().int()\n",
    "    \n",
    "    for i in range(len(batch_drum)):\n",
    "        bass_outputs = dnb_lstm(batch_drum)\n",
    "        bass_outputs = ((bass_outputs.squeeze() + 1) / 2 > 0.55).int()\n",
    "#         print(f\"i:{i}\")\n",
    "#         print(bass_outputs[i].shape)\n",
    "#         print(batch_drum[i].image.shape)\n",
    "            \n",
    "        img_dnb = np.concatenate((batch_drum[i].image,bass_outputs[i]), axis=1)\n",
    "#         print(f\"img_dnb:{list(bass_output)}\")\n",
    "        numpy_pair = NumpyImage(np.array(img_dnb)\n",
    "                                , batch_drum[i].tempo\n",
    "                                , batch_drum[i].instrument\n",
    "                                , 1\n",
    "                                , batch_drum[i].min_note)\n",
    "        pair = converter.convert_numpy_image_to_pair(numpy_pair)\n",
    "#         print(f\"pair.melody:{pair.melody}\")\n",
    "        mid = build_track(pair, tempo=pair.tempo)\n",
    "        mid.save(f\"midi/npy/sample{i+1}.mid\")\n",
    "#         np.save(f\"midi/npy/drum{i+1}.npy\", batch_drum[:,i,:].int())\n",
    "#         np.save(f\"midi/npy/bass{i+1}.npy\", bass_outputs[:,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдения:\n",
    "1. Нейронная сеть выдаёт по одной барабанной партии одну басовую партию. Следовательно, страдает разнообразие генерируемой музыки\n",
    "2. (следствие из 1) В датасете (скорее всего) имеются типичные ритмы, которым соответствуют абсолютно разные мелодии на разных инструментах. Это сильно сбивает с толку нейросеть. Как следствие, на уникальных ритмах нейросеть переобучается, а на типичных -- не понимает, какую мелодию сгенерировать\n",
    "\n",
    "Предложения:\n",
    "1. Придумать способ разделить пары (каким-то образом) в данной ситуации, возможно добавить жанр или ещё что..\n",
    "2. Разметить музыку тональностями -- это может упростить обучение нейросети\n",
    "3. Добавить случайность в латентное пространство"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
